{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.pinecone.io/docs/quickstart\n",
        "\n",
        "https://www.sbert.net/\n",
        "\n",
        "https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAlQoZlKGHUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading in CSV data for all publications of all professors in NC State UNC Charlotte, and UNC Chapel Hill"
      ],
      "metadata": {
        "id": "oweKqgzGxsiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "NCSU = pd.read_csv('/content/final_NCSU_data.csv')\n",
        "UNCC = pd.read_csv('/content/final_UNCC_data.csv')\n",
        "UNC = pd.read_csv('/content/final_UNC_data.csv')"
      ],
      "metadata": {
        "id": "OuD_udP0Rftk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NCSU.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Zj2y4qu1Rqhy",
        "outputId": "159ada0e-64ef-4342-9a80-4da322889263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                              Title  \\\n",
              "0           0  Generalizing electrocardiogram delineation -- ...   \n",
              "1           1  A Generalized Framework for Measuring Pedestri...   \n",
              "2           2  Using Open Data and Open-Source Software to De...   \n",
              "3           3  Oxygen vacancy-induced anomalous Hall effect i...   \n",
              "4           4  Coexistence of superconductivity and weak anti...   \n",
              "\n",
              "                        Date                                 Id  \\\n",
              "0  2021-11-25 10:11:41+00:00  http://arxiv.org/abs/2111.12996v2   \n",
              "1  2021-05-18 20:22:58+00:00  http://arxiv.org/abs/2105.08814v1   \n",
              "2  2022-05-11 02:39:16+00:00  http://arxiv.org/abs/2205.05240v1   \n",
              "3  2021-09-16 16:02:29+00:00  http://arxiv.org/abs/2109.08073v2   \n",
              "4  2021-09-20 18:33:17+00:00  http://arxiv.org/abs/2109.09786v1   \n",
              "\n",
              "                                             Summary  \\\n",
              "0  Obtaining per-beat information is a key task i...   \n",
              "1  Pedestrian accessibility is an important facto...   \n",
              "2  Benchmarking and monitoring urban design and t...   \n",
              "3  The anomalous Hall effect, a hallmark of broke...   \n",
              "4  The intersection of two-dimensional supercondu...   \n",
              "\n",
              "                                 URL          Author  \n",
              "0  http://arxiv.org/pdf/2111.12996v2     Juan Acosta  \n",
              "1  http://arxiv.org/pdf/2105.08814v1  Deepti Adlakha  \n",
              "2  http://arxiv.org/pdf/2205.05240v1  Deepti Adlakha  \n",
              "3  http://arxiv.org/pdf/2109.08073v2     Kaveh Ahadi  \n",
              "4  http://arxiv.org/pdf/2109.09786v1     Kaveh Ahadi  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb6c7c7a-714c-45fe-a2c5-3b938ae26054\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Title</th>\n",
              "      <th>Date</th>\n",
              "      <th>Id</th>\n",
              "      <th>Summary</th>\n",
              "      <th>URL</th>\n",
              "      <th>Author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Generalizing electrocardiogram delineation -- ...</td>\n",
              "      <td>2021-11-25 10:11:41+00:00</td>\n",
              "      <td>http://arxiv.org/abs/2111.12996v2</td>\n",
              "      <td>Obtaining per-beat information is a key task i...</td>\n",
              "      <td>http://arxiv.org/pdf/2111.12996v2</td>\n",
              "      <td>Juan Acosta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A Generalized Framework for Measuring Pedestri...</td>\n",
              "      <td>2021-05-18 20:22:58+00:00</td>\n",
              "      <td>http://arxiv.org/abs/2105.08814v1</td>\n",
              "      <td>Pedestrian accessibility is an important facto...</td>\n",
              "      <td>http://arxiv.org/pdf/2105.08814v1</td>\n",
              "      <td>Deepti Adlakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Using Open Data and Open-Source Software to De...</td>\n",
              "      <td>2022-05-11 02:39:16+00:00</td>\n",
              "      <td>http://arxiv.org/abs/2205.05240v1</td>\n",
              "      <td>Benchmarking and monitoring urban design and t...</td>\n",
              "      <td>http://arxiv.org/pdf/2205.05240v1</td>\n",
              "      <td>Deepti Adlakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Oxygen vacancy-induced anomalous Hall effect i...</td>\n",
              "      <td>2021-09-16 16:02:29+00:00</td>\n",
              "      <td>http://arxiv.org/abs/2109.08073v2</td>\n",
              "      <td>The anomalous Hall effect, a hallmark of broke...</td>\n",
              "      <td>http://arxiv.org/pdf/2109.08073v2</td>\n",
              "      <td>Kaveh Ahadi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Coexistence of superconductivity and weak anti...</td>\n",
              "      <td>2021-09-20 18:33:17+00:00</td>\n",
              "      <td>http://arxiv.org/abs/2109.09786v1</td>\n",
              "      <td>The intersection of two-dimensional supercondu...</td>\n",
              "      <td>http://arxiv.org/pdf/2109.09786v1</td>\n",
              "      <td>Kaveh Ahadi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb6c7c7a-714c-45fe-a2c5-3b938ae26054')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb6c7c7a-714c-45fe-a2c5-3b938ae26054 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb6c7c7a-714c-45fe-a2c5-3b938ae26054');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a List of all the abstracts and replacing the new line characters"
      ],
      "metadata": {
        "id": "BAw3iGxBx15c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(NCSU['Summary']) + list(UNCC['Summary']) + list(UNC['Summary'])\n",
        "\n",
        "data = [y.replace('\\n', '') for y in data]"
      ],
      "metadata": {
        "id": "Trb2-QRyRsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Might have to remove latex fomulas and other special characters\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ7g7abGR-_Z",
        "outputId": "cc35bfa9-1d1a-47ce-98f4-088b7037a7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Obtaining per-beat information is a key task in the analysis of cardiacelectrocardiograms (ECG), as many downstream diagnosis tasks are dependent onECG-based measurements. Those measurements, however, are costly to produce,especially in recordings that change throughout long periods of time. However,existing annotated databases for ECG delineation are small, being insufficientin size and in the array of pathological conditions they represent. Thisarticle delves has two main contributions. First, a pseudo-synthetic datageneration algorithm was developed, based in probabilistically composing ECGtraces given \"pools\" of fundamental segments, as cropped from the originaldatabases, and a set of rules for their arrangement into coherent synthetictraces. The generation of conditions is controlled by imposing expert knowledgeon the generated trace, which increases the input variability for training themodel. Second, two novel segmentation-based loss functions have been developed,which attempt at enforcing the prediction of an exact number of independentstructures and at producing closer segmentation boundaries by focusing on areduced number of samples. The best performing model obtained an $F_1$-score of99.38\\\\% and a delineation error of $2.19 \\\\pm 17.73$ ms and $4.45 \\\\pm 18.32$ msfor all wave\\'s fiducials (onsets and offsets, respectively), as averaged acrossthe P, QRS and T waves for three distinct freely available databases. Theexcellent results were obtained despite the heterogeneous characteristics ofthe tested databases, in terms of lead configurations (Holter, 12-lead),sampling frequencies ($250$, $500$ and $2,000$ Hz) and representedpathophysiologies (e.g., different types of arrhythmias, sinus rhythm withstructural heart disease), hinting at its generalization capabilities, whileoutperforming current state-of-the-art delineation approaches.',\n",
              " 'Pedestrian accessibility is an important factor in urban transport and landuse policy and critical for creating healthy, sustainable cities. Developingand evaluating indicators measuring inequalities in pedestrian accessibilitycan help planners and policymakers benchmark and monitor the progress of cityplanning interventions. However, measuring and assessing indicators of urbandesign and transport features at high resolution worldwide to enable citycomparisons is challenging due to limited availability of official, highquality, and comparable spatial data, as well as spatial analysis toolsoffering customizable frameworks for indicator construction and analysis. Toaddress these challenges, this study develops an open source software frameworkto construct pedestrian accessibility indicators for cities using open andconsistent data. It presents a generalized method to consistently measurepedestrian accessibility at high resolution and spatially aggregated scale, toallow for both within- and between-city analyses. The open source and open datamethods developed in this study can be extended to other cities worldwide tosupport local planning and policymaking. The software is made publiclyavailable for reuse in an open repository.',\n",
              " \"Benchmarking and monitoring urban design and transport features is criticalto achieving local and international health and sustainability goals. However,most urban indicator frameworks use coarse spatial scales that only allowbetween-city comparisons or require expensive, technical, local spatialanalyses for within-city comparisons. This study developed a reusableopen-source urban indicator computational framework using open data to enableconsistent local and global comparative analyses. We demonstrate this frameworkby calculating spatial indicators - for 25 diverse cities in 19 countries - ofurban design and transport features that support health and sustainability. Welink these indicators to cities' policy contexts and identify populationsliving above and below critical thresholds for physical activity throughwalking. Efforts to broaden participation in crowdsourcing data and tocalculate globally consistent indicators are essential for planningevidence-informed urban interventions, monitoring policy impacts, and learninglessons from peer cities to achieve health, equity, and sustainability goals.\",\n",
              " 'The anomalous Hall effect, a hallmark of broken time-reversal symmetry andspin-orbit coupling, is frequently observed in magnetically polarized systems.Its realization in non-magnetic systems, however, remains elusive. Here, wereport on the observation of anomalous Hall effect in nominally non-magneticKTaO3. Anomalous Hall effect emerges in reduced KTaO3 and shows an extrinsic tointrinsic crossover. A paramagnetic behavior is observed in reduced samplesusing first principles calculations and quantitative magnetometry. The observedanomalous Hall effect follows the oxygen vacancy-induced magnetizationresponse, suggesting that the localized magnetic moments of the oxygenvacancies scatter conduction electrons asymmetrically and give rise toanomalous Hall effect. The anomalous Hall conductivity becomes insensitive toscattering rate in the low temperature limit (T<5 K), implying that the Berrycurvature of the electrons on the Fermi surface controls the anomalous Halleffect. Our observations describe a detailed picture of many-body interactions,triggering anomalous Hall effect in a non-magnetic system.',\n",
              " 'The intersection of two-dimensional superconductivity and topologicallynontrivial states hosts a wide range of quantum phenomena, including Majoranafermions. Coexistence of topologically nontrivial states and superconductivityin a single material, however, remains elusive. Here, we report on theobservation of two-dimensional superconductivity and weak anti-localization atthe TiOx/KTaO3(111) interfaces. A remnant, saturating resistance persists belowthe transition temperature as superconducting puddles fail to reach phasecoherence. Signatures of weak anti-localization are observed below thesuperconducting transition, suggesting the coexistence of superconductivity andweak anti-localization. The superconducting interfaces show roughly one orderof magnitude larger weak anti-localization correction, compared tonon-superconducting interfaces, alluding to a relatively large coherence lengthin these interfaces.',\n",
              " 'The nature of superconductivity and its interplay with strong spin-orbitcoupling at the KTaO3(111) interfaces remains a subject of debate. To addressthis problem, we grew epitaxial LaMnO3/KTaO3(111) heterostructures. We showthat superconductivity is robust against the in-plane magnetic field, with thecritical field of superconductivity reaching 25 T in optimally dopedheterostructures. The superconducting order parameter is highly sensitive tocarrier density. We argue that spin-orbit coupling drives the formation ofanomalous quasiparticles with vanishing magnetic moment, providing thecondensate significant immunity against magnetic fields beyond the Pauliparamagnetic limit. These results offer design opportunities forsuperconductors with extreme resilience against magnetic field.',\n",
              " 'We report on the growth of epitaxial Sr2RuO4 films using a hybrid molecularbeam epitaxy approach in which a volatile precursor containing RuO4 is used tosupply ruthenium and oxygen. The use of the precursor overcomes a number ofissues encountered in traditional MBE that uses elemental metal sources.Phase-pure, epitaxial thin films of Sr2RuO4 are obtained. At high substratetemperatures, growth proceeds in a layer-by-layer mode with intensityoscillations observed in reflection high-energy electron diffraction. Films areof high structural quality, as documented by x-ray diffraction, atomic forcemicroscopy, and transmission electron microscopy. The method should be suitablefor the growth of other complex oxides containing ruthenium, opening upopportunities to investigate thin films that host rich exotic ground states.',\n",
              " 'The lattice response of a prototype Mott insulator, SmTiO3, to hole doping isinvestigated with atomic-scale spatial resolution. SmTiO3 films are doped withSr on the Sm site with concentrations that span the insulating and metallicsides of the filling-controlled Mott metal-insulator transition (MIT). TheGdFeO3-type distortions are investigated using an atomic resolution scanningtransmission electron microscopy technique that can resolve small latticedistortions with picometer precision. We show that these distortions aregradually and uniformly reduced as the Sr concentration is increased withoutany phase separation. Significant distortions persist into the metallic state.The results present a new picture of the physics of this prototypefilling-controlled MIT, which is discussed.',\n",
              " 'We report on the observation of a possible topological Hall effect in thinfilms of the itinerant ferromagnet Eu1-xSmxTiO3. EuTiO3 and Eu0.955Sm0.045TiO3films were grown by molecular beam epitaxy. The EuTiO3 film is insulating. TheHall resistivity of the Eu0.955Sm0.045TiO3 films exhibits the anomalous Halleffect below the Curie temperature of ~ 5 K as well as additional features thatappear at 2 K. It is shown that these features are magnetic in origin andconsistent with the topological Hall effect seen in materials systems withtopologically nontrivial spin textures, such as skyrmions. The results open upinteresting possibilities for epitaxial hybrid heterostructures that combinetopological magnetic states, tunable carrier densities, and other phenomena.',\n",
              " 'Magnetotransport and magnetism of epitaxial SmTiO3/EuTiO3 heterostructuresgrown by molecular beam epitaxy are investigated. It is shown that the polardiscontinuity at the interface introduces ~ 3.9x10^14 cm^-2 carriers into theEuTiO3. The itinerant carriers exhibit two distinct contributions to thespontaneous Hall effect. The anomalous Hall effect appears despite a very smallmagnetization, indicating a non-collinear spin structure and the secondcontribution resembles a topological Hall effect. Qualitative differences existin the temperature dependence of both Hall effects when compared to uniformlydoped EuTiO3. In particular, the topological Hall effect contribution appearsat higher temperatures and the anomalous Hall effect shows a sign change withtemperature. The results suggest that interfaces can be used to tunetopological phenomena in itinerant magnetic systems.',\n",
              " 'Sr3Ru2O7 belongs to the family of layered strontium ruthenates and exhibits arange of unusual emergent properties, such as electron nematic behavior andmetamagnetism. Here, we show that epitaxial film strain significantly modifiesthese phenomena. In particular, we observe enhanced magnetic interactions andan electron nematic phase that extends to much higher temperatures and over alarger magnetic field range than in bulk single crystals. Furthermore, thefilms show an unusual anisotropic non-Fermi liquid behavior that is controlledby the direction of the applied magnetic field. At high magnetic fields themetamagnetic transition to a ferromagnetic phase recovers isotropicFermi-liquid behavior. The results support the interpretation that thesephenomena are linked to the special features of the Fermi surface, which can betuned by both film strain and an applied magnetic field.',\n",
              " 'SrTiO$_3$ is an incipient ferroelectric on the verge of a polar instability,which is avoided at low temperatures by quantum fluctuations. Within thisunusual quantum paraelectric phase, superconductivity persists despiteextremely dilute carrier densities. Ferroelectric fluctuations have beensuspected to play a role in the origin of superconductivity by contributing toelectron pairing. To investigate this possibility, we used optical secondharmonic generation to measure the doping and temperature dependence of theferroelectric order parameter in compressively strained SrTiO$_3$ thin films.At low temperatures, we uncover a spontaneous out-of-plane ferroelectricpolarization with an onset that correlates perfectly with normal-stateelectrical resistivity anomalies. These anomalies have previously beenassociated with an enhancement of the superconducting critical temperature indoped SrTiO$_3$ films, directly linking the ferroelectric and superconductingphases. We develop a long-range mean-field Ising model of the ferroelectricphase transition to interpret the data and extract the relevant energy scalesin the system. Our results support a long-suspected connection betweenferroelectricity and superconductivity in SrTiO$_3$, but call into question therole played by ferroelectric fluctuations.',\n",
              " 'We report on a metal-insulator transition (MIT) that is observed in anelectron system at the SmTiO3/SrTiO3 interface. This MIT is characterized by anabrupt transition at a critical temperature, below which the resistance changesby more than an order of magnitude. The temperature of the transitionsystematically depends on the carrier density, which is tuned from ~ 1x10^14cm^-2 to 3x10^14 cm^-2 by changing the SmTiO3 thickness. Analysis of thetransport properties shows non-Fermi liquid behavior and mass enhancement asthe carrier density is lowered. We compare the MIT characteristics with thoseof known MITs in other materials systems and show that they are distinctlydifferent in several aspects. We tentatively conclude that both long rangeCoulomb interactions and the fixed charge at the polar interface are likely toplay a role in this MIT. The strong dependence on the carrier density makesthis MIT of interest for field-tunable devices.',\n",
              " 'Superconductors that possess both broken spatial inversion symmetry andspin-orbit interactions exhibit a mix of spin singlet and triplet pairing.Here, we report on measurements of the superconducting properties ofelectron-doped, strained SrTiO3 films. These films have an enhancedsuperconducting transition temperature and were previously shown to undergo atransition to a polar phase prior to becoming superconducting. We show thatsome films show signatures of an unusual superconducting state, such as anin-plane critical field that is higher than both the paramagnetic and orbitalpair breaking limits. Moreover, nonreciprocal transport, which reflects theratio of odd versus even pairing interactions, is observed. Together, thesecharacteristics indicate that these films provide a tunable platform forinvestigations of unconventional superconductivity.',\n",
              " 'We report on the evolution of the average and depth-dependent magnetic orderin thin film samples of biaxially stressed and electron-doped EuTiO$_3$ forsamples across a doping range $<$0.1 to 7.8 $\\\\times 10^{20}$ cm$^{-3}$. Underan applied in-plane magnetic field, the G-type antiferromagnetic ground stateundergoes a continuous spin-flop phase transition into in-plane,field-polarized ferromagnetism. The critical field for ferromagnetism slightlydecreases with an increasing number of free carriers, yet the field evolutionof the spin-flop transition is qualitatively similar across the doping range.Unexpectedly, we observe interfacial ferromagnetism with saturated Eu$^{2+}$moments at the substrate interface at low fields preceding ferromagneticsaturation throughout the bulk of the degenerate semiconductor film. We discussthe implications of these findings for the unusual magnetotransport propertiesof this compound.',\n",
              " 'SrTiO3 is an incipient ferroelectric that is believed to exhibit a prototypedisplacive, soft mode ferroelectric transition when subjected to mechanicalstress or alloying. We use high-angle annular dark-field imaging in scanningtransmission electron microscopy to reveal local polar regions in theroom-temperature, paraelectric phase of strained SrTiO3 films, which undergo aferroelectric transition at low temperatures. These films containnanometer-sized domains in which the Ti columns are displaced. In contrast,these nanodomains are absent in unstrained films, which do not becomeferroelectric. The results show that the ferroelectric transition of strainedSrTiO3 is an order-disorder transition. We discuss the impact of the results onthe nature of the ferroelectric transition of SrTiO3.',\n",
              " \"The spin-orbit coupling field, an atomic magnetic field inside a Kramer'ssystem, or discrete symmetries can create a topological torus in the BrillouinZone and provide protected edge or surface states, which can containrelativistic fermions, namely, Dirac and Weyl Fermions. The topology-protectedhelical edge or surface states and the bulk electronic energy band definedifferent quantum or topological phases of matters, offering an excellentprospect for some unique device applications. Device applications of thequantum materials rely primarily on understanding the topological properties,their mutual conversion processes under different external stimuli, and thephysical system for achieving the phase conversion. There have been tremendousefforts in finding new topological materials with exotic topological phases.However, the application of the topological properties in devices is stilllimited due to the slow progress in developing the physical structures forcontrolling the topological phase conversions. Such control systems oftenrequire extreme tuning conditions or the fabrication of complex multi-layeredtopological structures. This review article highlights the details of thetopological phases, their conversion processes, along with their potentialphysical systems, and the prospective application fields. A general overview ofthe critical factors for topological phases and the materials properties arefurther discussed to provide the necessary background for the followingsections.\",\n",
              " 'Strain-engineering is a powerful means to tune the polar, structural, andelectronic instabilities of incipient ferroelectrics. KTaO3 is near a polarinstability and shows anisotropic superconductivity in electron-doped samples.Here, we demonstrate growth of high quality KTaO3 thin films by molecular-beamepitaxy. Tantalum was provided by both a suboxide source emanating a TaO2 fluxfrom Ta2O5 contained in a conventional effusion cell as well as anelectron-beam-heated tantalum source. Excess potassium and a combination ofozone and oxygen (10 \\\\% O3 + 90 \\\\% O2) were simultaneously supplied with theTaO$_2$ (or tantalum) molecular beams to grow the KTaO$_3$ films. Laue fringessuggest that the films are smooth with an abrupt film/substrate interface.Cross-sectional scanning transmission electron microscopy does not show anyextended defects and confirms that the films have an atomically abruptinterface with the substrate. Atomic force microscopy reveals atomic steps atthe surface of the grown films. Reciprocal space mapping demonstrates that thefilms, when sufficiently thin, are coherently strained to the SrTiO$_3$ (001)and GdScO$_3$ (110) substrates.',\n",
              " 'We examined the sensitivity of African easterly waves (AEWs) to elevatedterrain over North Africa using a numerical weather prediction model. We formedfive ensembles of simulated AEW activity with orographic features independentlyreduced in four key regions. The ensemble members consisted of 10 consecutiveAEW seasons simulated separately. From the ensembles, the southern AEWstormtrack was most sensitive to the reduction of the Ethiopian highlands.Energy budgets showed that diminished diabatic heating associated withprecipitating convection was the likely driver of the weaker AEWs. Baroclinicoverturning was the dominant pathway for this response. The northern AEWstormtrack was most sensitive to the reduction of the Hoggar and Tibestimountains. In this case, a reduction in the vertical shear and diminishedbaroclinic energy conversions from the background state was associated withweaker AEWs. Through terrain reduction, our results provide a view ofthermodynamic and dynamic feedback in AEWs that is complementary to what hasbeen shown in past studies.',\n",
              " 'It is well known that rapid changes in tropical cyclone motion occur duringinteraction with extratropical waves. While the translation speed has receivedmuch attention in the published literature, acceleration has not. Using a largedata sample of Atlantic tropical cyclones, we formally examine the compositesynoptic-scale patterns associated with tangential and \\\\curvature components oftheir acceleration. During periods of rapid tangential acceleration, thecomposite tropical cyclone moves poleward between an upstream trough anddownstream ridge of a developing extratropical wavepacket. The two systemssubsequently merge in a manner that is consistent with extratropicaltransition. During rapid curvature acceleration, a prominent downstream ridgepromotes recurvature of the tropical cyclone. In contrast, during rapidtangential or curvature deceleration, a ridge is located directly poleward ofthe tropical cyclone. Locally, this arrangement takes the form of acyclone-anticyclone vortex pair somewhat akin to a dipole block. On average,the tangential acceleration peaks 18 hours prior to extratropical transitionwhile the curvature acceleration peaks at recurvature. These findings confirmthat rapid acceleration of tropical cyclones is mediated by interaction withextratropical baroclinic waves. Furthermore, The tails of the distribution ofacceleration and translation speed show a robust reduction over the past 5decades. We speculate that these trends may reflect the poleward shift andweakening of extratropical Rossby waves.',\n",
              " 'We examine the group dynamic of African easterly waves (AEW) generated in arealistic, spatially non-homogeneous African easterly jet (AEJ) using anidealized general circulation model. Our objective is to investigate whetherthe limited zonal extent of the AEJ is an impediment to AEW development. Weconstruct a series of basic states using global reanalysis fields andinitialize waves via transient heating over West Africa. The dominant responseis a localized wavepacket that disperses upstream and downstream. The inclusionof a crude representation of boundary layer damping stabilizes the waves inmost cases. In some basic states, however, exponential growth occurs even inthe presence of damping. This shows that AEWs can occasionally emergespontaneously. The key result is that the wavepacket in almost all casesremains within the AEJ instead of being swept away. Drawing from other studies,this also suggests that even the damped waves can grow if coupled withadditional sources of energy such as moist convection and dust radiativefeedback. The wavepacket in the localized AEJ appears to satisfy a conditionfor absolute instability, a form of spatial hydrodynamic instability. However,this needs to be verified more rigorously. Our results also suggest that theintermittent nature of AEWs is mediated, not by transitions between convectiveand absolute instability, but likely by external sources such as propagatingequatorial wave modes',\n",
              " 'We provide a detailed derivation of the Karhunen-Lo\\\\`eve expansion of astochastic process. We also discuss briefly Gaussian processes, and provide asimple numerical study for the purpose of illustration.',\n",
              " 'We recall some basics regarding the concept of Bayes risk in the context offinite-dimensional ill-posed linear inverse problem with Gaussian prior andnoise models. In particular, we rederive the following basic result: in thepresent Gaussian linear setting, the Bayes risk of the posterior mean, relativeto the sum of squares loss function, equals the trace of the posteriorcovariance operator.',\n",
              " 'We study the problem of characterizing the effective (homogenized) propertiesof materials whose diffusive properties are modeled with random fields.Focusing on elliptic PDEs with stationary and ergodic random coefficientfunctions, we provide a gentle introduction to the mathematical theory ofhomogenization of random media. We also present numerical examples to elucidatethe theoretical concepts and results.',\n",
              " 'The global sensitivity analysis of time-dependent processes requireshistory-aware approaches. We develop for that purpose a variance-based methodthat leverages the correlation structure of the problems under study andemploys surrogate models to accelerate the computations. The errors resultingfrom fixing unimportant uncertain parameters to their nominal values areanalyzed through a priori estimates. We illustrate our approach on a harmonicoscillator example and on a nonlinear dynamic cholera model.',\n",
              " 'We present a review of methods for optimal experimental design (OED) forBayesian inverse problems governed by partial differential equations withinfinite-dimensional parameters. The focus is on problems where one seeks tooptimize the placement of measurement points, at which data are collected, suchthat the uncertainty in the estimated parameters is minimized. We present themathematical foundations of OED in this context and survey the computationalmethods for the class of OED problems under study. We also outline somedirections for future research in this area.',\n",
              " 'We consider finite-dimensional Bayesian linear inverse problems with Gaussianpriors and additive Gaussian noise models. The goal of this note is to presenta simple derivation of the well-known fact that solving the Bayesian D-optimalexperimental design problem, i.e. maximizing expected information gain, isequivalent to minimizing the log-determinant of posterior covariance operator.The discussion focuses on the finite-dimensional inverse problems. However, thepresentation is kept abstract to facilitate the discussion of extensions toinfinite-dimensional inverse problems.',\n",
              " 'We provide a new perspective on the study of parameterized optimizationproblems. Our approach combines methods for post-optimal sensitivity analysisand ordinary differential equations to quantify the uncertainty in theminimizer due to uncertain parameters in the optimization problem. Weillustrate the proposed approach with a simple analytic example and an inverseproblem governed by an advection diffusion equation.',\n",
              " 'We consider Bayesian linear inverse problems in infinite-dimensionalseparable Hilbert spaces, with a Gaussian prior measure and additive Gaussiannoise model, and provide an extension of the concept of Bayesian D-optimalityto the infinite-dimensional case. To this end, we derive theinfinite-dimensional version of the expression for the Kullback-Leiblerdivergence from the posterior measure to the prior measure, which issubsequently used to derive the expression for the expected information gain.We also study the notion of Bayesian A-optimality in the infinite-dimensionalsetting, and extend the well known (in the finite-dimensional case) equivalenceof the Bayes risk of the MAP estimator with the trace of the posteriorcovariance, for the Gaussian linear case, to the infinite-dimensional Hilbertspace case.',\n",
              " 'We present a framework for derivative-based global sensitivity analysis (GSA)for models with high-dimensional input parameters and functional outputs. Wecombine ideas from derivative-based GSA, random field representation viaKarhunen--Lo\\\\`{e}ve expansions, and adjoint-based gradient computation toprovide a scalable computational framework for computing the proposedderivative-based GSA measures. We illustrate the strategy for a nonlinear ODEmodel of cholera epidemics and for elliptic PDEs with application examples fromgeosciences and biotransport.',\n",
              " 'We consider optimal design of PDE-based Bayesian linear inverse problems withinfinite-dimensional parameters. We focus on the A-optimal design criterion,defined as the average posterior variance and quantified by the trace of theposterior covariance operator. We propose using structure exploiting randomizedmethods to compute the A-optimal objective function and its gradient, andprovide a detailed analysis of the error for the proposed estimators. To ensuresparse and binary design vectors, we develop a novel reweighted$\\\\ell_1$-minimization algorithm. We also introduce a modified A-optimalcriterion and present randomized estimators for its efficient computation. Wepresent numerical results illustrating the proposed methods on a modelcontaminant source identification problem, where the inverse problem seeks torecover the initial state of a contaminant plume, using discrete measurementsof the contaminant in space and time.',\n",
              " 'We present randomized algorithms for estimating the trace and deter- minantof Hermitian positive semi-definite matrices. The algorithms are based onsubspace iteration, and access the matrix only through matrix vector products.We analyse the error due to randomization, for starting guesses whose elementsare Gaussian or Rademacher random variables. The analysis is cleanly separatedinto a structural (deterministic) part followed by a probabilistic part. Ourabsolute bounds for the expectation and concentration of the estimators arenon-asymptotic and informative even for matrices of low dimension. For thetrace estimators, we also present asymptotic bounds on the number of samples(columns of the starting guess) required to achieve a user-specified relativeerror. Numerical experiments illustrate the performance of the estimators andthe tightness of the bounds on low-dimensional matrices; and on a challengingapplication in uncertainty quantification arising from Bayesian optimalexperimental design.',\n",
              " 'We consider optimal design of infinite-dimensional Bayesian linear inverseproblems governed by partial differential equations that contain secondaryreducible model uncertainties, in addition to the uncertainty in the inversionparameters. By reducible uncertainties we refer to parametric uncertaintiesthat can be reduced through parameter inference. We seek experimental designsthat minimize the posterior uncertainty in the primary parameters, whileaccounting for the uncertainty in secondary parameters. We accomplish this byderiving a marginalized A-optimality criterion and developing an efficientcomputational approach for its optimization. We illustrate our approach forestimating an uncertain time-dependent source in a contaminant transport modelwith an uncertain initial state as secondary uncertainty. Our results indicatethat accounting for additional model uncertainty in the experimental designprocess is crucial.',\n",
              " \"We present a computational framework for dimension reduction and surrogatemodeling to accelerate uncertainty quantification in computationally intensivemodels with high-dimensional inputs and function-valued outputs. Our drivingapplication is multiphase flow in saturated-unsaturated porous media in thecontext of radioactive waste storage. For fast input dimension reduction, weutilize an approximate global sensitivity measure, for function-value outputs,motivated by ideas from the active subspace methods. The proposed approach doesnot require expensive gradient computations. We generate an efficient surrogatemodel by combining a truncated Karhunen-Lo\\\\'{e}ve (KL) expansion of the outputwith polynomial chaos expansions, for the output KL modes, constructed in thereduced parameter space. We demonstrate the effectiveness of the proposedsurrogate modeling approach with a comprehensive set of numerical experiments,where we consider a number of function-valued (temporally or spatiallydistributed) QoIs.\",\n",
              " 'Sensitivity analysis is routinely performed on simplified surrogate models asthe cost of such analysis on the original model may be prohibitive. Little isknown in general about the induced bias on the sensitivity results. Within theframework of chemical kinetics, we provide a full justification of the aboveapproach in the case of variance based methods provided the surrogate modelresults from the original one through the thermodynamic limit. We also provideillustrative numerical examples in context of a Michaelis--Menten system and abiochemical reaction network describing a genetic oscillator.',\n",
              " 'By their very nature, rare event probabilities are expensive to compute; theyare also delicate to estimate as their value strongly depends on distributionalassumptions on the model parameters. Hence, understanding the sensitivity ofthe computed rare event probabilities to the hyper-parameters that define thedistribution law of the model parameters is crucial. We show that by (i)accelerating the calculation of rare event probabilities through subsetsimulation and (ii) approximating the resulting probabilities through apolynomial chaos expansion, the global sensitivity of such problems can beanalyzed through a double-loop sampling approach. The resulting method isconceptually simple and computationally efficient; its performance isillustrated on a subsurface flow application and on an analytical example.',\n",
              " \"This work focuses on the simulation of $CO_2$ storage in deep undergroundformations under uncertainty and seeks to understand the impact ofuncertainties in reservoir properties on $CO_2$ leakage. To simulate theprocess, a non-isothermal two-phase two-component flow system with equilibriumphase exchange is used. Since model evaluations are computationally intensive,instead of traditional Monte Carlo methods, we rely on polynomial chaos (PC)expansions for representation of the stochastic model response. A non-intrusiveapproach is used to determine the PC coefficients. We establish the accuracy ofthe PC representations within a reasonable error threshold through systematicconvergence studies. In addition to characterizing the distributions of modelobservables, we compute probabilities of excess $CO_2$ leakage. Moreover, weconsider the injection rate as a design parameter and compute an optimuminjection rate that ensures that the risk of excess pressure buildup at theleaky well remains below acceptable levels. We also provide a comprehensiveanalysis of sensitivities of $CO_2$ leakage, where we compute the contributionsof the random parameters, and their interactions, to the variance by computingfirst, second, and total order Sobol' indices.\",\n",
              " 'We consider optimal experimental design (OED) for Bayesian nonlinear inverseproblems governed by partial differential equations (PDEs) under modeluncertainty. Specifically, we consider inverse problems in which, in additionto the inversion parameters, the governing PDEs include secondary uncertainparameters. We focus on problems with infinite-dimensional inversion andsecondary parameters and present a scalable computational framework for optimaldesign of such problems. The proposed approach enables Bayesian inversion andOED under uncertainty within a unfied framework. We build on the Bayesianapproximation error (BAE) framework, to incorporate modeling uncertainties inthe Bayesian inverse problem, and methods for A-optimal design ofinfinite-dimensional Bayesian nonlinear inverse problems. Specifically, aGaussian approximation to the posterior at the maximum a posteriori probabilitypoint is used to define an uncertainty aware OED objective that is tractable toevaluate and optimize. In particular, the OED objective can be computed at acost, in the number of PDE solves, that does not grow with the dimension of thediscretized inversion and secondary parameters. The OED problem is formulatedas a binary bilevel PDE constrained optimization problem and a greedyalgorithm, which provides a pragmatic approach, is used to find optimaldesigns. We demonstrate the effectiveness of the proposed approach for a modelinverse problem governed by an elliptic PDE on a three-dimensional domain. Ourcomputational results also highlight the pitfalls of ignoring modelinguncertainties in the OED and/or inference stages.',\n",
              " 'We present an efficient method for computing A-optimal experimental designsfor infinite-dimensional Bayesian linear inverse problems governed by partialdifferential equations (PDEs). Specifically, we address the problem ofoptimizing the location of sensors (at which observational data are collected)to minimize the uncertainty in the parameters estimated by solving the inverseproblem, where the uncertainty is expressed by the trace of the posteriorcovariance. Computing optimal experimental designs (OEDs) is particularlychallenging for inverse problems governed by computationally expensive PDEmodels with infinite-dimensional (or, after discretization, high-dimensional)parameters. To alleviate the computational cost, we exploit the problemstructure and build a low-rank approximation of the parameter-to-observablemap, preconditioned with the square root of the prior covariance operator. Thisrelieves our method from expensive PDE solves when evaluating the optimalexperimental design objective function and its derivatives. Moreover, we employa randomized trace estimator for efficient evaluation of the OED objectivefunction. We control the sparsity of the sensor configuration by employing asequence of penalty functions that successively approximate the$\\\\ell_0$-\"norm\"; this results in binary designs that characterize optimalsensor locations. We present numerical results for inference of the initialcondition from spatio-temporal observations in a time-dependentadvection-diffusion problem in two and three space dimensions. We find that anoptimal design can be computed at a cost, measured in number of forward PDEsolves, that is independent of the parameter and sensor dimensions. Wedemonstrate numerically that $\\\\ell_0$-sparsified experimental designs obtainedvia a continuation method outperform $\\\\ell_1$-sparsified designs.',\n",
              " 'We address the problem of optimal experimental design (OED) for Bayesiannonlinear inverse problems governed by PDEs. The goal is to find a placement ofsensors, at which experimental data are collected, so as to minimize theuncertainty in the inferred parameter field. We formulate the OED objectivefunction by generalizing the classical A-optimal experimental design criterionusing the expected value of the trace of the posterior covariance. We seek amethod that solves the OED problem at a cost (measured in the number of forwardPDE solves) that is independent of both the parameter and sensor dimensions. Tofacilitate this, we construct a Gaussian approximation to the posterior at themaximum a posteriori probability (MAP) point, and use the resulting covarianceoperator to define the OED objective function. We use randomized traceestimation to compute the trace of this (implicitly defined) covarianceoperator. The resulting OED problem includes as constraints the PDEscharacterizing the MAP point, and the PDEs describing the action of thecovariance operator to vectors. The sparsity of the sensor configurations iscontrolled using sparsifying penalty functions. We elaborate our OED method forthe problem of determining the sensor placement to best infer the coefficientof an elliptic PDE. Adjoint methods are used to compute the gradient of thePDE-constrained OED objective function. We provide numerical results forinference of the permeability field in a porous medium flow problem, anddemonstrate that the number of PDE solves required for the evaluation of theOED objective function and its gradient is essentially independent of both theparameter and sensor dimensions. The number of quasi-Newton iterations forcomputing an OED also exhibits the same dimension invariance properties.',\n",
              " 'We present a method for optimal control of systems governed by partialdifferential equations (PDEs) with uncertain parameter fields. We consider anobjective function that involves the mean and variance of the controlobjective, leading to a risk-averse optimal control problem. To make theproblem tractable, we invoke a quadratic Taylor series approximation of thecontrol objective with respect to the uncertain parameter. This enablesderiving explicit expressions for the mean and variance of the controlobjective in terms of its gradients and Hessians with respect to the uncertainparameter. The risk-averse optimal control problem is then formulated as aPDE-constrained optimization problem with constraints given by the forward andadjoint PDEs defining these gradients and Hessians. The expressions for themean and variance of the control objective under the quadratic approximationinvolve the trace of the (preconditioned) Hessian and are thus prohibitive toevaluate. To address this, we employ trace estimators that only require amodest number of Hessian-vector products. We illustrate our approach with twoproblems: the control of a semilinear elliptic PDE with an uncertain boundarysource term, and the control of a linear elliptic PDE with an uncertaincoefficient field. For the latter problem, we derive adjoint-based expressionsfor efficient computation of the gradient of the risk-averse objective withrespect to the controls. Our method ensures that the cost of computing therisk-averse objective and its gradient with respect to the control, measured inthe number of PDE solves, is independent of the (discretized) parameter andcontrol dimensions, and depends only on the number of random vectors employedin the trace estimation. Finally, we present a comprehensive numerical study ofan optimal control problem for fluid flow in a porous medium with uncertainpermeability field.',\n",
              " \"Stochastic models are necessary for the realistic description of anincreasing number of applications. The ability to identify influentialparameters and variables is critical to a thorough analysis and understandingof the underlying phenomena. We present a new global sensitivity analysisapproach for stochastic models, i.e., models with both uncertain parameters andintrinsic stochasticity. Our method relies on an analysis of variance through ageneralization of Sobol' indices and on the use of surrogate models. We showhow to efficiently compute the statistical properties of the resulting indicesand illustrate the effectiveness of our approach by computing first orderSobol' indices for two stochastic models.\",\n",
              " 'We develop a systematic approach for surrogate model construction in reducedinput parameter spaces. A sparse set of model evaluations in the original inputspace is used to approximate derivative based global sensitivity measures(DGSMs) for individual uncertain inputs of the model. An iterative screeningprocedure is developed that exploits DGSM estimates in order to identify theunimportant inputs. The screening procedure forms an integral part of anoverall framework for adaptive construction of a surrogate in the reducedspace. The framework is tested for computational efficiency through an initialimplementation in simple test cases such as the classic Borehole function, anda semilinear elliptic PDE with a random source term. The framework is thendeployed for a realistic application from chemical kinetics, where we study theignition delay in an H2/O2 reaction mechanism with 19 uncertain rate constants.It is observed that significant computational gains can be attained byconstructing accurate low-dimensional surrogates using the proposed framework.',\n",
              " 'We consider biotransport in tumors with uncertain heterogeneous materialproperties. Specifically, we focus on the elliptic partial differentialequation (PDE) modeling the pressure field inside the tumor. The permeabilityfield is modeled as a log-Gaussian random field with a prespecified covariancefunction. We numerically explore dimension reduction of the input parameter andmodel output. Truncated Karhunen--Lo\\\\`{e}ve (KL) expansions are used todecompose the log-permeability field, as well as the resulting random pressurefield. We find that although very high-dimensional representations are neededto accurately represent the permeability field, especially in presence of smallcorrelation lengths, the pressure field is not very sensitive to high-order KLterms of the input parameter. Moreover, we find that the pressure field itselfcan be represented accurately using a KL expansion with a small number ofterms. These observations are used to guide a reduced-order modeling approachto accelerate computational studies of biotransport in tumors.',\n",
              " 'We present a distributed active subspace method for training surrogate modelsof complex physical processes with high-dimensional inputs and function valuedoutputs. Specifically, we represent the model output with a truncatedKarhunen-Lo\\\\`eve (KL) expansion, screen the structure of the input space withrespect to each KL mode via the active subspace method, and finally form anoverall surrogate model of the output by combining surrogates of individualoutput KL modes. To ensure scalable computation of the gradients of the outputKL modes, needed in active subspace discovery, we rely on adjoint-basedgradient computation. The proposed method combines benefits of active subspacemethods for input dimension reduction and KL expansions used for spectralrepresentation of the output field. We provide a mathematical framework for theproposed method and conduct an error analysis of the mixed KL active subspaceapproach. Specifically, we provide an error estimate that quantifies errors dueto active subspace projection and truncated KL expansion of the output. Wedemonstrate the numerical performance of the surrogate modeling approach withan application example from biotransport.',\n",
              " 'We develop a computational framework for D-optimal experimental design forPDE-based Bayesian linear inverse problems with infinite-dimensionalparameters. We follow a formulation of the experimental design problem thatremains valid in the infinite-dimensional limit. The optimal design is obtainedby solving an optimization problem that involves repeated evaluation of thelog-determinant of high-dimensional operators along with their derivatives.Forming and manipulating these operators is computationally prohibitive forlarge-scale problems. Our methods exploit the low-rank structure in the inverseproblem in three different ways, yielding efficient algorithms. Our mainapproach is to use randomized estimators for computing the D-optimal criterion,its derivative, as well as the Kullback--Leibler divergence from posterior toprior. Two other alternatives are proposed based on a low-rank approximation ofthe prior-preconditioned data misfit Hessian, and a fixed low-rankapproximation of the prior-preconditioned forward operator. Detailed erroranalysis is provided for each of the methods, and their effectiveness isdemonstrated on a model sensor placement problem for initial statereconstruction in a time-dependent advection-diffusion equation in two spacedimensions.',\n",
              " 'In this paper, we investigate potential biases in datasets used to make drugbinding predictions using machine learning. We investigate a recently publishedmetric called the Asymmetric Validation Embedding (AVE) bias which is used toquantify this bias and detect overfitting. We compare it to a slightly revisedversion and introduce a new weighted metric. We find that the new metrics allowto quantify overfitting while not overly limiting training data and producemodels with greater predictive value.',\n",
              " 'Tanglegrams are special graphs that consist of a pair of rooted binary treeswith the same number of leaves, and a perfect matching between the twoleaf-sets. These objects are of use in phylogenetics and are represented withstraightline drawings where the leaves of the two plane binary trees are on twoparallel lines and only the matching edges can cross. The tangle crossingnumber of a tanglegram is the minimum crossing number over all such drawingsand is related to biologically relevant quantities, such as the number of timesa parasite switched hosts.  Our main results for tanglegrams which parallel known theorems for crossingnumbers are as follows. The removal of a single matching edge in a tanglegramwith $n$ leaves decreases the tangle crossing number by at most $n-3$, and thisis sharp. Additionally, if $\\\\gamma(n)$ is the maximum tangle crossing number ofa tanglegram with $n$ leaves, we prove$\\\\frac{1}{2}\\\\binom{n}{2}(1-o(1))\\\\le\\\\gamma(n)<\\\\frac{1}{2}\\\\binom{n}{2}$. Further,we provide an algorithm for computing non-trivial lower bounds on the tanglecrossing number in $O(n^4)$ time. This lower bound may be tight, even fortanglegrams with tangle crossing number $\\\\Theta(n^2)$.',\n",
              " 'Non-central heavy ion collisions create an out-of-plane-extended participantzone that expands toward a more round state as the system evolves. The recentRHIC Beam Energy Scan at sqrt{s_{NN}} of 7.7, 11.5, and 39 GeV provide anopportunity to explore the energy dependence of the freeze out eccentricity.The new low energy data from STAR complements high statistics data sets atsqrt{s_{NN}} of 62.4 and 200 GeV. Hanbury-Brown-Twiss (HBT) interferometryallows to determine the size of pion emitting source regions. The dependence ofthe HBT radius parameters on azimuthal angle relative to the reaction planehave been extracted. These dependencies can be related to the freeze outeccentricity. The new results from STAR are consistent with a monotonicallydecreasing freeze out eccentricity and constrain any minimum, suggested bypreviously available data, to lie in the range between 11.5 and 39 GeV. Ofseveral models UrQMD appears to best predict the STAR and AGS data.',\n",
              " 'A Bayesian approach is used to estimate the covariance matrix of Gaussiandata. Ideas from Gaussian graphical models and model selection are used toconstruct a prior for the covariance matrix that is a mixture over alldecomposable graphs. For this prior the probability of each graph size isspecified by the user and graphs of equal size are assigned equal probability.Most previous approaches assume that all graphs are equally probable. We showempirically that the prior that assigns equal probability over graph sizesoutperforms the prior that assigns equal probability over all graphs, both inidentifying the correct decomposable graph and in more efficiently estimatingthe covariance matrix.',\n",
              " 'This paper develops a Nearly Autonomous Management and Control (NAMAC) systemfor advanced reactors. The development process of NAMAC is characterized by athree layer-layer architecture: knowledge base, the Digital Twin (DT)developmental layer, and the NAMAC operational layer. The DT is described as aknowledge acquisition system from the knowledge base for intended uses in theNAMAC system. A set of DTs with different functions is developed withacceptable performance and assembled according to the NAMAC operationalworkflow to furnish recommendations to operators. To demonstrate the capabilityof the NAMAC system, a case study is designed, where a baseline NAMAC isimplemented for operating a simulator of the Experimental Breeder Reactor IIduring a single loss of flow accident. When NAMAC is operated in the trainingdomain, it can provide reasonable recommendations that prevent the peak fuelcenterline temperature from exceeding a safety criterion.',\n",
              " 'The Nearly Autonomous Management and Control System (NAMAC) is acomprehensive control system that assists plant operations by furnishingcontrol recommendations to operators in a broad class of situations. This studyrefines a NAMAC system for making reasonable recommendations during complexloss-of-flow scenarios with a validated Experimental Breeder Reactor IIsimulator, digital twins improved by machine-learning algorithms, amulti-attribute decision-making scheme, and a discrepancy checker foridentifying unexpected recommendation effects. We assessed the performance ofeach NAMAC component, while we demonstrated and evaluated the capability ofNAMAC in a class of loss-of-flow scenarios.',\n",
              " 'Emerging Non-Volatile Memories (NVMs) are promising contenders for buildingfuture memory systems. On the other side, unlike DRAM systems, NVMs can retaindata even after power loss and thus enlarge the attack surface. While dataencryption and integrity verification have been proposed earlier for DRAMsystems, protecting and recovering secure memories becomes more challengingwith persistent memory. Specifically, security metadata, e.g., encryptioncounters and Merkle Tree data, should be securely persisted and recoveredacross system reboots and during recovery from crashes. Not persisting updatesto security metadata can lead to data inconsistency, in addition to serioussecurity vulnerabilities.  In this paper, we pioneer a new direction that explores persistency of bothMerkle Tree and encryption counters to enable secure recovery ofdata-verifiable and encrypted memory systems. To this end, we coin a newconcept that we call Persistent-Security. We discuss the requirements for suchpersistently secure systems, propose novel optimizations, and evaluate theimpact of the proposed relaxation schemes and optimizations on performance,resilience and recovery time. To the best of our knowledge, our paper is thefirst to discuss the persistence of security metadata in integrity-protectedNVM systems and provide corresponding optimizations. We define a set ofrelaxation schemes that bring trade-offs between performance and recovery timefor large capacity NVM systems. Our results show that our proposed design,Triad-NVM, can improve the throughput by an average of ~2x (relative to strictpersistence). Moreover, Triad-NVM maintains a recovery time of less than 4seconds for an 8TB NVM system (30.6 seconds for 64TB), which is ~3648x fasterthan a system without security metadata persistence.',\n",
              " 'The Linux shell is a command-line interpreter that provides users with acommand interface to the operating system, allowing them to perform a varietyof functions. Although very useful in building capabilities at the edge, theLinux shell can be exploited, giving adversaries a prime opportunity to usethem for malicious activities. With access to IoT devices, malware authors canabuse the Linux shell of those devices to propagate infections and launchlarge-scale attacks, e.g., DDoS. In this work, we provide a first look at shellcommands used in Linux-based IoT malware towards detection. We analyzemalicious shell commands found in IoT malware and build a neural network-basedmodel, ShellCore, to detect malicious shell commands. Namely, we collected alarge dataset of shell commands, including malicious commands extracted from2,891 IoT malware samples and benign commands collected from real-world networktraffic analysis and volunteered data from Linux users. Using conventionalmachine and deep learning-based approaches trained with term- andcharacter-level features, ShellCore is shown to achieve an accuracy of morethan 99% in detecting malicious shell commands and files (i.e., binaries).',\n",
              " 'Bonsai Merkle tree (BMT) is a widely used data structure for authenticatingdata/metadata in a secure computing system. However, the predominantlyrecursive andsequential nature of traditional BMT algorithms make themchallenging to implement with Field-Programmable Gate Array (FPGA) in modernheterogeneous computing platforms. In this work, we introduce HMT, ahardware-friendly implementation methodology for BMT that enables theverification and update processes to function independently, as well as savesadditional write-backs by making the update conditions more flexible comparedto previous algorithms. The methodology of HMT contributes both novel algorithmrevisions and innovative hardware techniques to implementing BMT. Our empiricalperformance measurements have demonstrated that HMT can achieve up to 7ximprovement in bandwidth and 4.5x reduction in latency over the baseline.',\n",
              " 'Processing In Memory (PIM) accelerators are promising architecture that canprovide massive parallelization and high efficiency in various applications.Such architectures can instantaneously provide ultra-fast operation overextensive data, allowing real-time performance in data-intensive workloads. Forinstance, Resistive Memory (ReRAM) based PIM architectures are widely known fortheir inherent dot-product computation capability. While the performance ofsuch architecture is essential, reliability and accuracy are also important,especially in mission-critical real-time systems. Unfortunately, the PIMarchitectures have a fundamental limitation in guaranteeing error-freeoperation. As a result, current methods must pay high implementation costs orperformance penalties to achieve reliable execution in the PIM accelerator. Inthis paper, we make a fundamental observation of this reliability limitation ofReRAM based PIM architecture. Accordingly, we propose a novel solution--FalutTolerant PIM or FAT-PIM, that can improve reliability for such systemssignificantly at a low cost. Our evaluation shows that we can improve the errortolerance significantly with only 4.9% performance cost and 3.9% storageoverhead.',\n",
              " \"Emerging Non-Volatile Memories (NVMs) bring a unique challenge to thesecurity community, namely persistent security. As NVM-based memories areexpected to restore their data after recovery, the security metadata must berecovered as well. However, persisting all affected security metadata on eachmemory write would significantly degrade performance and exacerbate the writeendurance problem. Moreover, recovery time can increase significantly (up tohours for practical memory sizes) when security metadata are not updatedstrictly. Counter trees are used in state-of-the-art commercial secureprocessors, e.g., Intel's Safe Guard Extension (SGX). Counter trees have aunique challenge due to the inability to recover the whole tree from leaves.Thus, to ensure recoverability, all updates to the tree must be persisted,which can be tens of additional writes on each write. The state-of-art scheme,Anubis, enables recoverability but incurs an additional write per cacheeviction, i.e., reduces lifetime to approximately half. Additionally, Anubisdegrades performance significantly in many cases. In this paper, we proposePhoenix, a practical novel scheme which relies on elegantly reproducing thecache content before a crash, however with minimal overheads. Our evaluationresults show that Phoenix reduces persisting security metadata overhead writesfrom 87\\\\% extra writes (for Anubis) to less than write-back compared to anencrypted system without recovery, thus improving the NVM lifetime by 2x.Overall Phoenix performance is better than the baseline, unlike Anubis whichadds 7.9\\\\% (max of 35\\\\%) performance overhead.\",\n",
              " \"Cache side channel attacks obtain victim cache line access footprint to infersecurity-critical information. Among them, cross-core attacks exploiting theshared last level cache are more threatening as their simplicity to set up andhigh capacity. Stateful approaches of detection-based mitigation observeprecise cache behaviors and protect specific cache lines that are suspected ofbeing attacked. However, their recording structures incur large storageoverhead and are vulnerable to reverse engineering attacks. Exploring theintrinsic non-determinate layout of a traditional Cuckoo filter, this paperproposes a space efficient Auto-Cuckoo filter to record access footprints,which succeed to decrease storage overhead and resist reverse engineeringattacks at the same time. With Auto-Cuckoo filter, we propose PiPoMonitor todetect \\\\textit{Ping-Pong patterns} and prefetch specific cache line tointerfere with adversaries' cache probes. Security analysis shows thePiPoMonitor can effectively mitigate cross-core attacks and the Auto-Cuckoofilter is immune to reverse engineering attacks. Evaluation results indicatePiPoMonitor has negligible impact on performance and the storage overhead isonly 0.37$\\\\%$, an order of magnitude lower than previous stateful approaches.\",\n",
              " 'The exponential growth of data has driven technology providers to develop newprotocols, such as cache coherent interconnects and memory semantic fabrics, tohelp users and facilities leverage advances in memory technologies to satisfythese growing memory and storage demands. Using these new protocols,fabric-attached memories (FAM) can be directly attached to a systeminterconnect and be easily integrated with a variety of processing elements(PEs). Moreover, systems that support FAM can be smoothly upgraded and allowmultiple PEs to share the FAM memory pools using well-defined protocols. Thesharing of FAM between PEs allows efficient data sharing, improves memoryutilization, reduces cost by allowing flexible integration of different PEs andmemory modules from several vendors, and makes it easier to upgrade the system.One promising use-case for FAMs is in High-Performance Compute (HPC) systems,where the underutilization of memory is a major challenge. However, adoptingFAMs in HPC systems brings new challenges. In addition to cost, flexibility,and efficiency, one particular problem that requires rethinking is virtualmemory support for security and performance. To address these challenges, thispaper presents decoupled access control and address translation (DeACT), anovel virtual memory implementation that supports HPC systems equipped withFAM. Compared to the state-of-the-art two-level translation approach, DeACTachieves speedup of up to 4.59x (1.8x on average) without compromisingsecurity.',\n",
              " 'The lack of security measures among the Internet of Things (IoT) devices andtheir persistent online connection gives adversaries a prime opportunity totarget them or even abuse them as intermediary targets in larger attacks suchas distributed denial-of-service (DDoS) campaigns. In this paper, we analyzeIoT malware and focus on the endpoints reachable on the public Internet, thatplay an essential part in the IoT malware ecosystem. Namely, we analyzeendpoints acting as dropzones and their targets to gain insights into theunderlying dynamics in this ecosystem, such as the affinity between thedropzones and their target IP addresses, and the different patterns amongendpoints. Towards this goal, we reverse-engineer 2,423 IoT malware samples andextract strings from them to obtain IP addresses. We further gather informationabout these endpoints from public Internet-wide scanners, such as Shodan andCensys. For the masked IP addresses, we examine the Classless Inter-DomainRouting (CIDR) networks accumulating to more than 100 million (78.2% of totalactive public IPv4 addresses) endpoints. Our investigation from four differentperspectives provides profound insights into the role of endpoints in IoTmalware attacks, which deepens our understanding of IoT malware ecosystems andcan assist future defenses.',\n",
              " 'Existing logic-locking attacks are known to successfully decrypt functionallycorrect key of a locked combinational circuit. It is possible to extend theseattacks to real-world Silicon-based Intellectual Properties (IPs, which aresequential circuits) through scan-chains by selectively initializing thecombinational logic and analyzing the responses. In this paper, we proposeSeqL, which achieves functional isolation and locks selective flip-flopfunctional-input/scan-output pairs, thus rendering the decrypted keyfunctionally incorrect. We conduct a formal study of the scan-locking problemand demonstrate automating our proposed defense on any given IP. We show thatSeqL hides functionally correct keys from the attacker, thereby increasing thelikelihood of the decrypted key being functionally incorrect. When tested onpipelined combinational benchmarks (ISCAS,MCNC), sequential benchmarks (ITC)and a fully-fledged RISC-V CPU, SeqL gave 100% resilience to a broad range ofstate-of-the-art attacks including SAT[1], Double-DIP[2], HackTest[3], SMT[4],FALL[5], Shift-and-Leak[6] and Multi-cycle attacks[7].',\n",
              " \"Intellectual Property (IP) theft is a serious concern for the integratedcircuit (IC) industry. To address this concern, logic locking countermeasuretransforms a logic circuit to a different one to obfuscate its inner details.The transformation caused by obfuscation is reversed only upon application ofthe programmed secret key, thus preserving the circuit's original function.This technique is known to be vulnerable to Satisfiability (SAT)-based attacks.But in order to succeed, SAT-based attacks implicitly assume a perfectlyreverse-engineered circuit, which is difficult to achieve in practice due toreverse engineering (RE) errors caused by automated circuit extraction. In thispaper, we analyze the effects of random circuit RE-errors on the success ofSAT-based attacks. Empirical evaluation on ISCAS, MCNC benchmarks as well as afully-fledged RISC-V CPU reveals that the attack success degrades exponentiallywith increase in the number of random RE-errors. Therefore, the adversarieseither have to equip RE-tools with near perfection or propose better SAT-basedattacks that can work with RE-imperfections.\",\n",
              " \"Recent work on stealing machine learning (ML) models from inference engineswith physical side-channel attacks warrant an urgent need for effectiveside-channel defenses. This work proposes the first $\\\\textit{fully-masked}$neural network inference engine design.  Masking uses secure multi-party computation to split the secrets into randomshares and to decorrelate the statistical relation of secret-dependentcomputations to side-channels (e.g., the power draw). In this work, weconstruct secure hardware primitives to mask $\\\\textit{all}$ the linear andnon-linear operations in a neural network. We address the challenge of maskinginteger addition by converting each addition into a sequence of XOR and ANDgates and by augmenting Trichina's secure Boolean masking style. We improve thetraditional Trichina's AND gates by adding pipelining elements for betterglitch-resistance and we architect the whole design to sustain a throughput of1 masked addition per cycle.  We implement the proposed secure inference engine on a Xilinx Spartan-6(XC6SLX75) FPGA. The results show that masking incurs an overhead of 3.5\\\\% inlatency and 5.9$\\\\times$ in area. Finally, we demonstrate the security of themasked design with 2M traces.\",\n",
              " 'Differential Power Analysis (DPA) has been an active area of research for thepast two decades to study the attacks for extracting secret information fromcryptographic implementations through power measurements and their defenses.Unfortunately, the research on power side-channels have so far predominantlyfocused on analyzing implementations of ciphers such as AES, DES, RSA, andrecently post-quantum cryptography primitives (e.g., lattices). Meanwhile,machine-learning, and in particular deep-learning applications are becomingubiquitous with several scenarios where the Machine Learning Models areIntellectual Properties requiring confidentiality. Expanding side-channelanalysis to Machine Learning Model extraction, however, is largely unexplored.  This paper expands the DPA framework to neural-network classifiers. First, itshows DPA attacks during inference to extract the secret model parameters suchas weights and biases of a neural network. Second, it proposes the$\\\\textit{first countermeasures}$ against these attacks by augmenting$\\\\textit{masking}$. The resulting design uses novel masked components such asmasked adder trees for fully-connected layers and masked Rectifier Linear Unitsfor activation functions. On a SAKURA-X FPGA board, experiments show that thefirst-order DPA attacks on the unprotected implementation can succeed with only200 traces and our protection respectively increases the latency and area-costby 2.8x and 2.3x.',\n",
              " 'Machine learning (ML) models can be trade secrets due to their developmentcost. Hence, they need protection against malicious forms of reverseengineering (e.g., in IP piracy). With a growing shift of ML to the edgedevices, in part for performance and in part for privacy benefits, the modelshave become susceptible to the so-called physical side-channel attacks.  ML being a relatively new target compared to cryptography poses the problemof side-channel analysis in a context that lacks published literature. The gapbetween the burgeoning edge-based ML devices and the research on adequatedefenses to provide side-channel security for them thus motivates our study.Our work develops and combines different flavors of side-channel defenses forML models in the hardware blocks. We propose and optimize the first defensebased on Boolean masking. We first implement all the masked hardware blocks. Wethen present an adder optimization to reduce the area and latency overheads.Finally, we couple it with a shuffle-based defense.  We quantify that the area-delay overhead of masking ranges from 5.4$\\\\times$to 4.7$\\\\times$ depending on the adder topology used and demonstrate first-orderside-channel security of millions of power traces. Additionally, the shufflecountermeasure impedes a straightforward second-order attack on our first-ordermasked implementation.',\n",
              " \"In this work, we provide an industry research view for approaching thedesign, deployment, and operation of trustworthy Artificial Intelligence (AI)inference systems. Such systems provide customers with timely, informed, andcustomized inferences to aid their decision, while at the same time utilizingappropriate security protection mechanisms for AI models. Additionally, suchsystems should also use Privacy-Enhancing Technologies (PETs) to protectcustomers' data at any time. To approach the subject, we start by introducingcurrent trends in AI inference systems. We continue by elaborating on therelationship between Intellectual Property (IP) and private data protection insuch systems. Regarding the protection mechanisms, we survey the security andprivacy building blocks instrumental in designing, building, deploying, andoperating private AI inference systems. For example, we highlight opportunitiesand challenges in AI systems using trusted execution environments combined withmore recent advances in cryptographic techniques to protect data in use.Finally, we outline areas of further development that require the globalcollective attention of industry, academia, and government researchers tosustain the operation of trustworthy AI inference systems.\",\n",
              " \"A field algebra is a ``non-commutative'' generalization of a vertex algebra.In this paper we develop foundations of the theory of field algebras.\",\n",
              " 'We consider inhomogeneous supersymmetric bilinear forms, i.e., forms that areneither even nor odd. We classify such forms up to dimension seven in the casewhen the restrictions of the form to the even and odd parts of the superspaceare nondegenerate. As an application, we introduce a new type of oscillator Liesuperalgebra.',\n",
              " 'Given a simple finite-dimensional Lie algebra and an automorphism of finiteorder, one defines the notion of a twisted toroidal Lie algebra. In this paper,we construct representations of twisted toroidal Lie algebras from twistedmodules over affine and lattice vertex algebras.',\n",
              " 'We give a short introduction to generalized vertex algebras, using the notionof polylocal fields. We construct a generalized vertex algebra associated to avector space h with a symmetric bilinear form. It contains as subalgebras alllattice vertex algebras of rank equal to dim h and all irreduciblerepresentations of these vertex algebras.',\n",
              " 'Vertex algebras provide an axiomatic algebraic description of the operatorproduct expansion (OPE) of chiral fields in 2-dimensional conformal fieldtheory. Vertex Lie algebras (= Lie conformal algebras) encode the singular partof the OPE, or, equivalently, the commutators of chiral fields. We discussgeneralizations of vertex algebras and vertex Lie algebras, which are relevantfor higher-dimensional quantum field theory.',\n",
              " 'We construct embeddings of $\\\\widehat{\\\\mathfrak{sl}}_2$ in lattice vertexalgebras by composing the Wakimoto realization with theFriedan-Martinec-Shenker bosonization. The Kac-Wakimoto hierarchy then givesrise to two new hierarchies of integrable, non-autonomous, non-linear partialdifferential equations. A new feature of our construction is that it works forany value of the central element of $\\\\widehat{\\\\mathfrak{sl}}_2$; that is, thelevel becomes a parameter in the equations.',\n",
              " 'Motivated by logarithmic conformal field theory and Gromov-Witten theory, weintroduce a notion of a twisted module of a vertex algebra under an arbitrary(not necessarily semisimple) automorphism. Its main feature is that the twistedfields involve the logarithm of the formal variable. We develop the theory ofsuch twisted modules and, in particular, derive a Borcherds identity andcommutator formula for them. We investigate in detail the examples of affineand Heisenberg vertex algebras.',\n",
              " 'The extended bigraded Toda hierarchy (EBTH) is an integrable system satisfiedby the total descendant potential of $\\\\mathbb{CP}^1$ with two orbifold points.We construct additional symmetries of the EBTH and describe explicitly theiraction on the Lax operator, wave operators, and tau-function of the hierarchy.In particular, we obtain infinitesimal symmetries of the EBTH that act on thetau-function as a subalgebra of the Virasoro algebra, generalizing those ofDubrovin and Zhang.',\n",
              " 'Given a non-semisimple automorphism $\\\\varphi$ of a vertex algebra $V$, thefields in a $\\\\varphi$-twisted $V$-module involve the logarithm of the formalvariable, and the action of the Virasoro operator $L_0$ on such module is notsemisimple. We construct examples of such modules and realize them explicitlyas Fock spaces when $V$ is generated by free fields. Specifically, we considerthe cases of symplectic fermions (odd superbosons), free fermions, and$\\\\beta\\\\gamma$-system (even superfermions). In each case, we determine theaction of the Virasoro algebra.',\n",
              " 'The extended bigraded Toda hierarchy (EBTH) is an integrable system satisfiedby the Gromov-Witten total descendant potential of $\\\\mathbb{CP}^1$ with twoorbifold points. We write a bilinear equation for the tau-function of the EBTHand derive Fay identities from it. We show that the action of Darbouxtransformations on the tau-function is given by vertex operators. As aconsequence, we obtain generalized Fay identities.',\n",
              " 'We prove that, for a Poisson vertex algebra V, the canonical injectivehomomorphism of the variational cohomology of V to its classical cohomology isan isomorphism, provided that V, viewed as a differential algebra, is analgebra of differential polynomials in finitely many differential variables.This theorem is one of the key ingredients in the computation of vertex algebracohomology. For its proof, we introduce the sesquilinear Hochschild andHarrison cohomology complexes and prove a vanishing theorem for the symmetricsesquilinear Harrison cohomology of the algebra of differential polynomials infinitely many differential variables.',\n",
              " 'We introduce and study the notion of a logarithmic vertex algebra, which is avertex algebra with logarithmic singularities in the operator product expansionof quantum fields; thus providing a rigorous formulation of the algebraicproperties of quantum fields in logarithmic conformal field theory. We developa framework that allows many results about vertex algebras to be extended tologarithmic vertex algebras, including in particular the Borcherds identity andKac Existence Theorem. Several examples are investigated in detail, and theyexhibit some unexpected new features that are peculiar to the logarithmic case.',\n",
              " 'We construct a canonical map from the Poisson vertex algebra cohomologycomplex to the differential Harrison cohomology complex, which restricts to anisomorphism on the top degree. This is an important step in the computation ofPoisson vertex algebra and vertex algebra cohomologies.',\n",
              " \"Conformal algebra is an axiomatic description of the operator productexpansion of chiral fields in conformal field theory. On the other hand, it isan adequate tool for the study of infinite-dimensional Lie algebras satisfyingthe locality property. The main examples of such Lie algebras are those``based'' on the punctured complex plane, like the Virasoro algebra and loopalgebras. In the present paper we develop a cohomology theory of conformalalgebras with coefficients in an arbitrary module. It possesses standardproperties of cohomology theories; for example, it describes extensions anddeformations. We offer explicit computations for most of the importantexamples.\",\n",
              " 'For any integral lattice $Q$, one can construct a vertex algebra $V_Q$ calleda lattice vertex algebra. If $\\\\sigma$ is an automorphism of $Q$ of finiteorder, it can be lifted to an automorphism of $V_Q$. In this paper we classifythe irreducible $\\\\sigma$-twisted $V_Q$-modules. We show that the category of$\\\\sigma$-twisted $V_Q$-modules is a semisimple abelian category with finitelymany isomorphism classes of simple objects.',\n",
              " 'One of the algebraic structures that has emerged recently in the study of theoperator product expansions of chiral fields in conformal field theory is thatof a Lie conformal algebra. A Lie pseudoalgebra is a generalization of thenotion of a Lie conformal algebra for which C[\\\\partial] is replaced by theuniversal enveloping algebra H of a finite-dimensional Lie algebra. The finite(i.e., finitely generated over H) simple Lie pseudoalgebras were classified inour previous work. The present paper is the second in our series onrepresentation theory of simple Lie pseudoalgebras. In the first paper weshowed that any finite irreducible module over a simple Lie pseudoalgebra oftype W or S is either an irreducible tensor module or the kernel of thedifferential in a member of the pseudo de Rham complex. In the present paper weestablish a similar result for Lie pseudoalgebras of type K, with the pseudo deRham complex replaced by a certain reduction called the contact pseudo de Rhamcomplex. This reduction in the context of contact geometry was discovered byRumin.',\n",
              " 'Simple, or Kleinian, singularities are classified by Dynkin diagrams of typeADE. Let g be the corresponding finite-dimensional Lie algebra, and W its Weylgroup. The set of g-invariants in the basic representation of the affineKac-Moody algebra g^ is known as a W-algebra and is a subalgebra of theHeisenberg vertex algebra F. Using period integrals, we construct an analyticcontinuation of the twisted representation of F. Our construction yields aglobal object, which may be called a W-twisted representation of F. Our mainresult is that the total descendant potential of the singularity, introduced byGivental, is a highest weight vector for the W-algebra.',\n",
              " 'Every isometry $\\\\sigma$ of a positive-definite even lattice $Q$ can be liftedto an automorphism of the lattice vertex algebra $V_Q$. An important problem invertex algebra theory and conformal field theory is to classify therepresentations of the $\\\\sigma$-invariant subalgebra $V_Q^\\\\sigma$ of $V_Q$,known as an orbifold. In the case when $\\\\sigma$ is an isometry of $Q$ of ordertwo, we classify the irreducible modules of the orbifold vertex algebra$V_Q^\\\\sigma$ and identify them as submodules of twisted or untwisted$V_Q$-modules. The examples where $Q$ is a root lattice and $\\\\sigma$ is aDynkin diagram automorphism are presented in detail.',\n",
              " 'To a positive-definite even lattice $Q$, one can associate the lattice vertexalgebra $V_Q$, and any automorphism $\\\\sigma$ of $Q$ lifts to an automorphism of$V_Q$. In this paper, we investigate the orbifold vertex algebra $V_Q^\\\\sigma$,which consists of the elements of $V_Q$ fixed under $\\\\sigma$, in the case when$\\\\sigma$ has prime order. We describe explicitly the irreducible$V_Q^\\\\sigma$-modules, compute their characters, and determine the modulartransformations of characters. As an application, we find the asymptotic andquantum dimensions of all irreducible $V_Q^\\\\sigma$-modules. We consider indetail the cases when the order of $\\\\sigma$ is $2$ or $3$, as well as the caseof permutation orbifolds.',\n",
              " 'Psychiatric illnesses are often associated with multiple symptoms, whoseseverity must be graded for accurate diagnosis and treatment. This grading isusually done by trained clinicians based on human observations and judgmentsmade within doctor-patient sessions. Current research provides sufficientreason to expect that the human voice may carry biomarkers or signatures ofmany, if not all, these symptoms. Based on this conjecture, we explore thepossibility of objectively and automatically grading the symptoms ofpsychiatric illnesses with reference to various standard psychiatric ratingscales. Using acoustic data from several clinician-patient interviews withinhospital settings, we use non-parametric models to learn and predict therelations between symptom-ratings and voice. In the process, we show thatdifferent articulatory-phonetic units of speech are able to capture the effectsof different symptoms differently, and use this to establish a plausiblemethodology that could be employed for automatically grading psychiatricsymptoms for clinical purposes.',\n",
              " \"Proper orthogonal decomposition (POD) allows reduced-order modeling ofcomplex dynamical systems at a substantial level, while maintaining a highdegree of accuracy in modeling the underlying dynamical systems. Advances inmachine learning algorithms enable learning POD-based dynamics from data andmaking accurate and fast predictions of dynamical systems. In this paper, weleverage the recently proposed heavy-ball neural ODEs (HBNODEs) [Xia et al.NeurIPS, 2021] for learning data-driven reduced-order models (ROMs) in the PODcontext, in particular, for learning dynamics of time-varying coefficientsgenerated by the POD analysis on training snapshots generated from solving fullorder models. HBNODE enjoys several practical advantages for learning POD-basedROMs with theoretical guarantees, including 1) HBNODE can learn long-termdependencies effectively from sequential observations and 2) HBNODE iscomputationally efficient in both training and testing. We compare HBNODE withother popular ROMs on several complex dynamical systems, including the vonK\\\\'{a}rm\\\\'{a}n Street flow, the Kurganov-Petrova-Popov equation, and theone-dimensional Euler equations for fluids modeling.\",\n",
              " 'Learning neural ODEs often requires solving very stiff ODE systems, primarilyusing explicit adaptive step size ODE solvers. These solvers arecomputationally expensive, requiring the use of tiny step sizes for numericalstability and accuracy guarantees. This paper considers learning neural ODEsusing implicit ODE solvers of different orders leveraging proximal operators.The proximal implicit solver consists of inner-outer iterations: the inneriterations approximate each implicit update step using a fast optimizationalgorithm, and the outer iterations solve the ODE system over time. Theproximal implicit ODE solver guarantees superiority over explicit solvers innumerical stability and computational efficiency. We validate the advantages ofproximal implicit solvers over existing popular neural ODE solvers on variouschallenging benchmark tasks, including learning continuous-depth graph neuralnetworks and continuous normalizing flows.',\n",
              " 'The search for meaningful structure in biological data has relied oncutting-edge advances in computational technology and data science methods.However, challenges arise as we push the limits of scale and complexity inbiological problems. Innovation in massively parallel, classical computinghardware and algorithms continues to address many of these challenges, butthere is a need to simultaneously consider new paradigms to circumvent currentbarriers to processing speed. Accordingly, we articulate a view towards quantumcomputation and quantum information science, where algorithms have demonstratedpotential polynomial and exponential computational speedups in certainapplications, such as machine learning. The maturation of the field of quantumcomputing, in hardware and algorithm development, also coincides with thegrowth of several collaborative efforts to address questions across length andtime scales, and scientific disciplines. We use this coincidence to explore thepotential for quantum computing to aid in one such endeavor: the merging ofinsights from genetics, genomics, neuroimaging and behavioral phenotyping. Byexamining joint opportunities for computational innovation across fields, wehighlight the need for a common language between biological data analysis andquantum computing. Ultimately, we consider current and future prospects for theemployment of quantum computing algorithms in the biological sciences.',\n",
              " 'Advanced large language models like ChatGPT have gained considerableattention recently, including among students. However, while the debate onChatGPT in academia is making waves, more understanding is needed amonglecturers and teachers on how students use and perceive ChatGPT. To addressthis gap, we analyzed the content on ChatGPT available on TikTok in February2023. TikTok is a rapidly growing social media platform popular amongindividuals under 30. Specifically, we analyzed the content of the 100 mostpopular videos in English tagged with #chatgpt, which collectively garneredover 250 million views. Most of the videos we studied promoted the use ofChatGPT for tasks like writing essays or code. In addition, many videosdiscussed AI detectors, with a focus on how other tools can help to transformChatGPT output to fool these detectors. This also mirrors the discussion amongeducators on how to treat ChatGPT as lecturers and teachers in teaching andgrading. What is, however, missing from the analyzed clips on TikTok are videosthat discuss ChatGPT producing content that is nonsensical or unfaithful to thetraining data.',\n",
              " 'In this paper, a new Volt/Var Control (VVC) scheme is proposed to facilitatethe coordination between the conventional VVC devices and the new smart PVinverters to provide an effective voltage control on a system with high PVpenetration. The proposed scheme decomposes the problem into two levels. Thefirst level uses Load Tap Changer (LTC) and Voltage Regulators (VRs) to adjustthe voltage level on the circuit to keep the voltages along the circuit withinthe desired range. The second level determines Var support needed from smartinverters to smooth the fast voltage variations while providing effective powerfactor correction to keep the power losses at minimum. The case study showsthat the proposed VVC method is very effective in maintaining acceptablevoltages on the system under various operating conditions while meeting theoperational constrains. The results also show the computational efficiency ofthe method.',\n",
              " \"Distribution system integrated community microgrids (CMGs) can restore loadsduring extended outages. The CMG is challenged with limited resourceavailability, absence of a robust grid-support, and demand-supply uncertainty.To address these challenges, this paper proposes a three-stage hierarchicalmulti-timescale framework for scheduling and real-time (RT) dispatch of CMGs.The CMG's ability to dynamically expand its boundary to support the neighboringgrid sections is also considered. The first stage solves a stochastic day-ahead(DA) scheduling problem to obtain referral plans for optimal resourcerationing. The intermediate near real-time scheduling stage updates the DAschedule closer to the dispatch time, followed by the RT dispatch stage. Theproposed methodology is validated via numerical simulations on a modified IEEE123-bus system, which shows superior performance in terms of RT load suppliedunder different forecast error cases, outage duration scenarios, and againstthe traditionally used two-stage approach.\",\n",
              " 'This paper presents a smart meter phase identification algorithm for twocases: meter-phase-label-known and meter-phase-label-unknown. To improve theidentification accuracy, a data segmentation method is proposed to exclude datasegments that are collected when the voltage correlation between smart meterson the same phase are weakened. Then, using the selected data segments, ahierarchical clustering method is used to calculate the correlation distancesand cluster the smart meters. If the phase labels are unknown, aConnected-Triple-based Similarity (CTS) method is adapted to further improvethe phase identification accuracy of the ensemble clustering method. Themethods are developed and tested on both synthetic and real feeder data sets.Simulation results show that the proposed phase identification algorithmoutperforms the state-of-the-art methods in both accuracy and robustness.',\n",
              " 'This paper focuses on the effective use of smart inverters for Volt/Varcontrol (VVC) on a distribution system. New smart inverters offer Var supportcapability but for their effective use they need to be coordinated withexisting Volt/Var schemes. A new VVC scheme is proposed to facilitate suchcoordination. The proposed scheme decomposes the problem into two levels. Thefirst level uses Load Tap Changer (LTC) and Voltage Regulators (VRs) andcoordinates their control with smart inverters to adjust the voltage level onthe circuit to keep the voltages along the circuit within the desired range.The second level determines Var support needed from smart inverters to minimizethe overall power loss in the circuit. The results of the supervisory controlare sent to the devices which have their local controllers. To avoid frequentdispatch, smart inverters are supervised by shifting their Volt/Varcharacteristics as needed. This allows for the smart inverters to operate closeto their optimal control while meeting the limited communication requirementson a distribution system. A case study using the IEEE 34 bus system shows theeffectiveness of this supervisory control scheme compared to traditionalvolt/var schemes.',\n",
              " \"Increasing penetration of Photovoltaic (PV) generation brings an opportunity,and sometimes necessity, for this new resource to provide ancillary servicessuch as frequency support. Recent efforts toward this goal focused mainly onthe large-scale PV plants with identical subsystems but cannot handledistributed PV systems with diversities. Therefore, directly applying them todistributed PV requires repetitive control design for each unit, and thusimplies huge design efforts. In this paper, we propose a novel frequencysupport control scheme focusing on the distributed PV to overcome the abovelimitation. Considering the diversities, we first derive a reduced-orderaggregate model to represent the overall dynamic behaviors of a group ofdistributed PV. Using this model, we can then design one single frequencysupport controller for a large number of distributed PV without excessivedesign computational efforts. This controller aims at controlling the PV totaloutput to provide frequency support to the grid. We also supplement it with theproposed inversion method to obtain individual PV's control signals from theaggregate one. The proposed reduced-order aggregate model is validated againsta group of distributed PV systems represented by the detailed nonlinear models.We also demonstrate the effectiveness of proposed control scheme, includingboth controller and the inversion method, through time-domain simulations usinga standard test system.\",\n",
              " 'This paper presents a load switching group based energy management system(LSG-EMS) for operating microgrids on a distribution feeder powered by one ormultiple grid-forming distributed energy resources. Loads on a distributionfeeder are divided into load switching groups that can be remotely switched onand off. The LSG-EMS algorithm, formulated as a mixed-integer linearprogramming (MILP) problem, has an objective function of maximizing the servedloads while minimizing the total number of switching actions. A new set oftopology constraints are developed for allowing multiple microgrids to beformed on the feeder and selecting the optimal supply path. Customer comfort isaccounted for by maximizing the supply duration in the customer preferredservice period and enforcing a minimum service duration. The proposed method isdemonstrated on a modified IEEE 33-bus system using actual customer data.Simulation results show that the LSG-EMS successfully coordinates multiplegrid-forming sources by selecting an optimal supply topology that maximizes thesupply period of both the critical and noncritical loads while minimizingcustomer service interruptions in the service restoration process.',\n",
              " 'Distribution system integrated community microgrids (CMGs) can partake inrestoring loads during extended duration outages. At such times, the CMG ischallenged with limited resource availability, absence of robust grid support,and heightened demand-supply uncertainty. This paper proposes a secure andadaptive three-stage hierarchical multi-timescale framework for scheduling andreal-time (RT) dispatch of CMGs with hybrid PV systems to address thesechallenges. The framework enables the CMG to dynamically expand its boundary tosupport the neighboring grid sections and is adaptive to the changing forecasterror impacts. The first stage solves a stochastic extended duration scheduling(EDS) problem to obtain referral plans for optimal resource rationing. Theintermediate near-real-time (NRT) scheduling stage updates the EDS schedulecloser to the dispatch time using newly obtained forecasts, followed by the RTdispatch stage. To make the dispatch decisions more secure and robust againstforecast errors, a novel concept called delayed recourse is proposed. Themethodology is evaluated via numerical simulations on a modified IEEE 123-bussystem and validated using OpenDSS/hardware-in-loop simulations. The resultsshow superior performance in maximizing load supply and continuous secure CMGoperation under numerous operating scenarios.',\n",
              " 'This paper presents a novel 2-stage microgrid unit commitment (Microgrid-UC)algorithm considering cold-load pickup (CLPU) effects, three-phase loadbalancing requirements, and feasible reconfiguration options. Microgrid-UCschedules the operation of switches, generators, battery energy storagesystems, and demand response resources to supply 3-phase unbalanced loads in anislanded microgrid for multiple days. A performance-based CLPU model isdeveloped to estimate additional energy needs of CLPU so that CLPU can beformulated into the traditional 2-stage UC scheduling process. A per-phasedemand response budget term is added to the 1st stage UC objective function tomeet 3-phase load unbalance limits. To reduce computational complexity in the1st stage UC, we replace the spanning tree method with a feasiblereconfiguration topology list method. The proposed algorithm is developed on amodified IEEE 123-bus system and tested on the real-time simulation testbedusing actual load and PV data. Simulation results show that Microgrid-UCsuccessfully accounts for CLPU, phase imbalance, and feeder reconfigurationrequirements.',\n",
              " 'Forming a microgrid on a distribution system with large scale outage after asevere weather event is emerging as a viable solution to improve resiliency atthe distribution level. This option becomes more attractive when thedistribution system has high levels of distributed PV. The management of suchfeeder-level microgrid has however many challenges, such as limited resourcesthat can be deployed on the feeder quickly, and the limited real-timemonitoring and control on the distribution system. Effective use of thedistributed PV is also challenging as they are not monitored and controlled. Tohandle these challenges, the paper proposes a 2-stage hierarchical energymanagement scheme to securely operate these feeder level micorgrids. The firststage of the scheme solves a sequential rolling optimization problem tooptimally schedule the main resources (such as a mobile diesel generator andbattery storage unit). The second stage adopts a dispatching scheme for themain resources to adjust the stage-1 set-points closer to real- time. Theproposed scheme has unique features to assure that the scheme is robust underhighly varying operating conditions with limited system observability: (i) aninnovative PV forecast error adjustment and a dynamic reserve adjustment schemeto handle the extreme uncertainty on PV power output, and (ii) an intelligentfuel management scheme to assure that the resources are utilized optimally overthe multiple days of the restoration period. The proposed algorithm is testedon sample system with real-time data. The results show that the proposed schemeperforms well in maximizing service to loads by effective use of all theresources and by properly taking into account the challenging operatingconditions.',\n",
              " 'The amount of sun cast on roads and parking lots determines the chargingopportunities for solar vehicles and impacts the efficiency of conventionalvehicles. Estimates of solar energy potential on urban surfaces to assessparking and driving conditions need to account for the shadows cast bysurrounding trees and buildings. However, though existing GIS tools cancalculate solar potential on surfaces that have buildings and trees, thesetools do not estimate the conditions beneath trees and do not consider theseasonal changes in deciduous trees. We introduce a new approach to addressthese factors using pixel substitution and a light penetration factor. In thispaper, we describe how to integrate these techniques into a workflow forcomputing solar potential estimates for parking and driving conditions. Wedemonstrate the methodology in an urban setting in North Carolina that includesa mixture of urban structures and trees. We provide code samples so that thisworkflow is easily repeatable. The solar maps produced with our method are auseful resource for planning solar vehicle parking and routing, and identifyingshaded conditions for conventional vehicles.',\n",
              " 'This paper analyzes the potential effects of connected and automated vehicleson saturation headway and capacity at signalized intersections. A signalizedintersection is created in Vissim as a testbed, where four vehicle types aremodeled and tested: (I) human-driven vehicles (HVs), (II) connected vehicles(CVs), (III) automated vehicles (AVs), and (IV) connected automated vehicles(CAVs). Various scenarios are defined based on different market penetrationrates of these four vehicle types. AVs are assumed to move more cautiouslycompared to human drivers. CVs and CAVs are supposed to receive informationabout the future state of traffic lights and adjust their speeds to avoidstopping at the intersection. As a result, their movements are expected to besmoother with a lower number of stops. The effects of these vehicle types inmixed traffic are investigated in terms of saturation headway, capacity, traveltime, delay, and queue length in different lane groups of an intersection. APython script code developed by Vissim is used to provide the communicationbetween the signal controller and CVs and CAVs to adjust their speedsaccordingly. The results show that increasing CV and CAV market penetrationrate reduces saturation headway and consequently increases capacity atsignalized intersections. On the other hand, increasing the AV marketpenetration rate deteriorates traffic operations. Results also indicate thatthe highest increase (80%) and decrease (20%) in lane group capacity areobserved, respectively, in a traffic stream of 100% CAVs and 100% AVs.',\n",
              " 'The expansion of artificial intelligence (AI) and autonomous systems hasshown the potential to generate enormous social good while also raising seriousethical and safety concerns. AI technology is increasingly adopted intransportation. A survey of various in-vehicle technologies found thatapproximately 64% of the respondents used a smartphone application to assistwith their travel. The top-used applications were navigation and real-timetraffic information systems. Among those who used smartphones during theircommutes, the top-used applications were navigation and entertainment. There isa pressing need to address relevant social concerns to allow for thedevelopment of systems of intelligent agents that are informed and cognizant ofethical standards. Doing so will facilitate the responsible integration ofthese systems in society. To this end, we have applied Multi-Criteria DecisionAnalysis (MCDA) to develop a formal Multi-Attribute Impact Assessment (MAIA)questionnaire for examining the social and ethical issues associated with theuptake of AI. We have focused on the domain of autonomous vehicles (AVs)because of their imminent expansion. However, AVs could serve as a stand-in forany domain where intelligent, autonomous agents interact with humans, either onan individual level (e.g., pedestrians, passengers) or a societal level.',\n",
              " \"Blended courses have become the norm in post-secondary education.Universities use large-scale learning management systems to manage classcontent. Instructors deliver readings, lectures, and office hours online;students use intelligent tutors, web forums, and online submission systems; andclasses communicate via web forums. These online tools allow students to formnew social networks or bring social relationships online. They also allow us tocollect data on students' social relationships. In this paper we report on ourresearch on community formation in blended courses based on online foruminteractions. We found that it was possible to group students into communitiesusing standard community detection algorithms via their posts and replystructure and that the students' grades are significantly correlated with theirclosest peers.\",\n",
              " \"In this paper, we compare predictive models for students' final performancein a blended course using a set of generic features collected from the firstsix weeks of class. These features were extracted from students' onlinehomework submission logs as well as other online actions. We compare theeffectiveness of 5 different ML algorithms (SVMs, Support Vector Regression,Decision Tree, Naive Bayes and K-Nearest Neighbor). We found that SVMsoutperform other models and improve when compared to the baseline. This studydemonstrates feasible implementations for predictive models that rely on commondata from blended courses that can be used to monitor students' progress and totailor instruction.\",\n",
              " \"Online tools provide unique access to research students' study habits andproblem-solving behavior. In MOOCs, this online data can be used to informinstructors and to provide automatic guidance to students. However, thesetechniques may not apply in blended courses with face to face and onlinecomponents. We report on a study of integrated user-system interaction logsfrom 3 computer science courses using four online systems: LMS, forum, versioncontrol, and homework system. Our results show that students rarely work acrossplatforms in a single session, and that final class performance can bepredicted from students' system use.\",\n",
              " \"Students' interactions with online tools can provide us with insights intotheir study and work habits. Prior research has shown that these habits, evenas simple as the number of actions or the time spent on online platforms candistinguish between the higher performing students and low-performers. Thesehabits are also often used to predict students' performance in classes. One keyfeature of these actions that is often overlooked is how and when the studentstransition between different online platforms. In this work, we study sequencesof student transitions between online tools in blended courses and identifywhich habits make the most difference between the higher and lower performinggroups. While our results showed that most of the time students focus on asingle tool, we were able to find patterns in their transitions todifferentiate high and low performing groups. These findings can helpinstructors to provide procedural guidance to the students, as well as toidentify harmful habits and make timely interventions.\",\n",
              " \"Open-ended programming increases students' motivation by allowing them tosolve authentic problems and connect programming to their own interests.However, such open-ended projects are also challenging, as they often encouragestudents to explore new programming features and attempt tasks that they havenot learned before. Code examples are effective learning materials for studentsand are well-suited to supporting open-ended programming. However, there islittle work to understand how novices learn with examples during open-endedprogramming, and few real-world deployments of such tools. In this paper, weexplore novices' learning barriers when interacting with code examples duringopen-ended programming. We deployed Example Helper, a tool that offersgalleries of code examples to search and use, with 44 novice students in anintroductory programming classroom, working on an open-ended project in Snap.We found three high-level barriers that novices encountered when usingexamples: decision, search and integration barriers. We discuss how thesebarriers arise and design opportunities to address them.\",\n",
              " 'Classroom dashboards are designed to help instructors effectively orchestrateclassrooms by providing summary statistics, activity tracking, and otherinformation. Existing dashboards are generally specific to an LMS or platformand they generally summarize individual work, not group behaviors. However, CScourses typically involve constellations of tools and mix on- and offlinecollaboration. Thus, cross-platform monitoring of individuals and teams isimportant to develop a full picture of the class. In this work, we describe ourwork on Concert, a data integration platform that collects data about studentactivities from several sources such as Piazza, My Digital Hand, and GitHub anduses it to support classroom monitoring through analysis and visualizations. Wediscuss team visualizations that we have developed to support effective groupmanagement and to help instructors identify teams in need of intervention.',\n",
              " \"Knowledge tracing (KT) models are a popular approach for predicting students'future performance at practice problems using their prior attempts. Though manyinnovations have been made in KT, most models including the state-of-the-artDeep KT (DKT) mainly leverage each student's response either as correct orincorrect, ignoring its content. In this work, we propose Code-based DeepKnowledge Tracing (Code-DKT), a model that uses an attention mechanism toautomatically extract and select domain-specific code features to extend DKT.We compared the effectiveness of Code-DKT against Bayesian and Deep KnowledgeTracing (BKT and DKT) on a dataset from a class of 50 students attempting tosolve 5 introductory programming assignments. Our results show that Code-DKTconsistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, acomparable improvement to other state-of-the-art domain-general KT models overDKT. Finally, we analyze problem-specific performance through a set of casestudies for one assignment to demonstrate when and how code features improveCode-DKT's predictions.\",\n",
              " 'Based on strategy-awareness (knowing which problem-solving strategy to use)and time-awareness (knowing when to use it), students are categorized into Rote(neither type of awareness), Dabbler (strategy-aware only) or Selective (bothtypes of awareness). It was shown that Selective is often significantly moreprepared for future learning than Rote and Dabbler (Abdelshiheed et al., 2020).In this work, we explore the impact of explicit strategy instruction on Roteand Dabbler students across two domains: logic and probability. During thelogic instruction, our logic tutor handles both Forward-Chaining (FC) andBackward-Chaining (BC) strategies, with FC being the default; the Experimentalcondition is taught how to use BC via worked examples and when to use it viaprompts. Six weeks later, all students are trained on a probability tutor thatsupports BC only. Our results show that Experimental significantly outperformsControl in both domains, and Experimental Rote catches up with Selective.',\n",
              " 'Deductive domains are typical of many cognitive skills in that no singleproblem-solving strategy is always optimal for solving all problems. It wasshown that students who know how and when to use each strategy (StrTime)outperformed those who know neither and stick to the default strategy(Default). In this work, students were trained on a logic tutor that supports adefault forward-chaining and a backward-chaining (BC) strategy, then aprobability tutor that only supports BC. We investigated three types ofinterventions on teaching the Default students how and when to use whichstrategy on the logic tutor: Example, Nudge and Presented. Meanwhile, StrTimestudents received no interventions. Overall, our results show that Nudgeoutperformed their Default peers and caught up with StrTime on both tutors.',\n",
              " 'Metacognitive skills have been commonly associated with preparation forfuture learning in deductive domains. Many researchers have regarded strategy-and time-awareness as two metacognitive skills that address how and when to usea problem-solving strategy, respectively. It was shown that students who areboth strategy-and time-aware (StrTime) outperformed their nonStrTime peersacross deductive domains. In this work, students were trained on a logic tutorthat supports a default forward-chaining (FC) and a backward-chaining (BC)strategy. We investigated the impact of mixing BC with FC on teaching strategy-and time-awareness for nonStrTime students. During the logic instruction, theexperimental students (Exp) were provided with two BC worked examples and someproblems in BC to practice how and when to use BC. Meanwhile, their control(Ctrl) and StrTime peers received no such intervention. Six weeks later, allstudents went through a probability tutor that only supports BC to evaluatewhether the acquired metacognitive skills are transferred from logic. Ourresults show that on both tutors, Exp outperformed Ctrl and caught up withStrTime.',\n",
              " \"In this work, we investigate how two factors, metacognitive skills andmotivation, would impact student learning across domains. More specifically,our primary goal is to identify the critical, yet robust, interaction patternsof these two factors that would contribute to students' performance in learninglogic first and then their performance on a subsequent new domain, probability.We are concerned with two types of metacognitive skills: strategy-awareness andtime-awareness, that is, which problem-solving strategy to use and when to useit. Our data were collected from 495 participants across three consecutivesemesters, and our results show that the only students who consistentlyoutperform their peers across both domains are those who are not only highlymotivated but also strategy-aware and time-aware.\",\n",
              " \"One fundamental goal of learning is preparation for future learning (PFL) andbeing able to extend acquired skills and problem-solving strategies todifferent domains and environments. While substantial research has shown thatPFL can be accelerated by obtaining metacognitive skills or influenced by theindividual's motivation, no prior work investigated whether the interaction ofthe two factors could assess students' competency for PFL. In this chapter, wetackle this research question in one type of highly interactive e-learningenvironment, intelligent tutoring systems. More specifically, we investigatewhether the combination of metacognitive skills and motivation would assessstudents' learning abilities in logic, and their competence to extend theseabilities to a subsequent domain, probability.\",\n",
              " \"This work compares two approaches to provide metacognitive interventions andtheir impact on preparing students for future learning across IntelligentTutoring Systems (ITSs). In two consecutive semesters, we conducted twoclassroom experiments: Exp. 1 used a classic artificial intelligence approachto classify students into different metacognitive groups and provide staticinterventions based on their classified groups. In Exp. 2, we leveraged DeepReinforcement Learning (DRL) to provide adaptive interventions that considerthe dynamic changes in the student's metacognitive levels. In both experiments,students received these interventions that taught how and when to use abackward-chaining (BC) strategy on a logic tutor that supports a defaultforward-chaining strategy. Six weeks later, we trained students on aprobability tutor that only supports BC without interventions. Our results showthat adaptive DRL-based interventions closed the metacognitive skills gapbetween students. In contrast, static classifier-based interventions onlybenefited a subset of students who knew how to use BC in advance. Additionally,our DRL agent prepared the experimental students for future learning bysignificantly surpassing their control peers on both ITSs.\",\n",
              " 'In deductive domains, three metacognitive knowledge types in ascending orderare declarative, procedural, and conditional learning. This work leverages DeepReinforcement Learning (DRL) in providing adaptive metacognitive interventionsto bridge the gap between the three knowledge types and prepare students forfuture learning across Intelligent Tutoring Systems (ITSs). Students receivedthese interventions that taught how and when to use a backward-chaining (BC)strategy on a logic tutor that supports a default forward-chaining strategy.Six weeks later, we trained students on a probability tutor that only supportsBC without interventions. Our results show that on both ITSs, DRL bridged themetacognitive knowledge gap between students and significantly improved theirlearning performance over their control peers. Furthermore, the DRL policyadapted to the metacognitive development on the logic tutor across declarative,procedural, and conditional students, causing their strategic decisions to bemore autonomous.',\n",
              " \"The large-scale online management systems (e.g. Moodle), online web forums(e.g. Piazza), and online homework systems (e.g. WebAssign) have been widelyused in the blended courses recently. Instructors can use these systems todeliver class content and materials. Students can communicate with theclassmates, share the course materials, and discuss the course questions viathe online forums. With the increased use of the online systems, a large amountof students' interaction data has been collected. This data can be used toanalyze students' learning behaviors and predict students' learning outcomes.In this work, we collected students' interaction data in three differentblended courses. We represented the data as directed graphs and investigatedthe correlation between the social graph properties and students' final grades.Our results showed that in all these classes, students who asked more answersand received more feedbacks on the forum tend to obtain higher grades. Thesignificance of this work is that we can use the results to encourage studentsto participate more in forums to learn the class materials better; we can alsobuild a predictive model based on the social metrics to show us low performingstudents early in the semester.\",\n",
              " \"Blended courses that mix in-person instruction with online platforms areincreasingly popular in secondary education. These tools record a rich amountof data on students' study habits and social interactions. Prior research hasshown that these metrics are correlated with students' performance in face toface classes. However, predictive models for blended courses are still limitedand have not yet succeeded at early prediction or cross-class predictions evenfor repeated offerings of the same course.  In this work, we use data from two offerings of two different undergraduatecourses to train and evaluate predictive models on student performance basedupon persistent student characteristics including study habits and socialinteractions. We analyze the performance of these models on the same offering,on different offerings of the same course, and across courses to see how wellthey generalize. We also evaluate the models on different segments of thecourses to determine how early reliable predictions can be made. This worktells us in part how much data is required to make robust predictions and howcross-class data may be used, or not, to boost model performance. The resultsof this study will help us better understand how similar the study habits,social activities, and the teamwork styles are across semesters for students ineach performance category. These trained models also provide an avenue toimprove our existing support platforms to better support struggling studentsearly in the semester with the goal of providing timely intervention.\",\n",
              " \"Teamwork, often mediated by version control systems such as Git and ApacheSubversion (SVN), is central to professional programming. As a consequence,many colleges are incorporating both collaboration and online developmentenvironments into their curricula even in introductory courses. In thisresearch, we collected GitHub logs from two programming projects in twoofferings of a CS2 Java programming course for computer science majors.Students worked in pairs for both projects (one optional, the other mandatory)in each year. We used the students' GitHub history to classify the studentteams into three groups, collaborative, cooperative, or solo-submit, based onthe division of labor. We then calculated different metrics for students'teamwork including the total number and the average number of commits indifferent parts of the projects and used these metrics to predict the students'teamwork style. Our findings show that we can identify the students' teamworkstyle automatically from their submission logs. This work helps us to betterunderstand novices' habits while using version control systems. These habitscan identify the harmful working styles among them and might lead to thedevelopment of automatic scaffolds for teamwork and peer support in the future.\",\n",
              " 'Within intelligent tutoring systems, considerable research has investigatedhints, including how to generate data-driven hints, what hint content topresent, and when to provide hints for optimal learning outcomes. However, lessattention has been paid to how hints are presented. In this paper, we propose anew hint delivery mechanism called \"Assertions\" for providing unsolicited hintsin a data-driven intelligent tutor. Assertions are partially-worked examplesteps designed to appear within a student workspace, and in the same format asstudent-derived steps, to show students a possible subgoal leading to thesolution. We hypothesized that Assertions can help address the well-known hintavoidance problem. In systems that only provide hints upon request, hintavoidance results in students not receiving hints when they are needed. Ourunsolicited Assertions do not seek to improve student help-seeking, but ratherseek to ensure students receive the help they need. We contrast Assertions withMessages, text-based, unsolicited hints that appear after student inactivity.Our results show that Assertions significantly increase unsolicited hint usagecompared to Messages. Further, they show a significant aptitude-treatmentinteraction between Assertions and prior proficiency, with Assertions leadingstudents with low prior proficiency to generate shorter (more efficient)posttest solutions faster. We also present a clustering analysis that showspatterns of productive persistence among students with low prior knowledge whenthe tutor provides unsolicited help in the form of Assertions. Overall, thiswork provides encouraging evidence that hint presentation can significantlyimpact how students use them and using Assertions can be an effective way toaddress help avoidance.',\n",
              " 'Research has shown assistance can provide many benefits to novices lackingthe mental models needed for problem solving in a new domain. However, varyingapproaches to assistance, such as subgoals and next-step hints, have beenimplemented with mixed results. Next-Step hints are common in data-driventutors due to their straightforward generation from historical student data, aswell as research showing positive impacts on student learning. However, thereis a lack of research exploring the possibility of extending data-drivenmethods to provide higher-level assistance. Therefore, we modified ourdata-driven Next-Step hint generator to provide Waypoints, hints that are a fewsteps ahead, representing problem-solving subgoals. We hypothesized thatWaypoints would benefit students with high prior knowledge, and that Next-Stephints would most benefit students with lower prior knowledge. In this study, weinvestigated the influence of data-driven hint type, Waypoints versus Next-Stephints, on student learning in a logic proof tutoring system, Deep Thought, in adiscrete mathematics course. We found that Next-Step hints were more beneficialfor the majority of students in terms of time, efficiency, and accuracy on theposttest. However, higher totals of successfully used Waypoints were correlatedwith improvements in efficiency and time in the posttest. These results suggestthat Waypoint hints could be beneficial, but more scaffolding may be needed tohelp students follow them.',\n",
              " \"Research on intelligent tutoring systems has been exploring data-drivenmethods to deliver effective adaptive assistance. While much work has been doneto provide adaptive assistance when students seek help, they may not seek helpoptimally. This had led to the growing interest in proactive adaptiveassistance, where the tutor provides unsolicited assistance upon predictions ofstruggle or unproductivity. Determining when and whether to providepersonalized support is a well-known challenge called the assistance dilemma.Addressing this dilemma is particularly challenging in open-ended domains,where there can be several ways to solve problems. Researchers have exploredmethods to determine when to proactively help students, but few of thesemethods have taken prior hint usage into account. In this paper, we present anovel data-driven approach to incorporate students' hint usage in predictingtheir need for help. We explore its impact in an intelligent tutor that dealswith the open-ended and well-structured domain of logic proofs. We present acontrolled study to investigate the impact of an adaptive hint policy based onpredictions of HelpNeed that incorporate students' hint usage. We showempirical evidence to support that such a policy can save students asignificant amount of time in training, and lead to improved posttest results,when compared to a control without proactive interventions. We also show thatincorporating students' hint usage significantly improves the adaptive hintpolicy's efficacy in predicting students' HelpNeed, thereby reducing trainingunproductivity, reducing possible help avoidance, and increasing possible helpappropriateness (a higher chance of receiving help when it was likely to beneeded). We conclude with suggestions on the domains that can benefit from thisapproach as well as the requirements for adoption.\",\n",
              " \"Learning to derive subgoals reduces the gap between experts and students andmakes students prepared for future problem solving. Researchers have exploredsubgoal labeled instructional materials with explanations in traditionalproblem solving and within tutoring systems to help novices learn to subgoal.However, only a little research is found on problem-solving strategies inrelationship with subgoal learning. Also, these strategies are under-exploredwithin computer-based tutors and learning environments. Backwardproblem-solving strategy is closely related to the process of subgoaling, whereproblem solving iteratively refines the goal into a new subgoal to reducedifficulty. In this paper, we explore a training strategy for backward strategylearning within an intelligent logic tutor that teaches logic proofconstruction. The training session involved backward worked examples (BWE) andproblem-solving (BPS) to help students learn backward strategy towardsimproving their subgoaling and problem-solving skills. To evaluate the trainingstrategy, we analyzed students' 1) experience with and engagement in learningbackward strategy, 2) performance, and 3) proof construction approaches in newproblems that they solved independently without tutor help after each level oftraining and in post-test. Our results showed that, when new problems weregiven to solve without any tutor help, students who were trained with both BWEand BPS outperformed students who received none of the treatment or only BWEduring training. Additionally, students trained with both BWE and BPS derivedsubgoals during proof construction with significantly higher efficiency thanthe other two groups.\",\n",
              " \"Data-driven programming feedback systems can help novices to program in theabsence of a human tutor. Prior evaluations showed that these systems improvelearning in terms of test scores, or task completion efficiency. However,crucial aspects which can impact learning or reveal insights important forfuture improvement of such systems are ignored in these evaluations. Theseaspects include inherent fallibility of current state-of-the-art, students'programming behavior in response to correct/incorrect feedback, andeffective/ineffective system components. Consequently, a great deal ofknowledge is yet to be discovered about such systems. In this paper, we apply amulti-criteria evaluation with 5 criteria on a data-driven feedback systemintegrated within a block-based novice programming environment. Each criterionin the evaluation reveals a unique pivotal aspect of the system: 1) Howaccurate the feedback system is; 2) How it guides students throughoutprogramming tasks; 3) How it helps students in task completion; 4) What happenswhen it goes wrong; and 5) How students respond generally to the system. Ourevaluation results showed that the system was helpful to students due to itseffective design and feedback representation despite being fallible. However,novices can be negatively impacted by this fallibility due to high reliance andlack of self-evaluation. The negative impacts include increased working time,implementation, or submission of incorrect/partially correct solutions. Theevaluation results reinforced the necessity of multi-criteria systemevaluations while revealing important insights helpful to ensuring proper usageof data-driven feedback systems, designing fallibility mitigation steps, anddriving research for future improvement.\",\n",
              " \"Intelligent tutoring systems can support students in solving multi-step tasksby providing hints regarding what to do next. However, engineering suchnext-step hints manually or via an expert model becomes infeasible if the spaceof possible states is too large. Therefore, several approaches have emerged toinfer next-step hints automatically, relying on past students' data. Inparticular, the Hint Factory (Barnes & Stamper, 2008) recommends edits that aremost likely to guide students from their current state towards a correctsolution, based on what successful students in the past have done in the samesituation. Still, the Hint Factory relies on student data being available forany state a student might visit while solving the task, which is not the casefor some learning tasks, such as open-ended programming tasks. In thiscontribution we provide a mathematical framework for edit-based hint policiesand, based on this theory, propose a novel hint policy to provide edit hints invast and sparsely populated state spaces. In particular, we extend the HintFactory by considering data of past students in all states which are similar tothe student's current state and creating hints approximating the weightedaverage of all these reference states. Because the space of possible weightedaverages is continuous, we call this approach the Continuous Hint Factory. Inour experimental evaluation, we demonstrate that the Continuous Hint Factorycan predict more accurately what capable students would do compared to existingprediction schemes on two learning tasks, especially in an open-endedprogramming task, and that the Continuous Hint Factory is comparable toexisting hint policies at reproducing tutor hints on a simple UML diagram task.\",\n",
              " 'Determining when and whether to provide personalized support is a well-knownchallenge called the assistance dilemma. A core problem in solving theassistance dilemma is the need to discover when students are unproductive sothat the tutor can intervene. Such a task is particularly challenging foropen-ended domains, even those that are well-structured with defined principlesand goals. In this paper, we present a set of data-driven methods to classify,predict, and prevent unproductive problem-solving steps in the well-structuredopen-ended domain of logic. This approach leverages and extends the HintFactory, a set of methods that leverages prior student solution attempts tobuild data-driven intelligent tutors. We present a HelpNeed classification,that uses prior student data to determine when students are likely to beunproductive and need help learning optimal problem-solving strategies. Wepresent a controlled study to determine the impact of an Adaptive pedagogicalpolicy that provides proactive hints at the start of each step based on theoutcomes of our HelpNeed predictor: productive vs. unproductive. Our resultsshow that the students in the Adaptive condition exhibited better trainingbehaviors, with lower help avoidance, and higher help appropriateness (a higherchance of receiving help when it was likely to be needed), as measured usingthe HelpNeed classifier, when compared to the Control. Furthermore, the resultsshow that the students who received Adaptive hints based on HelpNeedpredictions during training significantly outperform their Control peers on theposttest, with the former producing shorter, more optimal solutions in lesstime. We conclude with suggestions on how these HelpNeed methods could beapplied in other well-structured open-ended domains.',\n",
              " 'We consider an input $x$ generated by an unknown stationary ergodic source$X$ that enters a signal processing system $J$, resulting in $w=J(x)$. Weobserve $w$ through a noisy channel, $y=z(w)$; our goal is to estimate x from$y$, $J$, and knowledge of $f_{Y|W}$. This is universal estimation, because$f_X$ is unknown. We provide a formulation that describes a trade-off betweeninformation complexity and noise. Initial theoretical, algorithmic, andexperimental evidence is presented in support of our approach.',\n",
              " 'Compressive sensing (CS) is an emerging field based on the revelation that asmall collection of linear projections of a sparse signal contains enoughinformation for stable, sub-Nyquist signal acquisition. When a statisticalcharacterization of the signal is available, Bayesian inference can complementconventional CS methods based on linear programming or greedy algorithms. Weperform approximate Bayesian inference using belief propagation (BP) decoding,which represents the CS encoding matrix as a graphical model. Fast computationis obtained by reducing the size of the graphical model with sparse encodingmatrices. To decode a length-N signal containing K large coefficients, ourCS-BP decoding algorithm uses O(Klog(N)) measurements and O(Nlog^2(N))computation. Finally, although we focus on a two-state mixture Gaussian model,CS-BP is easily adapted to other signal models.',\n",
              " 'Motivated by the Markov chain Monte Carlo (MCMC) approach to the compressionof discrete sources developed by Jalali and Weissman, we propose a lossycompression algorithm for analog sources that relies on a finite reproductionalphabet, which grows with the input length. The algorithm achieves, in anappropriate asymptotic sense, the optimum Shannon theoretic tradeoff betweenrate and distortion, universally for stationary ergodic continuous amplitudesources. We further propose an MCMC-based algorithm that resorts to a reducedreproduction alphabet when such reduction does not prevent achieving theShannon limit. The latter algorithm is advantageous due to its reducedcomplexity and improved rates of convergence when employed on sources with afinite and small optimum reproduction alphabet.',\n",
              " 'Compressed sensing (CS) demonstrates that sparse signals can be recoveredfrom underdetermined linear measurements. We focus on the joint sparse recoveryproblem where multiple signals share the same common sparse support sets, andthey are measured through the same sensing matrix. Leveraging a recentinformation theoretic characterization of single signal CS, we formulate theoptimal minimum mean square error (MMSE) estimation problem, and derive abelief propagation algorithm, its relaxed version, for the joint sparserecovery problem and an approximate message passing algorithm. In addition,using density evolution, we provide a sufficient condition for exact recovery.',\n",
              " \"In this paper, compressed sensing with noisy measurements is addressed. Thetheoretically optimal reconstruction error is studied by evaluating Tanaka'sequation. The main contribution is to show that in several regions, which havedifferent measurement rates and noise levels, the reconstruction error behavesdifferently. This paper also evaluates the performance of the beliefpropagation (BP) signal reconstruction method in the regions discovered. Whenthe measurement rate and the noise level lie in a certain region, BP issuboptimal with respect to Tanaka's equation, and it may be possible to developreconstruction algorithms with lower error in that region.\",\n",
              " \"Two-part reconstruction is a framework for signal recovery in compressedsensing (CS), in which the advantages of two different algorithms are combined.Our framework allows to accelerate the reconstruction procedure withoutcompromising the reconstruction quality. To illustrate the efficacy of ourtwo-part approach, we extend the author's previous Sudocodes algorithm and makeit robust to measurement noise. In a 1-bit CS setting, promising numericalresults indicate that our algorithm offers both a reduction in run-time andimprovement in reconstruction quality.\",\n",
              " 'In this paper, a communication-efficient multi-processor compressed sensingframework based on the approximate message passing algorithm is proposed. Weperform lossy compression on the data being communicated between processors,resulting in a reduction in communication costs with a minor degradation inrecovery quality. In the proposed framework, a new state evolution formulationtakes the quantization error into account, and analytically determines thecoding rate required in each iteration. Two approaches for allocating thecoding rate, an online back-tracking heuristic and an optimal allocation schemebased on dynamic programming, provide significant reductions in communicationcosts.',\n",
              " 'This work considers millimeter-wave channel estimation in a setting whereparameters of the underlying mmWave channels are varying dynamically over timeand there is a single drifting path. In this setting, channel estimates at timeblock $k$ can be used as side information (SI) when estimating the channel atblock $k+1$. To estimate channel parameters, we employ an SI-aided (complex)approximate message passing algorithm and compare its performance to abenchmark based on orthogonal matching pursuit.',\n",
              " 'We consider the problem of identifying a pattern of faults from a set ofnoisy linear measurements. Unfortunately, maximum a posteriori probabilityestimation of the fault pattern is computationally intractable. To solve thefault identification problem, we propose a non-parametric belief propagationapproach. We show empirically that our belief propagation solver is moreaccurate than recent state-of-the-art algorithms including interior pointmethods and semidefinite programming. Our superior performance is explained bythe fact that we take into account both the binary nature of the individualfaults and the sparsity of the fault pattern arising from their rarity.',\n",
              " 'In compressive sensing, a small collection of linear projections of a sparsesignal contains enough information to permit signal recovery. Distributedcompressive sensing (DCS) extends this framework by defining ensemble sparsitymodels, allowing a correlated ensemble of sparse signals to be jointlyrecovered from a collection of separately acquired compressive measurements. Inthis paper, we introduce a framework for modeling sparse signal ensembles thatquantifies the intra- and inter-signal dependencies within and among thesignals. This framework is based on a novel bipartite graph representation thatlinks the sparse signal coefficients with the measurements obtained for eachsignal. Using our framework, we provide fundamental bounds on the number ofnoiseless measurements that each sensor must collect to ensure that the signalsare jointly recoverable.',\n",
              " 'Compressed sensing typically deals with the estimation of a system input fromits noise-corrupted linear measurements, where the number of measurements issmaller than the number of input components. The performance of the estimationprocess is usually quantified by some standard error metric such as squarederror or support set error. In this correspondence, we consider a noisycompressed sensing problem with any arbitrary error metric. We propose asimple, fast, and highly general algorithm that estimates the original signalby minimizing the error metric defined by the user. We verify that ouralgorithm is optimal owing to the decoupling principle, and we describe ageneral method to compute the fundamental information-theoretic performancelimit for any error metric. We provide two example metrics --- minimum meanabsolute error and minimum mean support error --- and give the theoreticalperformance limits for these two cases. Experimental results show that ouralgorithm outperforms methods such as relaxed belief propagation (relaxed BP)and compressive sampling matching pursuit (CoSaMP), and reaches the suggestedtheoretical limits for our two example metrics.',\n",
              " 'We consider the problem of reconstructing a signal from noisy measurements inlinear mixing systems. The reconstruction performance is usually quantified bystandard error metrics such as squared error, whereas we consider any additiveerror metric. Under the assumption that relaxed belief propagation (BP) cancompute the posterior in the large system limit, we propose a simple, fast, andhighly general algorithm that reconstructs the signal by minimizing theuser-defined error metric. For two example metrics, we provide performanceanalysis and convincing numerical results. Finally, our algorithm can beadjusted to minimize the $\\\\ell_\\\\infty$ error, which is not additive.Interestingly, $\\\\ell_{\\\\infty}$ minimization only requires to apply a Wienerfilter to the output of relaxed BP.',\n",
              " \"We consider the problem of estimating an input signal from noisy measurementsin both parallel scalar Gaussian channels and linear mixing systems. Theperformance of the estimation process is quantified by the $\\\\ell_\\\\infty$ normerror metric. We first study the minimum mean $\\\\ell_\\\\infty$ error estimator inparallel scalar Gaussian channels, and verify that, when the input isindependent and identically distributed (i.i.d.) mixture Gaussian, the Wienerfilter is asymptotically optimal with probability 1. For linear mixing systemswith i.i.d. sparse Gaussian or mixture Gaussian inputs, under the assumptionthat the relaxed belief propagation (BP) algorithm matches Tanaka's fixed pointequation, applying the Wiener filter to the output of relaxed BP is alsoasymptotically optimal with probability 1. However, in order to solve thepractical problem where the signal dimension is finite, we apply an estimationalgorithm that has been proposed in our previous work, and illustrate that an$\\\\ell_\\\\infty$ error minimizer can be approximated by an $\\\\ell_p$ errorminimizer provided the value of $p$ is properly chosen.\",\n",
              " 'We consider signals that follow a parametric distribution where the parametervalues are unknown. To estimate such signals from noisy measurements in scalarchannels, we study the empirical performance of an empirical Bayes (EB)approach and a full Bayes (FB) approach. We then apply EB and FB to solvecompressed sensing (CS) signal estimation problems by successively denoising ascalar Gaussian channel within an approximate message passing (AMP) framework.Our numerical results show that FB achieves better performance than EB inscalar channel denoising problems when the signal dimension is small. In the CSsetting, the signal dimension must be large enough for AMP to work well; forlarge signal dimensions, AMP has similar performance with FB and EB.',\n",
              " 'Consider the estimation of a signal ${\\\\bf x}\\\\in\\\\mathbb{R}^N$ from noisyobservations ${\\\\bf r=x+z}$, where the input~${\\\\bf x}$ is generated by anindependent and identically distributed (i.i.d.) Gaussian mixture source, and${\\\\bf z}$ is additive white Gaussian noise (AWGN) in parallel Gaussianchannels. Typically, the $\\\\ell_2$-norm error (squared error) is used toquantify the performance of the estimation process. In contrast, we considerthe $\\\\ell_\\\\infty$-norm error (worst case error). For this error metric, weprove that, in an asymptotic setting where the signal dimension $N\\\\to\\\\infty$,the $\\\\ell_\\\\infty$-norm error always comes from the Gaussian component that hasthe largest variance, and the Wiener filter asymptotically achieves the optimalexpected $\\\\ell_\\\\infty$-norm error. The i.i.d. Gaussian mixture case is easilyapplicable to i.i.d. Bernoulli-Gaussian distributions, which are often used tomodel sparse signals. Finally, our results can be extended to linear mixingsystems with i.i.d. Gaussian mixture inputs, in settings where a linear mixingsystem can be decoupled to parallel Gaussian channels.',\n",
              " 'We consider compressive imaging problems, where images are reconstructed froma reduced number of linear measurements. Our objective is to improve overexisting compressive imaging algorithms in terms of both reconstruction errorand runtime. To pursue our objective, we propose compressive imaging algorithmsthat employ the approximate message passing (AMP) framework. AMP is aniterative signal reconstruction algorithm that performs scalar denoising ateach iteration; in order for AMP to reconstruct the original input signal well,a good denoiser must be used. We apply two wavelet based image denoisers withinAMP. The first denoiser is the \"amplitude-scaleinvariant Bayes estimator\"(ABE), and the second is an adaptive Wiener filter; we call our AMP basedalgorithms for compressive imaging AMP-ABE and AMP-Wiener. Numerical resultsshow that both AMP-ABE and AMP-Wiener significantly improve over the state ofthe art in terms of runtime. In terms of reconstruction quality, AMP-Wieneroffers lower mean square error (MSE) than existing compressive imagingalgorithms. In contrast, AMP-ABE has higher MSE, because ABE does not denoiseas well as the adaptive Wiener filter.',\n",
              " \"We present a novel lossless universal source coding algorithm that usesparallel computational units to increase the throughput. The length-$N$ inputsequence is partitioned into $B$ blocks. Processing each block independently ofthe other blocks can accelerate the computation by a factor of $B$, butdegrades the compression quality. Instead, our approach is to first estimatethe minimum description length (MDL) source underlying the entire input, andthen encode each of the $B$ blocks in parallel based on the MDL source. Withthis two-pass approach, the compression loss incurred by using more parallelunits is insignificant. Our algorithm is work-efficient, i.e., itscomputational complexity is $O(N/B)$. Its redundancy is approximately$B\\\\log(N/B)$ bits above Rissanen's lower bound on universal coding performance,with respect to any tree source whose maximal depth is at most $\\\\log(N/B)$.\",\n",
              " 'We study the excess mean square error (EMSE) above the minimum mean squareerror (MMSE) in large linear systems where the posterior mean estimator (PME)is evaluated with a postulated prior that differs from the true prior of theinput signal. We focus on large linear systems where the measurements areacquired via an independent and identically distributed random matrix, and arecorrupted by additive white Gaussian noise (AWGN). The relationship between theEMSE in large linear systems and EMSE in scalar channels is derived, and closedform approximations are provided. Our analysis is based on the decouplingprinciple, which links scalar channels to large linear system analyses.Numerical examples demonstrate that our closed form approximations areaccurate.',\n",
              " 'We consider a compressive hyperspectral imaging reconstruction problem, wherethree-dimensional spatio-spectral information about a scene is sensed by acoded aperture snapshot spectral imager (CASSI). The approximate messagepassing (AMP) framework is utilized to reconstruct hyperspectral images fromCASSI measurements, and an adaptive Wiener filter is employed as athree-dimensional image denoiser within AMP. We call our algorithm\"AMP-3D-Wiener.\" The simulation results show that AMP-3D-Wiener outperformsexisting widely-used algorithms such as gradient projection for sparsereconstruction (GPSR) and two-step iterative shrinkage/thresholding (TwIST)given the same amount of runtime. Moreover, in contrast to GPSR and TwIST,AMP-3D-Wiener need not tune any parameters, which simplifies the reconstructionprocess.',\n",
              " 'We consider large-scale linear inverse problems in Bayesian settings. Wefollow a recent line of work that applies the approximate message passing (AMP)framework to multi-processor (MP) computational systems, where each processornode stores and processes a subset of rows of the measurement matrix along withcorresponding measurements. In each MP-AMP iteration, nodes of the MP systemand its fusion center exchange lossily compressed messages pertaining to theirestimates of the input. In this setup, we derive the optimal per-iterationcoding rates using dynamic programming. We analyze the excess mean squarederror (EMSE) beyond the minimum mean squared error (MMSE), and prove that, inthe limit of low EMSE, the optimal coding rates increase approximately linearlyper iteration. Additionally, we obtain that the combined cost of computationand communication scales with the desired estimation quality according to$O(\\\\log^2(1/\\\\text{EMSE}))$. Finally, we study trade-offs between the physicalcosts of the estimation process including computation time, communicationloads, and the estimation quality as a multi-objective optimization problem,and characterize the properties of the Pareto optimal surfaces.',\n",
              " 'We consider large-scale linear inverse problems in Bayesian settings. Ourgeneral approach follows a recent line of work that applies the approximatemessage passing (AMP) framework in multi-processor (MP) computational systemsby storing and processing a subset of rows of the measurement matrix along withcorresponding measurements at each MP node. In each MP-AMP iteration, nodes ofthe MP system and its fusion center exchange lossily compressed messagespertaining to their estimates of the input. There is a trade-off between thephysical costs of the reconstruction process including computation time,communication loads, and the reconstruction quality, and it is impossible tosimultaneously minimize all the costs. We pose this minimization as amulti-objective optimization problem (MOP), and study the properties of thebest trade-offs (Pareto optimality) in this MOP. We prove that the achievableregion of this MOP is convex, and conjecture how the combined cost ofcomputation and communication scales with the desired mean squared error. Theseproperties are verified numerically.',\n",
              " 'Approximate message passing (AMP) is an algorithmic framework for solvinglinear inverse problems from noisy measurements, with exciting applicationssuch as reconstructing images, audio, hyper spectral images, and various othersignals, including those acquired in compressive signal acquisiton systems. Thegrowing prevalence of big data systems has increased interest in large-scaleproblems, which may involve huge measurement matrices that are unsuitable forconventional computing systems. To address the challenge of large-scaleprocessing, multiprocessor (MP) versions of AMP have been developed. We providean overview of two such MP-AMP variants. In row-MP-AMP, each computing nodestores a subset of the rows of the matrix and processes correspondingmeasurements. In column- MP-AMP, each node stores a subset of columns, and issolely responsible for reconstructing a portion of the signal. We will discusspros and cons of both approaches, summarize recent research results for each,and explain when each one may be a viable approach. Aspects that arehighlighted include some recent results on state evolution for both MP-AMPalgorithms, and the use of data compression to reduce communication in the MPnetwork.',\n",
              " 'Real-world applications such as magnetic resonance imaging with multiplecoils, multi-user communication, and diffuse optical tomography often assume alinear model where several sparse signals sharing common sparse supports areacquired by several measurement matrices and then contaminated by noise.Multi-measurement vector (MMV) problems consider the estimation orreconstruction of such signals. In different applications, the estimation errorthat we want to minimize could be the mean squared error or other metrics suchas the mean absolute error and the support set error. Seeing that minimizingdifferent error metrics is useful in MMV problems, we studyinformation-theoretic performance limits for MMV signal estimation witharbitrary additive error metrics. We also propose a message passing algorithmicframework that achieves the optimal performance, and rigorously prove theoptimality of our algorithm for a special case. We further conjecture theoptimality of our algorithm for some general cases, and back it up throughnumerical examples. As an application of our MMV algorithm, we propose a novelsetup for active user detection in multi-user communication and demonstrate thepromise of our proposed setup.',\n",
              " 'Approximate message passing (AMP) methods have gained recent traction insparse signal recovery. Additional information about the signal, or \\\\emph{sideinformation} (SI), is commonly available and can aid in efficient signalrecovery. This work presents an AMP-based framework that exploits SI and can bereadily implemented in various settings for which the SI results in separabledistributions. To illustrate the simplicity and applicability of our approach,this framework is applied to a Bernoulli-Gaussian (BG) model and a time-varyingbirth-death-drift (BDD) signal model, motivated by applications in channelestimation. We develop a suite of algorithms, called AMP-SI, and derivedenoisers for the BDD and BG models. Numerical evidence demonstrating theadvantages of our approach are presented alongside empirical evidence of theaccuracy of a proposed state evolution.',\n",
              " 'Fast testing can help mitigate the coronavirus disease 2019 (COVID-19)pandemic. Despite their accuracy for single sample analysis, infectiousdiseases diagnostic tools, like RT-PCR, require substantial resources to testlarge populations. We develop a scalable approach for determining the viralstatus of pooled patient samples. Our approach converts group testing to alinear inverse problem, where false positives and negatives are interpreted asgenerated by a noisy communication channel, and a message passing algorithmestimates the illness status of patients. Numerical results reveal that ourapproach estimates patient illness using fewer pooled measurements thanexisting noisy group testing algorithms. Our approach can easily be extended tovarious applications, including where false negatives must be minimized.Finally, in a Utopian world we would have collaborated with RT-PCR experts; itis difficult to form such connections during a pandemic. We welcome newcollaborators to reach out and help improve this work!',\n",
              " \"For more than fifty years, taxonomists have proposed numerous alternativedefinitions of species while they searched for a unique, comprehensive, andpersuasive definition. This monograph shows that these efforts have beenunnecessary, and indeed have provably been a pursuit of a will o' the wispbecause they have failed to recognize the theoretical impossibility of whatthey seek to accomplish. A clear and rigorous understanding of the logicunderlying species definition leads both to a recognition of the inescapableambiguity that affects the definition of species, and to a framework-relativeapproach to species definition that is logically compelling, i.e., cannot notbe accepted without inconsistency. An appendix reflects upon the conclusionsreached, applying them in an intellectually whimsical taxonomic thoughtexperiment that conjectures the possibility of an emerging new human species.\",\n",
              " \"Excitons in semiconductors are usually non interacting and behave like anideal gas, but may condense to a strongly correlated liquid like state, i.e.electron hole liquid (EHL), at high density and appropriate temperature. EHL isa macroscopic quantum state with exotic properties and represents the ultimateattainable charge excitation density in steady states. It bears great promisefor a variety of fields such as ultrahigh power photonics and quantum scienceand technology. However, the condensation of gas like excitons to EHL has oftenbeen restricted to cryogenic temperatures, which significantly limits theprospect of EHL for use in practical applications. Herein we demonstrate theformation of EHL at room temperature in monolayer MoS2 by taking advantage ofthe monolayer's extraordinarily strong exciton binding energy. This workdemonstrates the potential for the liquid like state of charge excitations tobe a useful platform for the studies of macroscopic quantum phenomena and thedevelopment of optoelectronic devices.\",\n",
              " \"We extend an off-the-shelf, executable formal semantics of C (Ellison andRosu's K Framework semantics) with the core features of CUDA-C. The hybridCPU/GPU computation model of CUDA-C presents challenges not just forprogrammers, but also for practitioners of formal methods. Our formal semanticshelps expose and clarify these issues. We demonstrate the usefulness of oursemantics by generating a tool from it capable of detecting some raceconditions and deadlocks in CUDA-C programs. We discuss limitations of ourmodel and argue that its extensibility can easily enable a wider range ofverification tasks.\",\n",
              " 'In an effort to lower the barrier to the adoption of FPGAs by a broadercommunity, today major FPGA vendors offer compiler toolchains for OpenCL code.While using these toolchain allows porting existing code to FPGAs, ensuringperformance portability across devices (i.e., CPUs, GPUs and FPGAs) is not atrivial task. This is in part due to the different hardware characteristics ofthese devices, including the nature of the hardware parallelism and the memorybandwidth they offer. In particular, global memory accesses are known to be oneof the main performance bottlenecks for OpenCL kernels deployed on FPGA. Inthis paper, we investigate the use of pipes to improve memory bandwidthutilization and performance of OpenCL kernels running on FPGA. This is done byseparating the global memory accesses from the computation, enabling better useof the load units required to access global memory. We perform experiments on aset of broadly used benchmark applications with various compute and memoryaccess patterns. Our experiments, conducted on an Intel Arria GX board, showthat the proposed method is effective in improving the memory bandwidthutilization of most kernels, particularly those exhibiting irregular memoryaccess patterns. This, in turn, leads to performance improvements, in somecases significant.',\n",
              " 'GPUs have been widely used to accelerate computations exhibiting simplepatterns of parallelism - such as flat or two-level parallelism - and a degreeof parallelism that can be statically determined based on the size of the inputdataset. However, the effective use of GPUs for algorithms exhibiting complexpatterns of parallelism, possibly known only at runtime, is still an openproblem. Recently, Nvidia has introduced Dynamic Parallelism (DP) in its GPUs.By making it possible to launch kernels directly from GPU threads, this featureenables nested parallelism at runtime. However, the effective use of DP muststill be understood: a naive use of this feature may suffer from significantruntime overhead and lead to GPU underutilization, resulting in poorperformance. In this work, we target this problem. First, we demonstrate how anaive use of DP can result in poor performance. Second, we propose threeworkload consolidation schemes to improve performance and hardware utilizationof DP-based codes, and we implement these code transformations in adirective-based compiler. Finally, we evaluate our framework on two categoriesof applications: algorithms including irregular loops and algorithms exhibitingparallel recursion. Our experiments show that our approach significantlyreduces runtime overhead and improves GPU utilization, leading to speedupfactors from 90x to 3300x over basic DP-based solutions and speedups from 2x to6x over flat implementations.',\n",
              " 'In many Multimedia content analytics frameworks feature likelihood mapsrepresented as histograms play a critical role in the overall algorithm.Integral histograms provide an efficient computational framework for extractingmulti-scale histogram-based regional descriptors in constant time which areconsidered as the principle building blocks of many video content analyticsframeworks. We evaluate four different mappings of the integral histogramcomputation onto Graphics Processing Units (GPUs) using different kerneloptimization strategies. Our kernels perform cumulative sums on row and columnhistograms in a cross-weave or wavefront scan order, use different dataorganization and scheduling methods that is shown to critically affectutilization of GPU resources (cores and shared memory). Tiling the 3-D arrayinto smaller regular data blocks significantly speeds up the efficiency of thecomputation compared to a strip-based organization. The tiled integralhistogram using a diagonal wavefront scan has the best performance of about300.4 frames/sec for 640 x 480 images and 32 bins with a speedup factor ofabout 120 using GTX Titan X graphics card compared to a single threadedsequential CPU implementation. Double-buffering has been exploited to overlapcomputation and communication across sequence of images. Mapping integralhistogram bins computations onto multiple GPUs enables us to process 32 gigabytes integral histogram data (of 64MB Image and 128 bins) with a frame rate of0.73 Hz and speedup factor of 153X over single-threaded CPU implementation andthe speedup of 45X over 16-threaded CPU implementation.',\n",
              " 'Over the past few years, there has been an increased interest in includingFPGAs in data centers and high-performance computing clusters along with GPUsand other accelerators. As a result, it has become increasingly important tohave a unified, high-level programming interface for CPUs, GPUs and FPGAs. Thishas led to the development of compiler toolchains to deploy OpenCL code onFPGA. However, the fundamental architectural differences between GPUs and FPGAshave led to performance portability issues: it has been shown that OpenCL codeoptimized for GPU does not necessarily map well to FPGA, often requiring manualoptimizations to improve performance. In this paper, we explore the use ofthread coarsening - a compiler technique that consolidates the work of multiplethreads into a single thread - on OpenCL code running on FPGA. While thisoptimization has been explored on CPU and GPU, the architectural features ofFPGAs and the nature of the parallelism they offer lead to differentperformance considerations, making an analysis of thread coarsening on FPGAworthwhile. Our evaluation, performed on our microbenchmarks and on a set ofapplications from open-source benchmark suites, shows that thread coarseningcan yield performance benefits (up to 3-4x speedups) to OpenCL code running onFPGA at a limited resource utilization cost.',\n",
              " 'The threshold displacement energy (TDE) is the minimum amount of kineticenergy required to displace an atom from its lattice site. The magnitude of theTDE displays significant variance as a function of the crystallographicdirection, system temperature and applied strain, among a variety of otherfactors. It is critically important to determine an accurate value of the TDEin order to calculate the total number of displacements due to a givenirradiation condition, and thus to understand the materials response toirradiation. In this study, molecular dynamics simulations have been performedto calculate the threshold displacement energy in body-centered cubic iron as afunction of strain and temperature. With applied strain, a decrease of the TDEof up to approximately 14 eV was observed. A temperature increase from 300 K to500 K can result in an increase of the TDE of up to approximately 9 eV.',\n",
              " 'Radiation damage in body-centered cubic (BCC) Fe has been extensively studiedby computer simulations to quantify effects of temperature, impinging particleenergy, and the presence of extrinsic particles. However, limited investigationhas been conducted into the effects of mechanical stresses and strain. In areactor environment, structural materials are often mechanically strained, andan expanded understanding of how this strain affects the generation of defectsmay be important for predicting microstructural evolution and damageaccumulation under such conditions. In this study, we have performed moleculardynamics simulations in which various types of homogeneous strains are appliedto BCC Fe and the effect on defect generation is examined. It is found thatvolume-conserving shear strains yield no statistically significant variationsin the stable number of defects created via cascades in BCC Fe. However,strains that result in volume changes are found to produce significant effectson defect generation.',\n",
              " 'Computed Tomographic Imaging Spectrometers (CTIS) capture hyperspectralimages in realtime. However, post processing the imagery can require enormouscomputational resources; thus, limiting its application to non-realtimescenarios. To overcome these challenges we developed a highly parallelizablealgorithm that exploits spatial shift-invariance. To demonstrate theversatility of our new algorithm, we developed implementations on a desktop andan embedded graphics processing unit (GPU). To our knowledge, our results showthe fastest image reconstruction times reported.',\n",
              " 'Because of their unique structure, it has been proposed that carbon nanotuberopes may well provide an ideal container for the storage of molecularhydrogen. Indeed, there has been some experimental evidence of enhancedhydrogen uptake in doped Li and other alkali metal systems [Chen et al, Science285, 91 (1999)]. We have therefore addressed this issue of hydrogen storage inLi-doped graphite and carbon nanotube systems theoretically with ab initiosimulations. Our results find no evidence for such enhanced storage, based onthe induced structural changes. In addition, we have also investigated thediffusion barriers for hydrogen to enter into nanotube interiors, both in thepresence and absence of topological defects. Even if nanotube interiors aremade accessible, the hydrogen uptake remains modest, i.e., less than 3.5 wt%.Mechanically or chemically processing nanotubes is therefore not likely to leadto greatly increased hydrogen storage.',\n",
              " 'The influence of substrate steps on the bottom-up synthesis of atomicallyprecise graphene nanoribbons (GNRs) on an Au(111) surface is investigated. Astraight surface step is found to promote the assembly of long and compactarrays of polymers with enhanced interchain pi-pi stacking interaction, whichcreates a steric hindrance effect on cyclodehydrogenation to suppress the Hpassivation of polymer ends. The modified two-stage growth process results inperiodic arrays of GNRs with doubled average length near step edges.',\n",
              " 'The interplay of non-trivial band topology and magnetism gives rise to aseries of exotic quantum phenomena, such as the emergent quantum anomalous Hall(QAH) effect and topological magnetoelectric effect. Many of these quantumphenomena have local manifestations when the global symmetry is broken. Here,we report local signatures of the thickness dependent topology in intrinsicmagnetic topological insulator MnBi$_2$Te$_4$(MBT), using scanning tunnelingmicroscopy and spectroscopy on molecular beam epitaxy grown MBT thin films. Athickness-dependent band gap with an oscillatory feature is revealed, which wereproduce with theoretical calculations. Our theoretical results indicate atopological quantum phase transition beyond a film thickness of one monolayer,with alternating QAH and axion insulating states for even and odd layers,respectively. At an even-odd layer step, a localized gapped electronic state isobserved, in agreement with an axion insulator edge state that results from aphase transition across the step. The demonstration of thickness-dependenttopological properties highlights the role of nanoscale control over novelquantum states, reinforcing the necessity of thin film technology in quantuminformation science applications.',\n",
              " 'This work focuses on the identification of reliable and repeatable spatial(three-dimensional) trajectories that link the Earth and the Moon. For thispurpose, this paper aims to extend the 2:1 resonant prograde family and 2:1resonant retrograde family to three dimensions and to introduce spatial orbitsthat are not currently present in the literature. These orbits, named the 2:1resonant spatial family, bifurcate from the two-dimensional families andsmoothly transition between them in phase space. The stability properties ofthis new family of resonant orbits are discussed, and, interestingly, thisfamily includes marginally stable members. Furthermore, this new family oforbits is applied to several engineering problems in the Earth-Moon system.First, this paper selects an appropriate member of the 2:1 resonant spatialfamily on the basis of its stability properties and relationships with othermultibody orbits in the regime. Next, this work combines this trajectory withmomentum exchange tethers to transit payloads throughout the system in areliable and repeatable fashion. Finally, this paper studies the process ofaborting a momentum exchange tether catch and related recovery opportunities.',\n",
              " 'Metastability is a common obstacle to performing long molecular dynamicssimulations. Many numerical methods have been proposed to overcome it. Onemethod is parallel replica dynamics, which relies on the rapid convergence ofthe underlying stochastic process to a quasi-stationary distribution. Tworequirements for applying parallel replica dynamics are knowledge of the timescale on which the process converges to the quasi-stationary distribution and amechanism for generating samples from this distribution. By combining aFleming-Viot particle system with convergence diagnostics to simultaneouslyidentify when the process converges while also generating samples, we canaddress both points. This variation on the algorithm is illustrated withvarious numerical examples, including those with entropic barriers and the 2DLennard-Jones cluster of seven atoms.',\n",
              " 'Spatial multiscale methods have established themselves as useful tools forextending the length scales accessible by conventional statics (i.e., zerotemperature molecular dynamics). Recently, extensions of these methods, such asthe finite-temperature quasicontinuum (hot-QC) or Coarse-Grained MolecularDynamics (CGMD) methods, have allowed for multiscale molecular dynamicssimulations at finite temperature. Here, we assess the quality of the long-timedynamics these methods generate by considering canonical transition rates.Specifically, we analyze the transition state theory (TST) rates in CGMD andcompare them to the corresponding TST rate of the fully atomistic system. Theability of such an approach to reliably reproduce the TST rate is verifiedthrough a relative error analysis, which is then used to highlight the majorcontributions to the error and guide the choice of degrees of freedom. Finally,our analytical results are compared with numerical simulations for the case ofa 1-D chain.',\n",
              " \"This work applies the methods of signal processing and the concepts ofcontrol system design to model the maintenance and modulation of reading framein the process of protein synthesis. The model shows how translational speedcan modulate translational accuracy to accomplish programmed +1 frameshifts andcould have implications for the regulation of translational efficiency. Aseries of free energy estimates were calculated from the ribosome's interactionwith mRNA sequences during the process of translation elongation in eubacteria.A sinusoidal pattern of roughly constant phase was detected in these freeenergy signals. Signal phase was identified as a useful parameter for locatingprogrammed +1 frameshifts encoded in bacterial genes for release factor 2. Adisplacement model was developed that captures the mechanism of frameshiftbased on the information content of the signal parameters and the relativeabundance of tRNA in the bacterial cell. Results are presented usingexperimentally verified frameshift genes across eubacteria.\",\n",
              " 'In this paper we show existence of finite energy solutions for the Cauchyproblem associated with a semilinear wave equation with interior damping andsupercritical source terms. The main contribution consists in dealing withsuper-supercritical source terms (terms of the order of $|u|^p$ with $p\\\\geq 5$in $n=3$ dimensions), an open and highly recognized problem in the literatureon nonlinear wave equations.',\n",
              " \"We analyze a quasi-static Biot system of poroelasticity for both compressibleand incompressible constituents. The main feature of this model is a nonlinearcoupling of pressure and dilation through the system's permeability tensor.Such a model has been analyzed previously from the point of view ofconstructing weak solutions through a fully discretized approach. In thistreatment, we consider simplified Dirichlet type boundary conditions in boththe elastic displacement and pressure variables and give a full treatment ofweak solutions. Our construction of weak solutions for the nonlinear problem isbased on a priori estimates, a requisite feature in addressing thenonlinearity. We utilize a spatial semi-discretization and employ amulti-valued fixed point argument for a clear construction of weak solutions.We also provide regularity criteria for uniqueness of solutions.\",\n",
              " \"We consider the interaction between an incompressible, viscous fluid modeledby the dynamic Stokes equation and a multilayered poroelastic structure whichconsists of a thin, linear, poroelastic plate layer (in direct contact with thefree Stokes flow) and a thick Biot layer. The fluid flow and the elastodynamicsof the multilayered poroelastic structure are fully coupled across a fixedinterface through physical coupling conditions (including theBeavers-Joseph-Saffman condition), which present mathematical challengesrelated to the regularity of associated velocity traces. We prove existence ofweak solutions to this fluid-structure interaction problem with either (i) alinear, dynamic Biot model, or (ii) a nonlinear quasi-static Biot component,where the permeability is a nonlinear function of the fluid content (asmotivated by biological applications). The proof is based on constructingapproximate solutions through Rothe's method, and using energy methods and aversion of Aubin-Lions compactness lemma (in the nonlinear case) to recover theweak solution as the limit of approximate subsequences. We also provideuniqueness criteria and show that constructed weak solutions are indeed strongsolutions to the coupled problem if one assumes additional regularity.\",\n",
              " 'We consider quasi-static poroelastic systems with incompressibleconstituents. The nonlinear permeability is taken to be dependent on soliddilation, and physical types of boundary conditions (Dirichlet, Neumann, andmixed) for the fluid pressure are considered. Such dynamics are motivated byapplications in biomechanics and, in particular, tissue perfusion. This systemrepresents a nonlinear, implicit, degenerate evolution problem. We provide adirect fixed point strategy for proving the existence of weak solutions, whichis made possible by a novel result on the uniqueness of weak solution to theassociated linear system (the permeability a given function of space and time).The linear uniqueness proof is based on novel energy estimates for arbitraryweak solutions, rather than just for constructed solutions (as limits ofapproximants). The results of this work provide a foundation for addressingstrong solutions, as well uniqueness of weak solutions for the nonlinear porousmedia system.',\n",
              " 'We investigate and clarify the mathematical properties of linear poro-elasticsystems in the presence of classical (linear, Kelvin-Voigt) visco-elasticity.In particular, we quantify the time-regularizing and dissipative effects ofvisco-elasticity in the context of the quasi-static Biot equations. The full,coupled pressure-displacement presentation of the system is utilized, as wellas the framework of implicit, degenerate evolution equations, to demonstratesuch effects and characterize linear poro-visco-elastic systems. We consider asimple presentation of the dynamics (with convenient boundary conditions, etc.)for clarity in exposition across several relevant parameter ranges. Clearwell-posedness results are provided, with associated a priori estimates on thesolutions. In addition, precise statements of admissible initial conditions ineach scenario are given.',\n",
              " 'We consider a class of integer control problems that are governed by fluidflows through deformable porous media equations that arise in biomechanics. Weshow that the regularity assumptions for the convergence analysis of therecently proposed optimization algorithm in arXiv:2106.13453v3 [math.OC] holdfor the poro-elastic case of the governing equations and are violated for theporo-visco-elastic case. To bypass the restrictive regularity assumptions incase of violation, we introduce an additional partial regularization of thecontrol inputs by means of mollification and prove a $\\\\Gamma$-convergence-typeresult when the support parameter of the mollification is driven to zero.Numerical computations show that a homotopy that drives the support parameterof the mollification to zero may often lead to lower objective values andsmaller remaining instationarities, thereby of course inducing a computationalcost. Executing this homotopy is not always necessary, however, and we alsoobserve cases where the original algorithm is able to obtain points (finaliterates) of similar quality as the homotopy when applied without theadditional input regularization even if the regularity conditions are violated.',\n",
              " \"The main goal of this work is to clarify and quantify, by means ofmathematical analysis, the role of structural viscoelasticity in thebiomechanical response of deformable porous media with incompressibleconstituents to sudden changes in external applied loads. Models of deformableporous media with incompressible constituents are often utilized to describethe behavior of biological tissues, such as cartilages, bones and engineeredtissue scaffolds, where viscoelastic properties may change with age, disease orby design. Here, for the first time, we show that the fluid velocity within themedium could increase tremendously, even up to infinity, should the externalapplied load experience sudden changes in time and the structuralviscoelasticity be too small. In particular, we consider a one-dimensionalporo-visco-elastic model for which we derive explicit solutions in the caseswhere the external applied load is characterized by a step pulse or atrapezoidal pulse in time. By means of dimensional analysis, we identify somedimensionless parameters that can aid the design of structural propertiesand/or experimental conditions as to ensure that the fluid velocity within themedium remains bounded below a certain given threshold, thereby preventingpotential tissue damage. The application to confined compression tests forbiological tissues is discussed in detail. Interestingly, the loss ofviscoelastic tissue properties has been associated with various diseaseconditions, such as atherosclerosis, Alzheimer's disease and glaucoma. Thus,the findings of this work may be relevant to many applications in biology andmedicine.\",\n",
              " 'Current and future weak lensing surveys will rely on photometricallyestimated redshifts of very large numbers of galaxies. In this paper, weaddress several different aspects of the demanding photo-z performance thatwill be required for future experiments, such as the proposed ESA Euclidmission. It is first shown that the proposed all-sky near-infrared photometryfrom Euclid, in combination with anticipated ground-based photometry (e.g.PanStarrs-2 or DES) should yield the required precision in individual photo-zof sigma(z) < 0.05(1+z) at I_AB < 24.5. Simple a priori rejection schemes basedon the photometry alone can be tuned to recognise objects with wildlydiscrepant photo-z and to reduce the outlier fraction to < 0.25% with onlymodest loss of otherwise usable objects. Turning to the more challengingproblem of determining the mean redshift <z> of a set of galaxies to aprecision of 0.002(1+z) we argue that, for many different reasons, this is bestaccomplished by relying on the photo-z themselves rather than on the directmeasurement of <z> from spectroscopic redshifts of a representative subset ofthe galaxies. A simple adaptive scheme based on the statistical properties ofthe photo-z likelihood functions is shown to meet this stringent systematicrequirement. We also examine the effect of an imprecise correction for Galacticextinction and the effects of contamination by fainter over-lapping objects inphoto-z determination. The overall conclusion of this work is that theacquisition of photometrically estimated redshifts with the precision requiredfor Euclid, or other similar experiments, will be challenging but possible.(abridged)',\n",
              " 'We probe the spatial distribution of outflowing gas along four lines of sightseparated by up to 6 kpc in a gravitationally-lensed star-forming galaxy atz=1.70. Using MgII and FeII emission and absorption as tracers, we find thatthe clumps of star formation are driving galactic outflows with velocities of-170 to -250 km/sec. The velocities of MgII emission are redshifted withrespect to the systemic velocities of the galaxy, consistent with beingback-scattered. By contrast, the FeII fluorescent emission lines are eitherslightly blueshifted or at the systemic velocity of the galaxy. Taken together,the velocity structure of the MgII and FeII emission is consistent with arisingthrough scattering in galactic winds. Assuming a thin shell geometry for theout owing gas, the estimated masses carried out by these outfows are large (>30 - 50 $\\\\rm{M_{\\\\odot} yr^{-1}}$), with mass loading factors several times thestar-formation rate. Almost 20% to 50% of the blueshifted absorption probablyescapes the gravitational potential of the galaxy. In this galaxy, the outflowis \"locally sourced\", that is, the properties of the outflow in each line ofsight are dominated by the properties of the nearest clump of star formation;the wind is not global to the galaxy. The mass outflow rates and the momentumflux carried out by outflows in individual star forming knots of this objectare comparable to that of starburst galaxies in the local Universe.',\n",
              " 'We present a simple model that explains the origin of warm diffuse gas seenprimarily as highly ionized absorption line systems in the spectra ofbackground sources. We predict the observed column densities of several highlyionized transitions such as OVI, OVIII, NeVIII, NV, and MgX; and present aunified comparison of the model predictions with absorption lines seen in theMilky Way disk, Milky Way halo,starburst galaxies, the circumgalactic mediumand the intergalactic medium at low and high redshifts. We show that diffusegas seen in such diverse environments can be simultaneously explained by asimple model of radiatively cooling gas. We show that most of such absorptionline systems are consistent with being collisionally ionized, and estimate themaximum likelihood temperature of the gas in each observation. This modelsatisfactorily explains why OVI is regularly observed around star-forming low-zL* galaxies, and why NV is rarely seen around the same galaxies. We furtherpresent some consequences of this model in quantifying the dynamics of thecooling gas around galaxies and predict the shock velocities associated withsuch flows. A unique strength of this model is that while it has only one free(but physically well-constrained) parameter, it nevertheless successfullyreproduces the available data on O VI absorbers in the interstellar,circumgalactic, intra-group, and intergalactic media, as well as the availabledata on other absorption-line from highly ionized species.',\n",
              " 'We analyze the equivalent widths of HI Ly-$\\\\alpha$ ($W_{Ly\\\\alpha}$)absorption from the inner (R < 160 kpc) circumgalactic medium (CGM) of 85galaxies at $z \\\\sim 0$ with stellar masses $M*$ ranging $\\\\rm{8 \\\\leq log M* /M_{\\\\odot} \\\\leq 11.6}$. Across three orders of magnitude in stellar mass, theCGM of present-day galaxies exhibits a very high covering fraction of coolhydrogen gas ($f_C = 87\\\\pm 4$\\\\%) indicating that the CGM is ubiquitous inmodern, isolated galaxies. When HI Ly-$\\\\alpha$ is detected, its equivalentwidth declines with increasing radius regardless of the galaxy mass, but thescatter in this trend correlates closely with $M*$. Using the radial andstellar mass correlations, we construct a planar surface describing the coolCGM of modern galaxies: $\\\\log W^{\\\\rm{s}}_{HI 1215} \\\\; = \\\\; (0.34 \\\\pm 0.02) -(0.0026 \\\\pm 0.0005)\\\\times (R) + (0.286 \\\\pm 0.002) \\\\times \\\\log (M*/M_{\\\\odot})$.The RMS scatter around this bivariate relation is $\\\\sim$0.2 dex. We interpretthe explicit correlation between $W_{Ly\\\\alpha}$ and $M*$ to arise from theunderlying dark matter halo mass ($M_{halo}$), thereby suggesting a CGMfundamental plane between $W_{Ly\\\\alpha}$, $R$ and $M_{halo}$. This correlationcan be used to estimate the underlying dark matter halo mass from observationsof saturated HI Ly-$\\\\alpha$ in the CGM of a modern galaxy.',\n",
              " 'Reservoirs of dense atomic gas (primarily hydrogen), contain approximately 90percent of the neutral gas at a redshift of 3, and contribute to 2-3 percent ofthe total baryons in the Universe. These damped Lyman-${\\\\alpha}$ systems (socalled because they absorb Lyman-${\\\\alpha}$ photons from within and frombackground sources) have been studied for decades, but only through absorptionlines present in the spectra of background quasars and gamma-ray bursts. Suchpencil beams do not constrain the physical extent of the systems. Here, wereport integral-field spectroscopy of a bright, gravitationally lensed galaxyat a redshift of 2.7 with two foreground damped Lyman-${\\\\alpha}$ systems. Thesesystems are $>$ 238 $kpc^2$ in extent, with column densities of neutralhydrogen varying by more than an order of magnitude on $<$ 3 kpc-scales. Themean column densities are $10^{20.46}$ - $10^{20.84} cm^{-2}$ and the totalmasses are $> 5.5 \\\\times 10^{8}$ - $1.4 \\\\times 10^{9} M_{\\\\odot}$, showing thatthey contain the necessary fuel for the next generation of star formation,consistent with relatively massive, low-luminosity primeval galaxies atredshifts $>$ 2.',\n",
              " 'We report new observations of circumgalactic gas from the COS-Dwarfs survey,a systematic investigation of the gaseous halos around 43 low-mass z $\\\\leq$ 0.1galaxies using background QSOs observed with the Cosmic Origins Spectrograph.From the projected 1D and 2D distribution of C IV absorption, we find that C IVabsorption is detected out to ~ 0.5 R$_{vir}$ of the host galaxies. The C IVabsorption strength falls off radially as a power law and beyond 0.5 R$_{vir}$,no C IV absorption is detected above our sensitivity limit of ~ 50-100 m$\\\\AA$.We find a tentative correlation between detected C IV absorption strength andstar formation, paralleling the strong correlation seen in highly ionizedoxygen for L~L* galaxies by the COS-Halos survey. The data imply a large carbonreservoir in the CGM of these galaxies, corresponding to a minimum carbon massof $\\\\gtrsim$ 1.2$\\\\times 10^6$ $M_\\\\odot$ out to ~ 110 kpc. This mass iscomparable to the carbon mass in the ISM and more than the carbon masscurrently in stars of these galaxies. The C IV absorption seen around thesesub-L* galaxies can account for almost two-thirds of all $W_r$> 100 m$\\\\AA$ C IVabsorption detected at low z. Comparing the C IV covering fraction withhydrodynamical simulations, we find that an energy-driven wind model isconsistent with the observations whereas a wind model of constant velocityfails to reproduce the CGM or the galaxy properties.',\n",
              " 'We report new observations from a systematic, spectroscopic, ultravioletabsorption-line survey that maps the spatial and kinematic properties of thehigh-velocity gas in the Galactic Center region. We examine the hypothesis thatthis gas traces the biconical nuclear outflow. We use ultraviolet spectra of 47background QSOs and halo stars projected inside and outside the northern FermiBubble from the Hubble Space Telescope to study the incidence of high velocityabsorption around it. We use five lines of sight inside the northern FermiBubble to constrain the velocity and column densities of outflowing gas tracedby O I, Al II, C II, C IV, Si II, Si III, Si IV and other species. All fivelines of sight inside the northern Fermi Bubble exhibit blueshifted highvelocity absorption components, whereas only 9 out of the 42 lines of sightoutside the northern Fermi Bubble exhibit blueshifted high velocity absorptioncomponents. The observed outflow velocity profile decreases with Galacticlatitude and radial distance (R) from the Galactic Center. The observedblueshifted velocities change from $v_{GSR}$=-265 km/s at R~2.3 kpc to$v_{GSR}$=-91 km/s at R~6.5 kpc. We derive the metallicity of the entrained gasalong the 1H1613-097 sightline, which passes through the center of the northernFermi Bubble, finding [O/H] $\\\\gtrsim -0.54 \\\\pm 0.15$. A simple kinematic modeltuned to match the observed absorption component velocities along the fivelines of sight inside the Bubble, constrains the outflow velocities to~1000$-$1300 km/s, and the age of the outflow to be ~ 6$-$9 Myr. We estimate aminimum mass outflow rate for the nuclear outflow to be $\\\\gtrsim$ 0.2 $\\\\rm{M_{\\\\odot}\\\\; yr^{-1}}$. Combining the age and mass outflow rates, we determine aminimum mass of total UV absorbing cool gas entrained in the Fermi Bubbles tobe $\\\\gtrsim \\\\rm{ 2 \\\\times 10^{6} M_{\\\\odot}}$.',\n",
              " 'We present spatially resolved measurements of cool gas flowing into and outof the Milky Way (MW), using archival ultraviolet spectra of background quasarsfrom the Hubble Space Telescope/Cosmic Origins Spectrograph. We co-add spectraof different background sources at close projected angular separation on thesky. This novel stacking technique dramatically increases the signal-to-noiseratio of the spectra, allowing detection of low column density gas (down to$EW$ > 2 mA). We identify absorption as inflowing or outflowing, by usingblue/redshifted high velocity cloud (HVC) absorption components in theGalactocentric rest frame, respectively. The mass surface densities ofinflowing and outflowing gas both vary by more than an order of magnitudeacross the sky, with mean values of $\\\\langle \\\\Sigma_{in}\\\\rangle \\\\gtrsim10^{4.6\\\\pm0.1}$ $M_{\\\\odot}\\\\,\\\\mathrm{kpc}^{-2}$ for inflowing gas and $\\\\langle\\\\Sigma_{out}\\\\rangle \\\\gtrsim 10^{3.5\\\\pm 0.1}$ $M_{\\\\odot}\\\\,\\\\mathrm{kpc}^{-2}$ foroutflowing gas, respectively. The mass flow rate surface densities (mass flowrates per unit area) also show large variation across the sky with $\\\\langle\\\\dot{\\\\Sigma}(d)_{in}\\\\rangle \\\\gtrsim (10^{-3.6\\\\pm0.1})(d/12 \\\\mathrm{kpc})^{-1}M_{\\\\odot}\\\\,\\\\mathrm{kpc}^{-2} \\\\mathrm{yr}^{-1}$ for inflowing and $\\\\langle\\\\dot{\\\\Sigma}(d)_{out}\\\\rangle \\\\gtrsim (10^{-4.8\\\\pm0.1})(d/12\\\\,\\\\mathrm{kpc})^{-1}M_{\\\\odot}\\\\,\\\\mathrm{kpc}^{-2}\\\\,\\\\mathrm{yr}^{-1}$ for outflowing gas,respectively. The regions with highest surface mass density of inflowing gasare clustered at smaller angular scales ($\\\\theta < 40^\\\\circ$). This indicatesthat most of the mass in inflowing gas is confined to small, well-definedstructures, whereas the distribution of outflowing gas is spread more uniformlythroughout the sky. Our study confirms that the MW is predominantly accretinggas, but is also losing a non-negligible mass of gas via outflow.',\n",
              " 'We present medium-resolution, near-ultraviolet VLT/FLAMES observations of thestar USNO-A0600-15865535. We adapt a standard method of stellar typing to ourmeasurement of the shape of the Balmer epsilon absorption line to demonstratesthat USNO-A0600-15865535 is a blue horizontal branch star, residing in thelower stellar halo at a distance of 4.4 kpc from the Sun. We measure the H & Klines of singly-ionized calcium and find two isolated velocity components, oneoriginating in the disk, and one associated with high-velocity cloud complexWD. This detection demonstrated that complex WD is closer than ~4.4 kpc and isthe first distance constraint on the +100 km/s Galactic complex of clouds. Wefind that Complex WD is not in corotation with the Galactic disk as has beenassumed for decades. We examine a number of scenarios, and find that the mostlikely is that Complex WD was ejected from the solar neighborhood and is only afew kpc from the Sun.',\n",
              " 'We present rest-frame ultraviolet and optical spectroscopy of the brightestlensed galaxy yet discovered, at redshift z = 2.4. This source reveals acharacteristic, triple-peaked Lyman {\\\\alpha} profile which has been predictedby various theoretical works but to our knowledge has not been unambiguouslyobserved previously. The feature is well fit by a superposition of twocomponents: a double-peak profile emerging from substantial radiative transfer,and a narrow, central component resulting from directly escaping Lyman {\\\\alpha}photons; but is poorly fit by either component alone. We demonstrate that thefeature is unlikely to contain contamination from nearby sources, and that thecentral peak is unaffected by radiative transfer effects apart from very slightabsorption. The feature is detected at signal-to-noise ratios exceeding 80 perpixel at line center, and bears strong resemblance to synthetic profilespredicted by numerical models.',\n",
              " 'The ionizing photons produced by massive stars are key actors in galaxyevolution. Ionizing photon production and escape is poorly understood. Improvedspace-based, spatially-resolved, multiplexed spectroscopic capabilitiescovering observed wavelengths of 1000 to 3000 Angstrom, in concert withspectroscopy from the ELTs and JWST, would lead to definitive answers as to howionizing photons are produced and leaked, what populations of galaxies areresponsible for ionizing photon leakage, what determines whether escape ispossible, and how ionizing galaxy populations evolve over cosmic time.',\n",
              " \"We present Gemini/NIFS near-IR integral field spectroscopy of thefields-of-view around two AGNs behind the Fermi Bubbles (PDS 456 and1H1613-097) to search for molecular gas in the Milky Way's nuclear wind. Thesetwo AGN sightlines were selected by the presence of high-velocity neutral andionized gas seen in UV absorption. We do not detect any extended emission fromthe H2 ro-vibrational S(0) and S(1) lines at 2.224 and 2.122 microns in eitherdirection. For the S(1) line, the 3-sigma surface brightness limits derivedfrom spectra extracted across the full 3x3 arcsecond NIFS field-of-view are2.4e-17 erg/cm2/s/A/arcsec2 for PDS 456 and and 4.9e-18 erg/cm2/s/A/arcsec2 for1H1613-097. Given these non-detections, we conclude that CO emission-linestudies and H2 UV absorption-line studies are more promising approaches forcharacterizing the molecular gas in the Fermi Bubbles.\",\n",
              " 'Comparison of ISM absorption in the UV spectrum of LS 4825, a B1 Ib-II stard=21+/-5 kpc from the Sun toward l = 1.67 deg and b = -6.63 deg, with ISMabsorption toward an aligned foreground star at d < 7.0+/-1.7 kpc, allows us toisolate and study gas associated with the Milky Way nuclear wind. Spectra fromthe Space Telescope Imaging Spectrograph (STIS) show low ion absorption out tod < 7 kpc ( e.g., O I, C II, Mg II, Si II, Fe II, S II) only between 0 and 40km/s, while absorption at d > 7 kpc, ~1 kpc below the galactic plane, iscomplex and spans -290 to + 94 km/ s. The intermediate and high ions Si III, CIV, Si IV and N V show extremely strong absorption with multiple componentsfrom -283 to 107 km/ s implying that the ISM ~1 kpc below the galactic centerhas a substantial reservoir of plasma, and more gas containing C IV and N Vthan in the Carina OB1 association at z = 0 kpc. Abundances and physicalconditions are presented for many absorption components. The high ionabsorption traces cooling transition temperature plasma probably driven by theoutflowing hot gas, while the extraordinary large thermal pressure, p/k ~ 10^5cm^-3 K^-1 in an absorption component at -114 km/s probably arises from the rampressure of the outflowing hot gas. The observations are consistent with a flowwhose ionization structure in the high ions can be understood through acombination of non-equilibrium radiative cooling and turbulent mixing.',\n",
              " 'We report on observations made with the Cosmic Origins Spectrograph (COS) onthe Hubble Space Telescope (HST) using background QSOs to probe thecircum-galactic medium (CGM) around 17 low-redshift galaxies that areundergoing or have recently undergone a strong starburst (the COS-Burstprogram). The sightlines extend out to roughly the virial radius of the galaxyhalo. We construct control samples of normal star-forming low-redshift galaxiesfrom the COS/HST archive that match the starbursts in terms of galaxy stellarmass and impact parameter.  We find clear evidence that the CGM around the starbursts differssystematically compared to the control galaxies. The Ly$\\\\alpha$, Si III, C IV,and possibly O VI absorption-lines are stronger as a function of impactparameter, and the ratios of the equivalent widths of C IV/Ly$\\\\alpha$ and SiIII/Ly$\\\\alpha$ are both larger than in normal star-forming galaxies. We alsofind that the widths and the velocity offsets (relative to $v_{sys}$) of theLy$\\\\alpha$ absorption-lines are significantly larger in the CGM of thestarbursts, implying velocities of the absorbing material that are roughlytwice the halo virial velocity.  We show that these properties can be understood as a consequence of theinteraction between a starburst-driven wind and the pre-existing CGM. Theseresults underscore the importance of winds driven from intensely star-forminggalaxies in helping drive the evolution of galaxies and the intergalacticmedium. They also offer a new probe of the properties of starburst-driven windsand of the CGM itself.',\n",
              " 'The Fermi Bubbles are two giant gamma-ray emitting lobes extending55$^{\\\\circ}$ above and below the Galactic Center. While the Northern Bubble hasbeen extensively studied in ultraviolet (UV) absorption, little is known aboutthe gas kinematics of the southern Bubble. We use UV absorption-line spectrafrom the Cosmic Origins Spectrograph (COS) on the Hubble Space Telescope toprobe the southern Fermi Bubble using a sample of 17 background AGN projectedbehind or near the Bubble. We measure the incidence of high-velocity clouds(HVC), finding that four out of six sightlines passing through the Bubble showHVC absorption, versus six out of eleven passing outside. We find strongevidence that the maximum absolute LSR velocity of the HVC components decreasesas a function of galactic latitude within the Bubble, for both blueshifted andredshifted components, as expected for a decelerating outflow. We explorewhether the column-density ratios SiIV/SiIII, SiIV/SiII and SiIII/SiIIcorrelate with the absolute galactic latitude within the Bubble. These resultsdemonstrate the use of UV absorption-line spectroscopy to characterize thekinematics and ionization conditions of embedded clouds in the Galactic Centeroutflow.',\n",
              " 'Two giant plasma lobes, known as the Fermi Bubbles, extend 10 kpc above andbelow the Galactic Center. Since their discovery in X-rays in 2003 (and ingamma-rays in 2010), the Bubbles have been recognized as a new morphologicalfeature of our Galaxy and a striking example of energetic feedback from thenuclear region. They remain the subject of intense research and their originvia AGN activity or nuclear star formation is still debated. While imaging atgamma-ray, X-ray, microwave, and radio wavelengths has revealed theirmorphology and energetics, spectroscopy at radio and UV wavelengths hasrecently been used to study the kinematics and chemical abundances ofoutflowing gas clouds embedded in the Bubbles (the nuclear wind). Here weidentify the scientific themes that have emerged from the spectroscopicstudies, determine key open questions, and describe further observations neededin the next ten years to characterize the basic physical conditions in thenuclear wind and its impact on the rest of the Galaxy. Nuclear winds areubiquitous in galaxies, and the Galactic Center represents the best opportunityto study the constitution and structure of a nuclear wind in close detail.',\n",
              " 'We present new calculations of the mass inflow and outflow rates around theMilky Way, derived from a catalog of ultraviolet metal-line high velocityclouds (HVCs). These calculations are conducted by transforming the HVCvelocities into the Galactic Standard of Rest (GSR) reference frame,identifying inflowing (v_GSR < 0 km/s) and outflowing (v_GSR > 0 km/s)populations, and using observational constraints on the distance, metallicity,dust content, covering fractions, and total hydrogen column density of eachpopulation. After removing HVCs associated with the Magellanic Stream and theFermi Bubbles, we find inflow and outflow rates in cool (T~10^4 K) ionized gasof dM_in/dt >~ 0.53+/-0.17 (d/12 kpc) (Z/0.2 Z_sun)^-1 M_sun/yr and dM_out/dt>~ 0.16+/-0.06 (d/12 kpc) (Z/0.5 Z_sun)^-1 M_sun/yr. The excess of inflowingover outflowing gas suggests that the Milky Way is currently in aninflow-dominated phase, but the presence of substantial mass flux in bothdirections supports a Galactic fountain model, in which gas is constantlyrecycled between the disk and the halo. We also find that the metal flux inboth directions (in and out) is indistinguishable. By comparing the outflowrate to the Galactic star formation rate, we present the first estimate of themass loading factor (etc_HVC) of the disk-wide Milky Way wind, finding eta_HVC>~ 0.10+/-0.06 (d/12 kpc) (Z/0.5 Z_sun)^-1. Including the contributions fromlow- and intermediate-velocity clouds and from hot gas would increase theseinflow and outflow estimates.',\n",
              " 'Present-day galaxies are surrounded by cool and enriched halo gas extendingto hundreds of kiloparsecs. This halo gas is thought to be the dominantreservoir of material available to fuel future star formation, but directconstraints on its mass and physical properties have been difficult to obtain.We report the detection of a fast radio burst (FRB 181112) with arcsecondprecision, which passes through the halo of a foreground galaxy. Analysis ofthe burst shows the halo gas has low net magnetization and turbulence. Ourresults imply predominantly diffuse gas in massive galactic halos, even thosehosting active supermassive black holes, contrary to some previous results.',\n",
              " 'Modern cosmology predicts that matter in our Universe has assembled todayinto a vast network of filamentary structures colloquially termed the CosmicWeb. Because this matter is either electromagnetically invisible (i.e., dark)or too diffuse to image in emission, tests of this cosmic web paradigm arelimited. Wide-field surveys do reveal web-like structures in the galaxydistribution, but these luminous galaxies represent less than 10% of baryonicmatter. Statistics of absorption by the intergalactic medium (IGM) viaspectroscopy of distant quasars support the model yet have not conclusivelytied the diffuse IGM to the web. Here, we report on a new method inspired bythe Physarum polycephalum slime mold that is able to infer the density field ofthe Cosmic Web from galaxy surveys. Applying our technique to galaxy andabsorption-line surveys of the local Universe, we demonstrate that the bulk ofthe IGM indeed resides in the Cosmic Web. From the outskirts of Cosmic Webfilaments, at approximately the cosmic mean matter density (rho_m) and approx.5 virial radii from nearby galaxies, we detect an increasing H I absorptionsignature towards higher densities and the circumgalactic medium, to approx.200 rho_m. However, the absorption is suppressed within the densestenvironments, suggesting shock-heating and ionization deep within filamentsand/or feedback processes within galaxies.',\n",
              " 'We present a novel intelligent quasar continuum neural network (iQNet),predicting the intrinsic continuum of any quasar in the rest-frame wavelengthrange 1020 Angstroms $\\\\leq \\\\lambda \\\\leq$ 1600 Angstroms. We train this networkusing high-resolution Hubble Space Telescope/Cosmic Origin Spectrographultraviolet quasar spectra at low redshift ($z \\\\sim 0.2$) from the HubbleSpectroscopic Legacy Archive, and apply it to predict quasar continua fromdifferent astronomical surveys. We utilize the HSLA quasar spectra that arewell-defined in the rest-frame wavelength range [1020, 1600] Angstroms with anoverall median signal-to-noise ratio of at least five. The iQNet achieves amedian AFFE of 2.24% on the training quasar spectra, and 4.17% on the testingquasar spectra. We apply iQNet and predict the continua of $\\\\sim$3200 SDSS-DR16quasar spectra at higher redshift ($2< z \\\\leq 5$) and measure the redshiftevolution of mean transmitted flux ($< F >$) in the Ly-$\\\\alpha$ forest region.We measure a gradual evolution of $< F >$ with redshift, which we characterizeas a power-law fit to the effective optical depth of the Ly-$\\\\alpha$ forest.Our measurements are broadly consistent with other estimates of $<F>$ in theliterature, but provide a more accurate measurement as we are directlymeasuring the quasar continuum where there is minimum contamination from theLy-$\\\\alpha$ forest. This work proves that the deep learning iQNet model canpredict the quasar continuum with high accuracy and shows the viability of suchmethods for quasar continuum prediction.',\n",
              " 'We present the results of the COS Intragroup Medium (COS-IGrM) Survey thatused the Cosmic Origins Spectrograph on the Hubble Space Telescope to observe asample of 18 UV bright quasars, each probing the intragroup medium (IGrM) of agalaxy group. We detect Ly$\\\\alpha$, C II, N V, Si II, Si III, and O VI inmultiple sightlines. The highest ionization species detected in our data is OVI, which was detected in 8 out of 18 quasar sightlines. The wide range ofionization states observed provide evidence that the IGrM is patchy andmultiphase. We find that the O VI detections generally align with radiativelycooling gas between $10^{5.8}$ and $10^6$ K. The lack of O VI detections in 10of the 18 groups illustrates that O VI may not be the ideal tracer of thevolume filling component of the IGrM. Instead, it either exists at trace levelsin a hot IGrM or is generated in the boundary between the hotter IGrM andcooler gas.',\n",
              " \"This study addresses how the incidence rate of strong O VI absorbers in agalaxy's circumgalactic medium (CGM) depends on galaxy mass and, independently,on the amount of star formation in the galaxy. We use HST/COS absorptionspectroscopy of quasars to measure O VI absorption within 400 projected kpc and300 km s$^{-1}$ of 52 $M_{*}\\\\sim 10^{10}$ $M_\\\\odot$ galaxies. The galaxies haveredshifts $0.12<z<0.6$, stellar masses $10^{10.1} < M_* < 10^{10.9}$ $M_\\\\odot$,and spectroscopic classifications as star-forming or passive. We compare theincidence rates of high column density O VI absorption ($N_{\\\\rm O\\\\, VI} \\\\geq10^{14.3}$ cm$^{-2}$) near star-forming and passive galaxies in two narrowstellar mass ranges and, separately, in a matched halo mass range. In all threemass ranges, the O VI covering fraction within 150 kpc is higher aroundstar-forming galaxies than around passive galaxies with greater than$3\\\\sigma$-equivalent statistical significance. On average, the CGM of $M_*\\\\sim10^{10}$ $M_\\\\odot$ star-forming galaxies contains more O VI than the CGM ofpassive galaxies with the same mass. This difference is evidence for a CGMtransformation that happens together with galaxy quenching and is not drivenprimarily by halo mass.\",\n",
              " 'We combine datasets from the CGM$^{2}$ and CASBaH surveys to model atransition point, $R_{\\\\rm cross}$, between circumgalactic and intergalacticmedia (CGM and IGM, respectively). In total, our data consist of 7244 galaxiesat z < 0.5 with precisely measured spectroscopic redshifts, all having impactparameters of 0.01 - 20 comoving Mpc from 28 QSO sightlines withhigh-resolution UV spectra that cover H I Ly$\\\\alpha$. Our best-fitting model isan exclusionary two-component model that combines a 3D absorber-galaxy crosscorrelation function with a simple Gaussian profile at inner radii to representthe CGM. By design, this model gives rise to a determination of $R_{\\\\rm cross}$as a function of galaxy stellar mass, which can be interpreted as the boundarybetween the CGM and IGM. For galaxies with $10^8 \\\\leq M_{\\\\star}/M_{\\\\odot} \\\\leq10^{10.5}$, we find that $R_{\\\\rm cross}(M_{\\\\star}) \\\\approx 2 \\\\pm 0.6 R_{\\\\rmvir}$. Additionally, we find excellent agreement between $R_{\\\\rmcross}(M_{\\\\star})$ and the theoretically-determined splashback radius forgalaxies in this mass range. Overall, our results favor models of galaxyevolution at z < 0.5 that distribute $T \\\\approx 10^{4}$K gas to distancesbeyond the virial radius.',\n",
              " 'The OpenITI team has achieved Optical Character Recognition (OCR) accuracyrates for classical Arabic-script texts in the high nineties. These numbers arebased on our tests of seven different Arabic-script texts of varying qualityand typefaces, totaling over 7,000 lines. These accuracy rates not onlyrepresent a distinct improvement over the actual accuracy rates of the variousproprietary OCR options for classical Arabic-script texts, but, equallyimportant, they are produced using an open-source OCR software, thus enablingus to make this Arabic-script OCR technology freely available to the broaderIslamic, Persian, and Arabic Studies communities.',\n",
              " 'In this paper, we present an approach for dynamic exploration and mapping ofunknown environments using a swarm of biobotic sensing agents, with astochastic natural motion model and a leading agent (e.g., an unmanned aerialvehicle). The proposed robust mapping technique constructs a topological map ofthe environment using only encounter information from the swarm. A slidingwindow strategy is adopted in conjunction with a topological mapping strategybased on local interactions among the swarm in a coordinate-free fashion toobtain local maps of the environment. These maps are then merged into a globaltopological map which can be visualized using a graphical representation thatintegrates geometric as well as topological feature of the environment.Localized robust topological features are extracted using tools fromtopological data analysis. Simulation results have been presented to illustrateand verify the correctness of our dynamic mapping algorithm.',\n",
              " 'In this study, we present and analyze a framework for geometric andtopological estimation for mapping of unknown environments. We consider agentsmimicking motion behaviors of cyborg insects, known as biobots, and exploitcoordinate-free local interactions among them to infer geometric andtopological information about the environment, under minimal sensing andlocalization constraints. Local interactions are used to create a graphicalrepresentation referred to as the encounter graph. A metric is estimated overthe encounter graph of the agents in order to construct a geometric point cloudusing manifold learning techniques. Topological data analysis (TDA), inparticular persistent homology, is used in order to extract topologicalfeatures of the space and a classification method is proposed to infer robustfeatures of interest (e.g. existence of obstacles). We examine the asymptoticbehavior of the proposed metric in terms of the convergence to the geodesicdistances in the underlying manifold of the domain, and provide stabilityanalysis results for the topological persistence. The proposed framework andits convergences and stability analysis are demonstrated through numericalsimulations and experiments.',\n",
              " 'Animal Assisted Interventions (AAIs) involve pleasant interactions betweenhumans and animals and can potentially benefit both types of participants.Research in this field may help to uncover universal insights aboutcross-species bonding, dynamic affect detection, and the influence ofenvironmental factors on dyadic interactions. However, experiments evaluatingthese outcomes are limited to methodologies that are qualitative, subjective,and cumbersome due to the ergonomic challenges related to attaching sensors tothe body. Current approaches in AAIs also face challenges when translatingbeyond controlled clinical environments or research contexts. These also oftenneglect the measurements from the animal throughout the interaction. Here, wepresent our preliminary effort toward a contact-free approach to facilitate AAIassessment via the physiological sensing of humans and canines usingconsumer-grade cameras. This initial effort focuses on verifying thetechnological feasibility of remotely sensing the heart rate signal of thehuman subject and the breathing rate signal of the dog subject while they areinteracting. Small amounts of motion such as patting and involuntary bodyshaking or movement can be tolerated with our custom designed vision-basedalgorithms. The experimental results show that the physiological measurementsobtained by our algorithms were consistent with those provided by the standardreference devices. With further validation and expansion to other physiologicalparameters, the presented approach offers great promise for many scenarios fromthe AAI research space to veterinary, surgical, and clinical applications.',\n",
              " 'Ultrahigh-power terahertz (THz) radiation sources are essential for manyapplications, such as nonlinear THz physics, THz-wave based compactaccelerators, etc. However, until now none of THz sources reported, whetherbased upon large-scale accelerators or high power lasers, have produced THzpulses with energies above the millijoule (mJ) barrier. Here we report on theefficient generation of low-frequency (<3 THz) THz pulses with unprecedentedlyhigh energies over 50 mJ. The THz radiation is produced by coherent transitionradiation of a picosecond laser-accelerated ultra-bright bunch of relativisticelectrons from a solid target. Such high energy THz pulses can not only triggervarious nonlinear dynamics in matter, but also open up a new research field ofrelativistic THz optics.',\n",
              " 'For the first time, a global model of electromagnetic pulse (EMP) emissionconnects charge separation in the laser target to quantitative measurements ofthe electromagnetic field. We present a frequency-domain dipole antenna modelwhich predicts the quantity of charge accumulated in a laser target as well asthe EMP amplitude and frequency. The model is validated against measurementsfrom several high-intensity laser facilities, providing insight into targetphysics and informing the design of next-generation ultra-intense laserfacilities. EMP amplitude is proportional to the total charge accumulated onthe target, but we demonstrate that it is not directly affected by targetcharging time (and therefore the laser pulse duration) provided the chargingtime is shorter than the antenna characteristic time. We propose twoindependent methods for estimating the charging time based on the laser pulseduration. We also investigate the impact of target holder geometry on EMPsusing cylindrical, conical and helical holders.',\n",
              " 'We describe the direct measurement of the expulsion of a magnetic field froma plasma driven by heat flow. Using a laser to heat a column of gas within anapplied magnetic field, we isolate Nernst advection and show how it changes thefield over a nanosecond timescale. Reconstruction of the magnetic field mapfrom proton radiographs demonstrates that the field is advected by heat flow inadvance of the plasma expansion. This changes the dynamics of high energydensity plasmas, in which heat flows and fields are strongly coupled, and maydisrupt magnetised inertial confinement fusion schemes.',\n",
              " 'A quasi-linear hyperbolic partial differential equation with a discontinuousflux models geologic carbon dioxide migration and storage. Dual flux curvescharacterize the model, giving rise to flux discontinuities. One convex fluxdescribes the invasion of the plume into pore space, and the other captures theflow as the plume leaves carbon dioxide bubbles behind, which are then trappedin the pore space. We investigate the method of characteristics, the structureof shock and rarefaction waves, and the result of binary wave interactions. Thedual flux property introduces unexpected differences between the structure ofthese solutions and those of a scalar conservation law with a convex flux.During our analysis, we introduce a new construction of cross-hatchcharacteristics in regions of the space-time plane where the solution isconstant, and there are two characteristic speeds. This construction is used togeneralize the notion of the Lax entropy condition for admissible shocks, andis crucial to continuing the propagation of a shock wave if its speed becomescharacteristic.',\n",
              " \"One route to numerically propagating quantum systems is time-dependentdensity functional theory (TDDFT). The application of TDDFT to a particularsystem's time evolution is predicated on $V$-representability which we haveanalyzed in a previous publication. Here we describe a newly developed solverfor the scalar time-dependent Kohn-Sham potential. We present and interpret theforce-balance equation central to our numerical method, describe details of itsimplementation, and present illustrative numerical results for one- andtwo-electron systems. A new characterization of $V$-representability forone-electron systems is also included along with possible improvements andfuture directions.\",\n",
              " 'In this work we empirically measure the detection efficiency of Keplerpipeline used to create the final Kepler Threshold Crossing Event (TCE; Twickenet al. 2016) and planet candidate catalogs (Thompson et al. 2018), a necessaryingredient for occurrence rate calculations using these lists. By injectingsimulated signals into the calibrated pixel data and processing those pixelsthrough the pipeline as normal, we quantify the detection probability ofsignals as a function of their signal strength and orbital period. In additionwe investigate the dependence of the detection efficiency on parameters of thetarget stars and their location in the Kepler field of view. We find that theend-of-mission version of the Kepler pipeline returns to a high overalldetection efficiency, averaging a 90-95% rate of detection for strong signalsacross a wide variety of parameter space. We find a weak dependence of thedetection efficiency on the number of transits contributing to the signal andthe orbital period of the signal, and a stronger dependence on the stellareffective temperature and correlated noise properties. We also find a weakdependence of the detection efficiency on the position within the field ofview. By restricting the Kepler stellar sample to stars with well-behavedcorrelated noise properties, we can define a set of stars with high detectionefficiency for future occurrence rate calculations.',\n",
              " 'We provide a new version of delta theorem, that takes into account of highdimensional parameter estimation. We show that depending on the structure ofthe function, the limits of functions of estimators have faster or slower rateof convergence than the limits of estimators. We illustrate this via twoexamples. First, we use it for testing in high dimensions, and second inestimating large portfolio risk. Our theorem works in the case of larger numberof parameters, $p$, than the sample size, $n$: $p>n$.',\n",
              " 'We provide an upper bound as a random variable for the functions ofestimators in high dimensions. This upper bound may help establish the rate ofconvergence of functions in high dimensions. The upper bound random variablemay converge faster, slower, or at the same rate as estimators depending on thebehavior of the partial derivative of the function. We illustrate this viathree examples. The first two examples use the upper bound for testing in highdimensions, and third example derives the estimated out-of-sample variance oflarge portfolios. All our results allow for a larger number of parameters, p,than the sample size, n.',\n",
              " 'We consider situations where a user feeds her attributes to a machinelearning method that tries to predict her best option based on a random sampleof other users. The predictor is incentive-compatible if the user has noincentive to misreport her covariates. Focusing on the popular Lasso estimationtechnique, we borrow tools from high-dimensional statistics to characterizesufficient conditions that ensure that Lasso is incentive compatible in largesamples. We extend our results to the Conservative Lasso estimator and providenew moment bounds for this generalized weighted version of Lasso. Our resultsshow that incentive compatibility is achieved if the tuning parameter is keptabove some threshold. We present simulations that illustrate how this can bedone in practice.',\n",
              " 'We provide a new theory for nodewise regression when the residuals from afitted factor model are used. We apply our results to the analysis of theconsistency of Sharpe ratio estimators when there are many assets in aportfolio. We allow for an increasing number of assets as well as timeobservations of the portfolio. Since the nodewise regression is not feasibledue to the unknown nature of idiosyncratic errors, we provide afeasible-residual-based nodewise regression to estimate the precision matrix oferrors which is consistent even when number of assets, p, exceeds the time spanof the portfolio, n. In another new development, we also show that theprecision matrix of returns can be estimated consistently, even with anincreasing number of factors and p>n. We show that: (1) with p>n, the Sharperatio estimators are consistent in global minimum-variance and mean-varianceportfolios; and (2) with p>n, the maximum Sharpe ratio estimator is consistentwhen the portfolio weights sum to one; and (3) with p<<n, themaximum-out-of-sample Sharpe ratio estimator is consistent.',\n",
              " 'In this paper, we introduce structured sparsity estimators in GeneralizedLinear Models. Structured sparsity estimators in the least squares loss areintroduced by Stucky and van de Geer (2018) recently for fixed design andnormal errors. We extend their results to debiased structured sparsityestimators with Generalized Linear Model based loss. Structured sparsityestimation means penalized loss functions with a possible sparsity structureused in the chosen norm. These include weighted group lasso, lasso and normsgenerated from convex cones. The significant difficulty is that it is not clearhow to prove two oracle inequalities. The first one is for the initialpenalized Generalized Linear Model estimator. Since it is not clear how aparticular feasible-weighted nodewise regression may fit in an oracleinequality for penalized Generalized Linear Model, we need a second oracleinequality to get oracle bounds for the approximate inverse for the sampleestimate of second-order partial derivative of Generalized Linear Model.  Our contributions are fivefold: 1. We generalize the existing oracleinequality results in penalized Generalized Linear Models by proving theunderlying conditions rather than assuming them. One of the key issues is theproof of a sample one-point margin condition and its use in an oracleinequality. 2. Our results cover even non sub-Gaussian errors and regressors.3. We provide a feasible weighted nodewise regression proof which generalizesthe results in the literature from a simple l_1 norm usage to norms generatedfrom convex cones. 4. We realize that norms used in feasible nodewiseregression proofs should be weaker or equal to the norms in penalizedGeneralized Linear Model loss. 5. We can debias the first step estimator viagetting an approximate inverse of the singular-sample second order partialderivative of Generalized Linear Model loss.',\n",
              " 'In this paper, we connect deep learning literature with non-linear factormodels and show that deep learning estimation makes a substantial improvementin the non-linear additive factor model literature. We provide bounds on theexpected risk and show that these upper bounds are uniform over a set ofmultiple response variables by extending Schmidt-Hieber (2020) theorems.  We show that our risk bound does not depend on the number of factors.  In order to construct a covariance matrix estimator for asset returns, wedevelop a novel data-dependent estimator of the error covariance matrix in deepneural networks. The estimator refers to a flexible adaptive thresholdingtechnique which is robust to outliers in the innovations. We prove that theestimator is consistent in spectral norm. Then using that result, we showconsistency and rate of convergence of covariance matrix and precision matrixestimator for asset returns. The rate of convergence in both results do notdepend on the number of factors, hence ours is a new result in the factor modelliterature due to the fact that number of factors are impediment to betterestimation and prediction. Except from the precision matrix result, all ourresults are obtained even with number of assets are larger than the time span,and both quantities are growing.  Various Monte Carlo simulations confirm our large sample findings and revealsuperior accuracies of the DNN-FM in estimating the true underlying functionalform which connects the factors and observable variables, as well as thecovariance and precision matrix compared to competing approaches. Moreover, inan out-of-sample portfolio forecasting application it outperforms in most ofthe cases alternative portfolio strategies in terms of out-of-sample portfoliostandard deviation and Sharpe ratio.',\n",
              " 'This paper consider penalized empirical loss minimization of convex lossfunctions with unknown non-linear target functions. Using the elastic netpenalty we establish a finite sample oracle inequality which bounds the loss ofour estimator from above with high probability. If the unknown target is linearthis inequality also provides an upper bound of the estimation error of theestimated parameter vector. These are new results and they generalize theeconometrics and statistics literature. Next, we use the non-asymptotic resultsto show that the excess loss of our estimator is asymptotically of the sameorder as that of the oracle. If the target is linear we give sufficientconditions for consistency of the estimated parameter vector. Next, we brieflydiscuss how a thresholded version of our estimator can be used to performconsistent variable selection. We give two examples of loss functions coveredby our framework and show how penalized nonparametric series estimation iscontained as a special case and provide a finite sample upper bound on the meansquare error of the elastic net series estimator.',\n",
              " 'In this paper we consider the conservative Lasso which we argue penalizesmore correctly than the Lasso and show how it may be desparsified in the senseof van de Geer et al. (2014) in order to construct asymptotically honest(uniform) confidence bands. In particular, we develop an oracle inequality forthe conservative Lasso only assuming the existence of a certain number ofmoments. This is done by means of the Marcinkiewicz-Zygmund inequality. Weallow for heteroskedastic non-subgaussian error terms and covariates. Next, wedesparsify the conservative Lasso estimator and derive the asymptoticdistribution of tests involving an increasing number of parameters. Oursimulations reveal that the desparsified conservative Lasso estimates theparameters more precisely than the desparsified Lasso, has better sizeproperties and produces confidence bands with superior coverage rates.',\n",
              " 'We propose a new estimator, the thresholded scaled Lasso, in high dimensionalthreshold regressions. First, we establish an upper bound on the $\\\\ell_\\\\infty$estimation error of the scaled Lasso estimator of Lee et al. (2012). This is anon-trivial task as the literature on high-dimensional models has focusedalmost exclusively on $\\\\ell_1$ and $\\\\ell_2$ estimation errors. We show thatthis sup-norm bound can be used to distinguish between zero and non-zerocoefficients at a much finer scale than would have been possible usingclassical oracle inequalities. Thus, our sup-norm bound is tailored toconsistent variable selection via thresholding.  Our simulations show that thresholding the scaled Lasso yields substantialimprovements in terms of variable selection. Finally, we use our estimator toshed further empirical light on the long running debate on the relationshipbetween the level of debt (public and private) and GDP growth.',\n",
              " 'This paper considers inference in a partially identified moment (in)equalitymodel with many moment inequalities. We propose a novel two-step inferenceprocedure that combines the methods proposed by Chernozhukov, Chetverikov andKato (2018a) (CCK18, hereafter) with a first step moment inequality selectionbased on the Lasso. Our method controls asymptotic size uniformly, both inunderlying parameter and data distribution. Also, the power of our methodcompares favorably with that of the corresponding two-step method in CCK18 forlarge parts of the parameter space, both in theory and in simulations. Finally,we show that our Lasso-based first step can be implemented by thresholdingstandardized sample averages, and so it is straightforward to implement.',\n",
              " 'This paper investigates the large sample properties of the variance, weights,and risk of high-dimensional portfolios where the inverse of the covariancematrix of excess asset returns is estimated using a technique called nodewiseregression. Nodewise regression provides a direct estimator for the inversecovariance matrix using the Least Absolute Shrinkage and Selection Operator(Lasso) of Tibshirani (1994) to estimate the entries of a sparse precisionmatrix. We show that the variance, weights, and risk of the global minimumvariance portfolios and the Markowitz mean-variance portfolios are consistentlyestimated with more assets than observations. We show, empirically, that thenodewise regression-based approach performs well in comparison to factor modelsand shrinkage methods.',\n",
              " 'This paper proposes a desparsified GMM estimator for estimatinghigh-dimensional regression models allowing for, but not requiring, many moreendogenous regressors than observations. We provide finite sample upper boundson the estimation error of our estimator and show how asymptotically uniformlyvalid inference can be conducted in the presence of conditionallyheteroskedastic error terms. We do not require the projection of the endogenousvariables onto the linear span of the instruments to be sparse; that is we donot impose the instruments to be sparse for our inferential procedure to beasymptotically valid. Furthermore, the variables of the model are not requiredto be sub-gaussian and we also explain how our results carry over to theclassic linear dynamic panel data model. Simulations show that our estimatorhas a low mean square error and does well in terms of size and power of thetests constructed based on the estimator.',\n",
              " 'Motivation: Site directed mutagenesis is widely used to understand thestructure and function of biomolecules. Computational prediction of proteinmutation impacts offers a fast, economical and potentially accurate alternativeto laboratory mutagenesis. Most existing methods rely on geometricdescriptions, this work introduces a topology based approach to provide anentirely new representation of protein mutation impacts that could not beobtained from conventional techniques. Results: Topology based mutationpredictor (T-MP) is introduced to dramatically reduce the geometric complexityand number of degrees of freedom of proteins, while element specific persistenthomology is proposed to retain essential biological information. The presentapproach is found to outperform other existing methods in globular proteinmutation impact predictions. A Pearson correlation coefficient of 0.82 with anRMSE of 0.92 kcal/mol is obtained on a test set of 350 mutation samples. Forthe prediction of membrane protein stability changes upon mutation, theproposed topological approach has a 84% higher Pearson correlation coefficientthan the current state-of-the-art empirical methods, achieving a Pearsoncorrelation of 0.57 and an RMSE of 1.09 kcal/mol in a 5-fold cross validationon a set of 223 membrane protein mutation samples.',\n",
              " 'Protein-ligand binding is a fundamental biological process that is paramountto many other biological processes, such as signal transduction, metabolicpathways, enzyme construction, cell secretion, gene expression, etc. Accurateprediction of protein-ligand binding affinities is vital to rational drugdesign and the understanding of protein-ligand binding and binding inducedfunction. Existing binding affinity prediction methods are inundated withgeometric detail and involve excessively high dimensions, which underminestheir predictive power for massive binding data. Topology provides an ultimatelevel of abstraction and thus incurs too much reduction in geometricinformation. Persistent homology embeds geometric information into topologicalinvariants and bridges the gap between complex geometry and abstract topology.However, it over simplifies biological information. This work introduceselement specific persistent homology (ESPH) to retain crucial biologicalinformation during topological simplification. The combination of ESPH andmachine learning gives rise to one of the most efficient and powerful tools forrevealing protein-ligand binding mechanism and for predicting bindingaffinities.',\n",
              " 'Although deep learning approaches have had tremendous success in image, videoand audio processing, computer vision, and speech recognition, theirapplications to three-dimensional (3D) biomolecular structural data sets havebeen hindered by the entangled geometric complexity and biological complexity.We introduce topology, i.e., element specific persistent homology (ESPH), tountangle geometric complexity and biological complexity. ESPH represents 3Dcomplex geometry by one-dimensional (1D) topological invariants and retainscrucial biological information via a multichannel image representation. It isable to reveal hidden structure-function relationships in biomolecules. Wefurther integrate ESPH and convolutional neural networks to construct amultichannel topological neural network (TopologyNet) for the predictions ofprotein-ligand binding affinities and protein stability changes upon mutation.To overcome the limitations to deep learning arising from small and noisytraining sets, we present a multitask topological convolutional neural network(MT-TCNN). We demonstrate that the present TopologyNet architectures outperformother state-of-the-art methods in the predictions of protein-ligand bindingaffinities, globular protein mutation impacts, and membrane protein mutationimpacts.',\n",
              " 'This work introduces a number of algebraic topology approaches, such asmulticomponent persistent homology, multi-level persistent homology andelectrostatic persistence for the representation, characterization, anddescription of small molecules and biomolecular complexes. Multicomponentpersistent homology retains critical chemical and biological information duringthe topological simplification of biomolecular geometric complexity.Multi-level persistent homology enables a tailored topological description ofinter- and/or intra-molecular interactions of interest. Electrostaticpersistence incorporates partial charge information into topologicalinvariants. These topological methods are paired with Wasserstein distance tocharacterize similarities between molecules and are further integrated with avariety of machine learning algorithms, including k-nearest neighbors, ensembleof trees, and deep convolutional neural networks, to manifest their descriptiveand predictive powers for chemical and biological problems. Extensive numericalexperiments involving more than 4,000 protein-ligand complexes from the PDBBinddatabase and near 100,000 ligands and decoys in the DUD database are performedto test respectively the scoring power and the virtual screening power of theproposed topological approaches. It is demonstrated that the present approachesoutperform the modern machine learning based methods in protein-ligand bindingaffinity predictions and ligand-decoy discrimination.',\n",
              " 'Time dependence is a universal phenomenon in nature, and a variety ofmathematical models in terms of dynamical systems have been developed tounderstand the time-dependent behavior of real-world problems. Originallyconstructed to analyze the topological persistence over spatial scales,persistent homology has rarely been devised for time evolution. We propose theuse of a new filtration function for persistent homology which takes as inputthe adjacent oscillator trajectories of a dynamical system. We also regulatethe dynamical system by a weighted graph Laplacian matrix derived from thenetwork of interest, which embeds the topological connectivity of the networkinto the dynamical system. The resulting topological signatures, which we callevolutionary homology (EH) barcodes, reveal the topology-function relationshipof the network and thus give rise to the quantitative analysis of nodalproperties. The proposed EH is applied to protein residue networks for proteinthermal fluctuation analysis, rendering the most accurate B-factor predictionof a set of 364 proteins. This work extends the utility of dynamical systems tothe quantitative modeling and analysis of realistic physical systems.',\n",
              " 'Optimal Transport, a theory for optimal allocation of resources, is widelyused in various fields such as astrophysics, machine learning, and imagingscience. However, many applications impose elementwise constraints on thetransport plan which traditional optimal transport cannot enforce. Here weintroduce Supervised Optimal Transport (sOT) that formulates a constrainedoptimal transport problem where couplings between certain elements areprohibited according to specific applications. sOT is proved to be equivalentto an $l^1$ penalized optimization problem, from which efficient algorithms aredesigned to solve its entropy regularized formulation. We demonstrate thecapability of sOT by comparing it to other variants and extensions oftraditional OT in color transfer problem. We also study the barycenter problemin sOT formulation, where we discover and prove a unique reverse and portionselection (control) mechanism. Supervised optimal transport is broadlyapplicable to applications in which constrained transport plan is involved andthe original unit should be preserved by avoiding normalization.',\n",
              " 'Persistent homology is a powerful tool for characterizing the topology of adata set at various geometric scales. When applied to the description ofmolecular structures, persistent homology can capture the multiscale geometricfeatures and reveal certain interaction patterns in terms of topologicalinvariants. However, in addition to the geometric information, there is a widevariety of non-geometric information of molecular structures, such as elementtypes, atomic partial charges, atomic pairwise interactions, and electrostaticpotential function, that is not described by persistent homology. Althoughelement specific homology and electrostatic persistent homology can encode somenon-geometric information into geometry based topological invariants, it isdesirable to have a mathematical framework to systematically embed bothgeometric and non-geometric information, i.e., multicomponent heterogeneousinformation, into unified topological descriptions. To this end, we propose amathematical framework based on persistent cohomology. In our framework,non-geometric information can be either distributed globally or resided locallyon the datasets in the geometric sense and can be properly defined ontopological spaces, i.e., simplicial complexes. Using the proposed persistentcohomology based framework, enriched barcodes are extracted from datasets torepresent heterogeneous information. We consider a variety of datasets tovalidate the present formulation and illustrate the usefulness of the proposedpersistent cohomology. It is found that the proposed framework using cohomologyboosts the performance of persistent homology based methods in theprotein-ligand binding affinity prediction on massive biomolecular datasets.',\n",
              " 'Protein function and dynamics are closely related to its sequence andstructure. However prediction of protein function and dynamics from itssequence and structure is still a fundamental challenge in molecular biology.Protein classification, which is typically done through measuring thesimilarity be- tween proteins based on protein sequence or physicalinformation, serves as a crucial step toward the understanding of proteinfunction and dynamics. Persistent homology is a new branch of algebraictopology that has found its success in the topological data analysis in avariety of disciplines, including molecular biology. The present work exploresthe potential of using persistent homology as an indepen- dent tool for proteinclassification. To this end, we propose a molecular topological fingerprintbased support vector machine (MTF-SVM) classifier. Specifically, we constructmachine learning feature vectors solely from protein topological fingerprints,which are topological invariants generated during the filtration process. Tovalidate the present MTF-SVM approach, we consider four types of problems.First, we study protein-drug binding by using the M2 channel protein ofinfluenza A virus. We achieve 96% accuracy in discriminating drug bound andunbound M2 channels. Additionally, we examine the use of MTF-SVM for theclassification of hemoglobin molecules in their relaxed and taut forms andobtain about 80% accuracy. The identification of all alpha, all beta, andalpha-beta protein domains is carried out in our next study using 900 proteins.We have found a 85% success in this identifica- tion. Finally, we apply thepresent technique to 55 classification tasks of protein superfamilies over 1357samples. An average accuracy of 82% is attained. The present study establishescomputational topology as an independent and effective alternative for proteinclassification.',\n",
              " 'Advanced mathematics, such as multiscale weighted colored graph and elementspecific persistent homology, and machine learning including deep neuralnetworks were integrated to construct mathematical deep learning models forpose and binding affinity prediction and ranking in the last two D3R grandchallenges in computer-aided drug design and discovery. D3R Grand Challenge 2(GC2) focused on the pose prediction and binding affinity ranking and freeenergy prediction for Farnesoid X receptor ligands. Our models obtained the topplace in absolute free energy prediction for free energy Set 1 in Stage 2. Thelatest competition, D3R Grand Challenge 3 (GC3), is considered as the mostdifficult challenge so far. It has 5 subchallenges involving Cathepsin S andfive other kinase targets, namely VEGFR2, JAK2, p38-$\\\\alpha$, TIE2, and ABL1.There is a total of 26 official competitive tasks for GC3. Our predictions wereranked 1st in 10 out of 26 official competitive tasks.',\n",
              " 'High-dimensional multimodal data arises in many scientific fields. Theintegration of multimodal data becomes challenging when there is no knowncorrespondence between the samples and the features of different datasets. Totackle this challenge, we introduce AVIDA, a framework for simultaneouslyperforming data alignment and dimension reduction. In the numericalexperiments, Gromov-Wasserstein optimal transport and t-distributed stochasticneighbor embedding are used as the alignment and dimension reduction modulesrespectively. We show that AVIDA correctly aligns high-dimensional datasetswithout common features with four synthesized datasets and two real multimodalsingle-cell datasets. Compared to several existing methods, we demonstrate thatAVIDA better preserves structures of individual datasets, especially distinctlocal structures in the joint low-dimensional visualization, while achievingcomparable alignment performance. Such a property is important in multimodalsingle-cell data analysis as some biological processes are uniquely captured byone of the datasets. In general applications, other methods can be used for thealignment and dimension reduction modules.',\n",
              " 'Recently, machine learning (ML) has established itself in various worldwidebenchmarking competitions in computational biology, including CriticalAssessment of Structure Prediction (CASP) and Drug Design Data Resource (D3R)Grand Challenges. However, the intricate structural complexity and high MLdimensionality of biomolecular datasets obstruct the efficient application ofML algorithms in the field. In addition to data and algorithm, an efficient MLmachinery for biomolecular predictions must include structural representationas an indispensable component. Mathematical representations that simplify thebiomolecular structural complexity and reduce ML dimensionality have emerged asa prime winner in D3R Grand Challenges. This review is devoted to the recentadvances in developing low-dimensional and scalable mathematicalrepresentations of biomolecules in our laboratory. We discuss three classes ofmathematical approaches, including algebraic topology, differential geometry,and graph theory. We elucidate how the physical and biological challenges haveguided the evolution and development of these mathematical apparatuses formassive and diverse biomolecular data. We focus the performance analysis on theprotein-ligand binding predictions in this review although these methods havehad tremendous success in many other applications, such as proteinclassification, virtual screening, and the predictions of solubility, solvationfree energy, toxicity, partition coefficient, protein folding stability changesupon mutation, etc.',\n",
              " 'Multi-view face synthesis from a single image is an ill-posed problem andoften suffers from serious appearance distortion. Producing photo-realistic andidentity preserving multi-view results is still a not well defined synthesisproblem. This paper proposes Load Balanced Generative Adversarial Networks(LB-GAN) to precisely rotate the yaw angle of an input face image to anyspecified angle. LB-GAN decomposes the challenging synthesis problem into twowell constrained subtasks that correspond to a face normalizer and a faceeditor respectively. The normalizer first frontalizes an input image, and thenthe editor rotates the frontalized image to a desired pose guided by a remotecode. In order to generate photo-realistic local details, the normalizer andthe editor are trained in a two-stage manner and regulated by a conditionalself-cycle loss and an attention based L2 loss. Exhaustive experiments oncontrolled and uncontrolled environments demonstrate that the proposed methodnot only improves the visual realism of multi-view synthetic images, but alsopreserves identity information well.',\n",
              " 'Face frontalization refers to the process of synthesizing the frontal view ofa face from a given profile. Due to self-occlusion and appearance distortion inthe wild, it is extremely challenging to recover faithful results and preservetexture details in a high-resolution. This paper proposes a High Fidelity PoseInvariant Model (HF-PIM) to produce photographic and identity-preservingresults. HF-PIM frontalizes the profiles through a novel texture warpingprocedure and leverages a dense correspondence field to bind the 2D and 3Dsurface spaces. We decompose the prerequisite of warping into densecorrespondence field estimation and facial texture map recovering, which areboth well addressed by deep networks. Different from those reconstructionmethods relying on 3D data, we also propose Adversarial Residual DictionaryLearning (ARDL) to supervise facial texture map recovering with only monocularimages. Exhaustive experiments on both controlled and uncontrolled environmentsdemonstrate that the proposed method not only boosts the performance ofpose-invariant face recognition but also dramatically improves high-resolutionfrontalization appearances.',\n",
              " 'The performance of multi-domain image-to-image translation has beensignificantly improved by recent progress in deep generative models. Existingapproaches can use a unified model to achieve translations between all thevisual domains. However, their outcomes are far from satisfying when there arelarge domain variations. In this paper, we reveal that improving the sampleselection strategy is an effective solution. To select informative samples, wedynamically estimate sample importance during the training of GenerativeAdversarial Networks, presenting Informative Sample Mining Network. Wetheoretically analyze the relationship between the sample importance and theprediction of the global optimal discriminator. Then a practical importanceestimation function for general conditions is derived. Furthermore, we proposea novel multi-stage sample training scheme to reduce sample hardness whilepreserving sample informativeness. Extensive experiments on a wide range ofspecific image-to-image translation tasks are conducted, and the resultsdemonstrate our superiority over current state-of-the-art methods.',\n",
              " 'Despite that the performance of image-to-image translation has beensignificantly improved by recent progress in generative models, current methodsstill suffer from severe degradation in training stability and sample qualitywhen applied to the high-resolution situation. In this work, we present a noveltraining framework for GANs, namely biphasic learning, to achieveimage-to-image translation in multiple visual domains at $1024^2$ resolution.Our core idea is to design an adjustable objective function that varies acrosstraining phases. Within the biphasic learning framework, we propose a novelinherited adversarial loss to achieve the enhancement of model capacity andstabilize the training phase transition. Furthermore, we introduce aperceptual-level consistency loss through mutual information estimation andmaximization. To verify the superiority of the proposed method, we apply it toa wide range of face-related synthesis tasks and conduct experiments onmultiple large-scale datasets. Through comprehensive quantitative analyses, wedemonstrate that our method significantly outperforms existing methods.',\n",
              " 'Automatically analyzing dialogue can help understand and guide behavior indomains such as counseling, where interactions are largely mediated byconversation. In this paper, we study modeling behavioral codes used to asses apsychotherapy treatment style called Motivational Interviewing (MI), which iseffective for addressing substance abuse and related problems. Specifically, weaddress the problem of providing real-time guidance to therapists with adialogue observer that (1) categorizes therapist and client MI behavioral codesand, (2) forecasts codes for upcoming utterances to help guide the conversationand potentially alert the therapist. For both tasks, we define neural networkmodels that build upon recent successes in dialogue modeling. Our experimentsdemonstrate that our models can outperform several baselines for both tasks. Wealso report the results of a careful analysis that reveals the impact of thevarious network design tradeoffs for modeling therapy dialogue.',\n",
              " 'Image-to-image (I2I) translation methods based on generative adversarialnetworks (GANs) typically suffer from overfitting when limited training data isavailable. In this work, we propose a data augmentation method (ReMix) totackle this issue. We interpolate training samples at the feature level andpropose a novel content loss based on the perceptual relations among samples.The generator learns to translate the in-between samples rather than memorizingthe training set, and thereby forces the discriminator to generalize. Theproposed approach effectively reduces the ambiguity of generation and renderscontent-preserving results. The ReMix method can be easily incorporated intoexisting GAN models with minor modifications. Experimental results on numeroustasks demonstrate that GAN models equipped with the ReMix method achievesignificant improvements.',\n",
              " 'In complex dynamics, the boundaries of higher dimensional hyperboliccomponents in holomorphic families of polynomials or rational maps aremysterious objects, whose topological and analytic properties are fundamentalproblems.  In this paper, we show that in some typical families of polynomials (i.e.algebraic varieties defined by periodic critical relations), the boundary of acapture hyperbolic component $\\\\mathcal H$ is homeomorphic to the sphere$S^{2\\\\dim_\\\\mathbb{C}(\\\\mathcal{H})-1}$. Furthermore, we establish an unexpectedidentity for the Hausdorff dimension of $\\\\partial \\\\mathcal H$:$$\\\\operatorname{H{.}dim}(\\\\partial\\\\mathcal{H}) = 2\\\\dim_\\\\mathbb{C}(\\\\mathcal{H})-2+\\\\max_{f\\\\in\\\\partial\\\\mathcal{H}}\\\\operatorname{H{.}dim}(\\\\partial A^J(f)),$$ where $A^J(f)$ is the union of thebounded attracting Fatou components of $f$ associated with the free criticalpoints in the Julia set $J(f)$.  In the proof, some new results with independent interests are discovered.',\n",
              " 'In this paper, we show that the solution map of the two-component Novikovsystem is not uniformly continuous on the initial data in Besov spaces $B_{p,r}^{s-1}(\\\\mathbb{R})\\\\times B_{p, r}^s(\\\\mathbb{R})$ with $s>\\\\max\\\\{1+\\\\frac{1}{p},\\\\frac{3}{2}\\\\}$, $1\\\\leq p< \\\\infty$, $1\\\\leq r<\\\\infty$. Our result covers andextends the previous non-uniform continuity in Sobolev spaces$H^{s-1}(\\\\mathbb{R})\\\\times H^s(\\\\mathbb{R})$ for $s>\\\\frac{5}{2}$ (J. Math.Phys., 2017) to Besov spaces.',\n",
              " 'In this paper, we propose the nonlinearity generation method to speed up andstabilize the training of deep convolutional neural networks. The proposedmethod modifies a family of activation functions as nonlinearity generators(NGs). NGs make the activation functions linear symmetric for their inputs tolower model capacity, and automatically introduce nonlinearity to enhance thecapacity of the model during training. The proposed method can be considered anunusual form of regularization: the model parameters are obtained by training arelatively low-capacity model, that is relatively easy to optimize at thebeginning, with only a few iterations, and these parameters are reused for theinitialization of a higher-capacity model. We derive the upper and lower boundsof variance of the weight variation, and show that the initial symmetricstructure of NGs helps stabilize training. We evaluate the proposed method ondifferent frameworks of convolutional neural networks over two objectrecognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental resultsshowed that the proposed method allows us to (1) speed up the convergence oftraining, (2) allow for less careful weight initialization, (3) improve or atleast maintain the performance of the model at negligible extra computationalcost, and (4) easily train a very deep model.',\n",
              " 'The paper demonstrates that exponential complexities with respect to grammarsize and input length have little impact on the performance of threeunification-based parsing algorithms, using a wide-coverage grammar. Theresults imply that the study and optimisation of unification-based parsing mustrely on empirical data until complexity theory can more accurately predict thepractical behaviour of such parsers.',\n",
              " 'We describe an approach to robust domain-independent syntactic parsing ofunrestricted naturally-occurring (English) input. The technique involvesparsing sequences of part-of-speech and punctuation labels using aunification-based grammar coupled with a probabilistic LR parser. We describethe coverage of several corpora using this grammar and report the results of aparsing experiment using probabilities derived from bracketed training data. Wereport the first substantial experiments to assess the contribution ofpunctuation to deriving an accurate syntactic analysis, by parsing identicaltexts both with and without naturally-occurring punctuation marks.',\n",
              " 'We describe a novel technique and implemented system for constructing asubcategorization dictionary from textual corpora. Each dictionary entryencodes the relative frequency of occurrence of a comprehensive set ofsubcategorization classes for English. An initial experiment, on a sample of 14verbs which exhibit multiple complementation patterns, demonstrates that thetechnique achieves accuracy comparable to previous approaches, which are alllimited to a highly restricted set of subcategorization classes. We alsodemonstrate that a subcategorization dictionary built with the system improvesthe accuracy of a parser by an appreciable amount.',\n",
              " 'We address the issue of how to associate frequency information withlexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as arepresentative framework. We consider systematically a number of alternativeprobabilistic frameworks, evaluating their adequacy from both a theoretical andempirical perspective using data from existing large treebanks. We also proposethree orthogonal approaches for backing off probability estimates to cope withthe large number of parameters involved.',\n",
              " 'Research into the automatic acquisition of lexical information from corporais starting to produce large-scale computational lexicons containing data onthe relative frequencies of subcategorisation alternatives for individualverbal predicates. However, the empirical question of whether this type offrequency information can in practice improve the accuracy of a statisticalparser has not yet been answered. In this paper we describe an experiment witha wide-coverage statistical grammar and parser for English andsubcategorisation frequencies acquired from ten million words of text whichshows that this information can significantly improve parse accuracy.',\n",
              " \"We describe an implemented system for robust domain-independent syntacticparsing of English, using a unification-based grammar of part-of-speech andpunctuation labels coupled with a probabilistic LR parser. We presentevaluations of the system's performance along several different dimensions;these enable us to assess the contribution that each individual part is makingto the success of the system as a whole, and thus prioritise the effort to bedevoted to its further enhancement. Currently, the system is able to parsearound 80% of sentences in a substantial corpus of general text containing anumber of distinct genres. On a random sample of 250 such sentences the systemhas a mean crossing bracket rate of 0.71 and recall and precision of 83% and84% respectively when evaluated against manually-disambiguated analyses.\",\n",
              " 'We describe a recently developed corpus annotation scheme for evaluatingparsers that avoids shortcomings of current methods. The scheme encodesgrammatical relations between heads and dependents, and has been used to markup a new public-domain corpus of naturally occurring English text. We show howthe corpus can be used to evaluate the accuracy of a robust parser, and relatethe corpus to extant resources.',\n",
              " 'Optical Tamm States (OTS) are confined optical modes that can occur at theinterface between two highly reflective structures. However, due to the strongreflectance required, their implemen-tation with highly processable andmetal-free flexible materials has proven challenging. Herein, we develop thefirst structure supporting OTS based only on organic polymeric materials,demon-strating a photonic platform based on non-critical, widely available, andeasily processable mate-rials. The structures fabricated present large areasand consist of a narrowband multi-layered polymeric Distributed Bragg Reflector(DBR) followed by a thin film of J-aggregate molecular exci-tonic material thatcan act as a highly reflective surface within a narrowband range. We takead-vantage of the narrowband spectral response of the DBR and of the reflectivemolecular layer to tune the OTS band by varying the periodicity of themultilayer, opening the door for the fabrica-tion of OTS structures based onlightweight integrable excitonic devices with cost-effective proce-dures.',\n",
              " 'The behaviour of a Dirac electron in graphene, under magnetic fields whichare orthogonal to the layer, is studied. The initial problem is reduced to anequivalent one, where two one-dimensional Schr\\\\\"{o}dinger Hamiltonians$H^{\\\\pm}$ are intertwined by a first order differential operator. Specialmagnetic field are initially chosen, in order that $V^{\\\\pm}$ will be shapeinvariant exactly solvable potentials. When looking for more general firstorder operators, intertwining $H^-$ with a non-necessarily shape invariantHamiltonian, new magnetic fields associated also to analytic solutions will begenerated. The iteration of this procedure is as well discussed.',\n",
              " 'The electric power grid is one of the largest and most complexinfrastructures ever built by mankind. Modern civilization depends on it forindustry production, human mobility, and comfortable living. However, manycritical technologies such as the 60 Hz transformers were developed at thebeginning of the 20th century and have changed very little since then.1 Thetraditional unidirectional power from the generation to the customer throughthe transmission-distribution grid has also changed nominally, but it no longermeets the need of the 21st century market energy customers. On one hand, 128mUS residential customers pay $15B/per month for their utility bill, yet theyhave no option to select their energy supplier. In a world of where manytraditional industries are transformed by digital Internet technology (Amazon,Ebay, Uber, Airbnb), the traditional electric energy market is laggingsignificantly behind. A move towards a true digital grid is needed. Such adigital grid requires a tight integration of the physical layer (energy andpower) with digital and cyber information to allow an open and real time marketakin to the world of e-commerce. Another major factor that is pushing for thisradical transformation are the rapidly changing patterns in energy resourcesownership and load flow. Driven by the decreasing cost in distributed solar,energy storage, electric vehicle, on site generation and microgrids, the highpenetration of Distributed Energy Resource (DER) is shifting challengessubstantially towards the edge of grid from the control point of view. Theenvisioned Digital Grid must facilitate the open competition and openinnovation needed to accelerate of the adoption of new DER technologies whilesatisfying challenges in grid stability, data explosion and cyber security.',\n",
              " 'In this paper we present a sufficient condition that guaranteesidentifiability of linear network dynamic systems exhibiting continuous-timeweighted consensus protocols with acyclic structure. Each edge of theunderlying network graph $\\\\mathcal G$ of the system is defined by a constantparameter, referred to as the weight of the edge, while each node is defined bya scalar state whose dynamics evolve as the weighted linear combination of itsdifference with the states of its neighboring nodes. Following the classicaldefinitions of identifiability and indistinguishability, we first derive acondition that ensure the identifiability of the edge weights of $\\\\mathcal G$in terms of the associated transfer function. Using this characterization, wepropose a sensor placement algorithm that guarantees identifiability of theedge weights. We describe our results using several illustrative examples.',\n",
              " 'In this paper we develop a set of algorithms that can detect the identitiesof malicious data-manipulators in distributed optimization loops for estimatingoscillation modes in large power system models. The estimation is posed interms of a consensus problem among multiple local estimators that jointly solvefor the characteristic polynomial of the network model. If any of these localestimates are compromised by a malicious attacker, resulting in an incorrectvalue of the consensus variable, then the entire estimation loop can bedestabilized. We present four iterative algorithms by which this instabilitycan be quickly detected, and the identities of the compromised estimators canbe revealed. The algorithms are solely based on the computed values of theestimates, and do not need any information about the model of the power system.Both large and covert attacks are considered. Results are illustrated usingsimulations of a IEEE 68-bus power system model.',\n",
              " 'In this paper we address the growing concerns of wind power integration fromthe perspective of power system dynamics and stability. We propose a newretrofit control technique where an additional controller is designed at thedoubly-fed induction generator site inside the wind power plant. Thiscontroller cancels the adverse impacts of the power flow from the wind side tothe grid side on the dynamics of the overall power system. The main advantageof this controller is that it can be implemented by feeding back only the windstates and wind bus voltage without depending on any of the other synchronousmachines in the rest of the system. Through simulations of a 4-machine Kundurpower system model we show that the retrofit can efficiently enhance thedamping performance of the system variable despite very high values of windpenetration.',\n",
              " 'Standard H2 optimal control of networked dynamic systems tend to becomeunscalable with network size. Structural constraints can be imposed on thedesign to counteract this problem albeit at the risk of making the solutionnon-convex. In this paper, we present a special class of structural constraintssuch that the H2 design satisfies a quadratic invariance condition, andtherefore can be reformulated as a convex problem. This special class consistsof structured and weighted projections of the input and output spaces. Thechoice of these projections can be optimized to match the closed-loopperformance of the reformulated controller with that of the standard H2controller. The advantage is that unlike the latter, the reformulatedcontroller results in a hierarchical implementation which requiressignificantly lesser number of communication links, while also admitting modeland controller reduction that helps the design to scale computationally. Weillustrate our design with simulations of a 500-node network.',\n",
              " 'The solution space of any set of power flow equations may contain differentnumber of real-valued solutions. The boundaries that separate these regions arereferred to as power flow solution space boundaries. Knowledge of theseboundaries is important as they provide a measure for voltage stability.Traditionally, continuation based methods have been employed to compute theseboundaries on the basis of initial guesses for the solution. However, withrapid growth of renewable energy sources these boundaries will be increasinglyaffected by variable parameters such as penetration levels, locations of therenewable sources, and voltage set-points, making it difficult to generate aninitial guess that can guarantee all feasible solutions for the power flowproblem. In this paper we solve this problem by applying a numerical polynomialhomotopy based continuation method. The proposed method guarantees to find allsolution boundaries within a given parameter space up to a chosen level ofdiscretization, independent of any initial guess. Power system operators canuse this computational tool conveniently to plan the penetration levels ofrenewable sources at different buses. We illustrate the proposed method throughsimulations on 3-bus and 10-bus power system examples with renewablegeneration.',\n",
              " 'This article presents a suite of new control designs for next-generationelectric smart grids. The future grid will consist of thousands ofnon-conventional renewable generation sources such as wind, solar, and energystorage. These new components are collectively referred to as distributedenergy resources (DER). The article presents a comprehensive list of dynamicmodels for DERs, and shows their coupling with the conventional generators andloads. It then presents several innovative control designs that can be used forfacilitating large-scale DER integration. Ideas from decentralized retrofitcontrol and distributed sparsity-promoting optimal control are used fordeveloping these designs, followed by illustrations on an IEEE power systemtest model.',\n",
              " 'In this paper we present an online wide-area oscillation damping control(WAC) design for uncertain models of power systems using ideas fromreinforcement learning. We assume that the exact small-signal model of thepower system at the onset of a contingency is not known to the operator and usethe nominal model and online measurements of the generator states and controlinputs to rapidly converge to a state-feedback controller that minimizes agiven quadratic energy cost. However, unlike conventional linear quadraticregulators (LQR), we intend our controller to be sparse, so its implementationreduces the communication costs. We, therefore, employ the gradient supportpursuit (GraSP) optimization algorithm to impose sparsity constraints on thecontrol gain matrix during learning. The sparse controller is thereafterimplemented using distributed communication. Using the IEEE 39-bus power systemmodel with 1149 unknown parameters, it is demonstrated that the proposedlearning method provides reliable LQR performance while the controller matchedto the nominal model becomes unstable for severely uncertain systems.',\n",
              " \"We formulate a resource-planning game between an attacker and a defender of anetwork control system. We consider the network to be operating in closed-loopwith a linear quadratic regulator (LQR). We construct a general-sum,two-player, mixed strategy game, where the attacker attempts to destroycommunication equipment of some nodes, and thereby render the LQR feedback gainmatrix to be sparse, leading to degradation of closed-loop performance. Thedefender, on the other hand, aims to prevent this loss. Both players tradetheir control performance objectives for the cost of their actions. A MixedStrategy Nash Equilibrium (MSNE) of the game represents the allocation of theplayers' respective resources for attacking or protecting the network nodes. Weanalyze the dependence of a MSNE on the relative budgets of the players as wellas on the important network nodes that must be preserved to achieve a desirableLQR performance. MSNE is computed using nonlinear programming. Results arevalidated using the New England power system model, and it is shown thatreliable defense is feasible unless the cost of attack is very low or muchsmaller than the cost of protection per generator.\",\n",
              " 'This paper presents a mathematical analysis of how wind generation impactsthe coherency property of power systems. Coherency arises from time-scaleseparation in the dynamics of synchronous generators, where generator statesinside a coherent area synchronize over a fast time-scale due to strongercoupling, while the areas themselves synchronize over a slower time-scale dueto weaker coupling. This time-scale separation is reflected in the form of aspectral separation in the weighted Laplacian matrix describing the swingdynamics of the generators. However, when wind farms with doubly-fed inductiongenerators (DFIG) are integrated in the system then this Laplacian matrixchanges based on both the level of wind penetration and the location of thewind farms. The modified Laplacian changes the effective slow eigenspace of thegenerators. Depending on penetration level, this change may result in changingthe identities of the coherent areas. We develop a theoretical framework toquantify this modification, and validate our results with numerical simulationsof the IEEE 68-bus system with one and multiple wind farms. We compare ourmodel based results on clustering with results using measurement-basedprincipal component analysis to substantiate our derivations.',\n",
              " 'Multi-agent networked control systems (NCSs) are often subject to modeluncertainty and are limited by large communication cost, associated withfeedback of data between the system nodes. To provide robustness against modeluncertainty and to reduce the communication cost, this paper investigates themixed $H_2/H_{\\\\infty}$ control problem for NCS under the sparsity constraint.First, proximal alternating linearized minimization (PALM) is employed to solvethe centralized social optimization where the agents have the same optimizationobjective. Next, we investigate a sparsity-constrained noncooperative game,which accommodates different control-performance criteria of different agents,and propose a best-response dynamics algorithm based on PALM that converges toan approximate Generalized Nash Equilibrium (GNE) of this game. A special caseof this game, where the agents have the same $H_2$ objective, produces apartially-distributed social optimization solution. We validate the proposedalgorithms using a network with unstable node dynamics and demonstrate thesuperiority of the proposed PALM-based method to a previously investigatedsparsity-constrained mixed $H_2/H_{\\\\infty}$ controller.',\n",
              " 'We present a set of model-free, reduced-dimensional reinforcement learning(RL) based optimal control designs for linear time-invariant singularlyperturbed (SP) systems. We first present a state-feedback and output-feedbackbased RL control design for a generic SP system with unknown state and inputmatrices. We take advantage of the underlying time-scale separation property ofthe plant to learn a linear quadratic regulator (LQR) for only its slowdynamics, thereby saving a significant amount of learning time compared to theconventional full-dimensional RL controller. We analyze the sub-optimality ofthe design using SP approximation theorems and provide sufficient conditionsfor closed-loop stability. Thereafter, we extend both designs to clusteredmulti-agent consensus networks, where the SP property reflects throughclustering. We develop both centralized and cluster-wise block-decentralized RLcontrollers for such networks, in reduced dimensions. We demonstrate thedetails of the implementation of these controllers using simulations ofrelevant numerical examples and compare them with conventional RL designs toshow the computational benefits of our approach.',\n",
              " \"Multi-agent networked linear dynamic systems have attracted attention ofresearchers in power systems, intelligent transportation, and industrialautomation. The agents might cooperatively optimize a global performanceobjective, resulting in social optimization, or try to satisfy their ownselfish objectives using a noncooperative differential game. However, in thesesolutions, large volumes of data must be sent from system states to possiblydistant control inputs, thus resulting in high cost of the underlyingcommunication network. To enable economically-viable communication, agame-theoretic framework is proposed under the \\\\textit{communication cost}, or\\\\textit{sparsity}, constraint, given by the number of communicatingstate/control input pairs. As this constraint tightens, the system transitionsfrom dense to sparse communication, providing the trade-off between dynamicsystem performance and information exchange. Moreover, using the proposedsparsity-constrained distributed social optimization and noncooperative gamealgorithms, we develop a method to allocate the costs of the communicationinfrastructure fairly and according to the agents' diverse needs for feedbackand cooperation. Numerical results illustrate utilization of the proposedalgorithms to enable and ensure economic fairness of wide-area control amongpower companies.\",\n",
              " \"We study nonlinear power systems consisting of generators, generator buses,and non-generator buses. First, looking at a generator and its bus' variablesjointly, we introduce a synchronization concept for a pair of such jointgenerators and buses. We show that this concept is related to graph symmetry.Next, we extend, in two ways, the synchronization from a pair to a partition ofall generators in the networks and show that they are related to either graphsymmetry or equitable partitions. Finally, we show how an exact reduced modelcan be obtained by aggregating the generators and associated buses in thenetwork when the original system is synchronized with respect to a partition,provided that the initial condition respects the partition. Additionally, theaggregation-based reduced model is again a power system.\",\n",
              " 'Wide-area control (WAC) has been shown to be an effective tool for dampinglow-frequency oscillations in power systems. In the current state of art, WACis challenged by two main factors - namely, scalability of design andcomplexity of implementation. In this paper we present a control design calledcontrol inversion that bypasses both of these challenges using the idea ofclustering. The basic philosophy behind this method is to project the originalpower system model into a lower-dimensional state-space through clustering andaggregation of generator states, and then designing an LQR controller for thelower-dimensional model. This controller is finally projected back to theoriginal coordinates for wide-area implementation. The main problem is,therefore, posed as finding the projection which best matches the closed-loopperformance of the WAC controller with that of a reference LQR controller fordamping low-frequency oscillations. We verify the effectiveness of the proposeddesign using the NPCC 48-machine power system model.',\n",
              " 'This paper presents a new counter-measure to mitigate denial-of-servicecyber-attacks in linear time-invariant (LTI) systems. We first design a sparselinear quadratic regulator (LQR) optimal controller for a given LTI plant andevaluate the priority of the feedback communication links in terms of the lossof closed-loop performance when the corresponding block of the feedback gainmatrix is removed. An attacker may know about this priority ordering, andthereby attack the links with the highest priority. To prevent this, we presenta message rerouting strategy by which the states that are scheduled to betransmitted through the high priority links can be rerouted through lowerpriority ones in case the former get attacked. Since the attacked link is notavailable for service, and the states of the low priority links can no longerbe accommodated either, we run a structured $\\\\mathcal{H}_2$ control algorithmto determine the post-attack optimal feedback gains. We illustrate variousaspects of the proposed algorithms by simulations.',\n",
              " 'In this paper, we propose a fast reinforcement learning (RL) controlalgorithm that enables online control of large-scale networked dynamic systems.RL is an effective way of designing model-free linear quadratic regulator (LQR)controllers for linear time-invariant (LTI) networks with unknown state-spacemodels. However, when the network size is large, conventional RL can result inunacceptably long learning times. The proposed approach is to construct acompressed state vector by projecting the measured state through a projectivematrix. This matrix is constructed from online measurements of the states in away that it captures the dominant controllable subspace of the open-loopnetwork model. Next, a RL-controller is learned using the reduced-dimensionalstate instead of the original state such that the resultant cost is close tothe optimal LQR cost. Numerical benefits as well as the cyber-physicalimplementation benefits of the approach are verified using illustrativeexamples including an example of wide-area control of the IEEE 68-bus benchmarkpower system.',\n",
              " 'We address the problem of sparsity-promoting optimal control ofcyber-physical systems with feedback delays. The delays are categorized intotwo classes - namely, intra-layer delay, and inter-layer delay between thecyber and the physical layers. Our objective is to minimize the H2-norm of theclosed-loop system by designing an optimal combination of these two delaysalong with a sparse state-feedback controller, while respecting a givenbandwidth constraint. We propose a two-loop optimization algorithm for this.The inner loop, based on alternating directions method of multipliers (ADMM),handles the conflicting directions of decreasing H2-norm and increasingsparsity of the controller. The outer loop comprises of semidefinite program(SDP)-based relaxations of non-convex inequalities necessary for stableco-design of the delays with the controller. We illustrate this algorithm usingsimulations that highlight various aspects of how delays and sparsity impactthe stability and $\\\\mc{H}_2$ performance of a LTI system.',\n",
              " 'Eigensystem Realization Algorithm (ERA) is a data-driven approach forsubspace system identification and is widely used in many areas of engineering.However, the computational cost of the ERA is dominated by a step that involvesthe singular value decomposition (SVD) of a large, dense matrix with blockHankel structure. This paper develops computationally efficient algorithms forreducing the computational cost of the SVD step by using randomized subspaceiteration and exploiting the block Hankel structure of the matrix. We provide adetailed analysis of the error in the identified system matrices and thecomputational cost of the proposed algorithms. We demonstrate the accuracy andcomputational benefits of our algorithms on two test problems: the firstinvolves a partial differential equation that models the cooling of steelrails, and the second is an application from power systems engineering.',\n",
              " 'We propose a new reinforcement learning based approach to designinghierarchical linear quadratic regulator (LQR) controllers for heterogeneouslinear multi-agent systems with unknown state-space models and separatedcontrol objectives. The separation arises from grouping the agents intomultiple non-overlapping groups, and defining the control goal as two distinctobjectives. The first objective aims to minimize a group-wiseblock-decentralized LQR function that models group-level mission. The secondobjective, on the other hand, tries to minimize an LQR function between theaverage states (centroids) of the groups. Exploiting this separation, weredefine the weighting matrices of the LQR functions in a way that they allowus to decouple their respective algebraic Riccati equations. Thereafter, wedevelop a reinforcement learning strategy that uses online measurements of theagent states and the average states to learn the respective controllers basedon the approximate Riccati equations. Since the first controller isblock-decentralized and, therefore, can be learned in parallel, while thesecond controller is reduced-dimensional due to averaging, the overall designenjoys a significantly reduced learning time compared to centralizedreinforcement learning.',\n",
              " 'We address the problem of model-free distributed stabilization ofheterogeneous multi-agent systems using reinforcement learning (RL). Twoalgorithms are developed. The first algorithm solves a centralized linearquadratic regulator (LQR) problem without knowing any initial stabilizing gainin advance. The second algorithm builds upon the results of the firstalgorithm, and extends it to distributed stabilization of multi-agent systemswith predefined interaction graphs. Rigorous proofs are provided to show thatthe proposed algorithms achieve guaranteed convergence if specific conditionshold. A simulation example is presented to demonstrate the theoretical results.',\n",
              " 'Existing distributed cooperative multi-agent reinforcement learning (MARL)frameworks usually assume undirected coordination graphs and communicationgraphs while estimating a global reward via consensus algorithms for policyevaluation. Such a framework may induce expensive communication costs andexhibit poor scalability due to requirement of global consensus. In this work,we study MARLs with directed coordination graphs, and propose a distributed RLalgorithm where the local policy evaluations are based on local valuefunctions. The local value function of each agent is obtained by localcommunication with its neighbors through a directed learning-inducedcommunication graph, without using any consensus algorithm. A zeroth-orderoptimization (ZOO) approach based on parameter perturbation is employed toachieve gradient estimation. By comparing with existing ZOO-based RLalgorithms, we show that our proposed distributed RL algorithm guarantees highscalability. A distributed resource allocation example is shown to illustratethe effectiveness of our algorithm.',\n",
              " 'We develop data-driven reinforcement learning (RL) control designs forinput-affine nonlinear systems. We use Carleman linearization to express thestate-space representation of the nonlinear dynamical model in the Carlemanspace, and develop a real-time algorithm that can learn nonlinearstate-feedback controllers using state and input measurements in theinfinite-dimensional Carleman space. Thereafter, we study the practicality ofhaving a finite-order truncation of the control signal, followed by itsclosed-loop stability analysis. Finally, we develop two additional designs thatcan learn structured as well as sparse representations of the RL-basednonlinear controller, and provide theoretical conditions for ensuring theirclosed-loop stability. We present numerical examples to show how our proposedmethod generates closed-loop responses that are close to the optimalperformance of the nonlinear plant. We also compare our designs to otherdata-driven nonlinear RL control methods such as those based on neuralnetworks, and illustrate their relative advantages and drawbacks.',\n",
              " 'We study equilibrium-independent passivity properties of nonlinear dynamicalmodels of electric power systems. The model of our interest comprises of afeedback interconnection of two subsystems, one being a first-order linearordinary differential equation and the other being a set of nonlineardifferential algebraic equations (DAE). We prove the following three facts byanalyzing the nonlinear DAE subsystem. First, a lossless transmission networkis necessary for guaranteeing equilibrium-independent passivity of the DAE.Second, the convexity of a strain energy function characterizes the largest setof equilibria over which this DAE subsystem is equilibrium-independent passive.Finally, we prove that the strain energy function of a power system withtwo-axis generator models is convex if and only if its flux linkage dynamicsare stable, and the strain energy function of a classical generator modelderived by a singular perturbation approximation of the flux linkage dynamicsis convex. These novel findings are derived by elaborating on linearization andKron reduction properties of the power system model. Numerical simulation ofthe IEEE 9-bus power system model demonstrates the practical implications ofthe various mathematical results.',\n",
              " 'In this paper we investigate how the equilibrium characteristics ofconventional power systems may change with an increase in wind penetration. Wefirst derive a differential-algebraic model of a power system networkconsisting of synchronous generators, loads and a wind power plant modeled by awind turbine and a doubly-fed induction generator (DFIG). The models of thesethree components are coupled via nonlinear power flow equations. In contrast tothe traditional approach for solving the power flows via iterative methods thatoften lead to only local solutions, we apply a recently developedparameter-homotopy based numerical continuation algorithm to compute allpossible solutions. The method solves the power flow equations over multiplevalues of the wind penetration level with far less computational effort insteadof solving them at each value individually. We observe that depending on thepenetration limit and the setpoint value for the magnitude of the wind busvoltage, the system may exhibit several undesired or even unstable equilibria.We illustrate these results through a detailed simulation of a 5-machine powersystem model with wind injection, and highlight how the solutions may behelpful for small-signal stability assessment.',\n",
              " 'We propose how to quantify high-frequency market sentiment usinghigh-frequency news from NASDAQ news platform and support vector machineclassifiers. News arrive at markets randomly and the resulting news sentimentbehaves like a stochastic process. To characterize the joint evolution ofsentiment, price, and volatility, we introduce a unified continuous-timesentiment-driven stochastic volatility model. We provide closed-form formulasfor moments of the volatility and news sentiment processes and study the newsimpact. Further, we implement a simulation-based method to calibrate theparameters. Empirically, we document that news sentiment raises the thresholdof volatility reversion, sustaining high market volatility.',\n",
              " 'In order to price contingent claims one needs to first understand thedynamics of these indices. Here we provide a first econometric analysis of theCRIX family within a time-series framework. The key steps of our analysisinclude model selection, estimation and testing. Linear dependence is removedby an ARIMA model, the diagnostic checking resulted in an ARIMA(2,0,2) modelfor the available sample period from Aug 1st, 2014 to April 6th, 2016. Themodel residuals showed the well known phenomenon of volatility clustering.Therefore a further refinement lead us to an ARIMA(2,0,2)-t-GARCH(1,1) process.This specification conveniently takes care of fat-tail properties that aretypical for financial markets. The multivariate GARCH models are implemented onthe CRIX index family to explore the interaction.',\n",
              " 'A standard quantitative method to access credit risk employs a factor modelbased on joint multivariate normal distribution properties. By extending aone-factor Gaussian copula model to make a more accurate default forecast, thispaper proposes to incorporate a state-dependent recovery rate into theconditional factor loading, and model them by sharing a unique common factor.The common factor governs the default rate and recovery rate simultaneously andcreates their association implicitly. In accordance with Basel III, this papershows that the tendency of default is more governed by systematic risk ratherthan idiosyncratic risk during a hectic period. Among the models considered,the one with random factor loading and a state-dependent recovery rate turnsout to be the most superior on the default prediction.',\n",
              " 'We investigate the relationship between underlying blockchain mechanism ofcryptocurrencies and its distributional characteristics. In addition to price,we emphasise on using actual block size and block time as the operationalfeatures of cryptos. We use distributional characteristics such as fourierpower spectrum, moments, quantiles, global we optimums, as well as the measuresfor long term dependencies, risk and noise to summarise the information fromcrypto time series. With the hypothesis that the blockchain structure explainsthe distributional characteristics of cryptos, we use characteristic basedspectral clustering to cluster the selected cryptos into five groups. Wescrutinise these clusters and find that indeed, the clusters of cryptos sharesimilar mechanism such as origin of fork, difficulty adjustment frequency, andthe nature of block size. This paper provides crypto creators and users with abetter understanding toward the connection between the blockchain protocoldesign and distributional characteristics of cryptos.',\n",
              " 'The integration of social media characteristics into an econometric frameworkrequires modeling a high dimensional dynamic network with dimensions ofparameter typically much larger than the number of observations. To cope withthis problem, we introduce SONIC, a new high-dimensional network model thatassumes that (1) only few influencers drive the network dynamics; (2) thecommunity structure of the network is characterized by homogeneity of responseto specific influencers, implying their underlying similarity. An estimationprocedure is proposed based on a greedy algorithm and LASSO regularization.Through theoretical study and simulations, we show that the matrix parametercan be estimated even when sample size is smaller than the size of the network.Using a novel dataset retrieved from one of leading social media platforms -StockTwits and quantifying their opinions via natural language processing, wemodel the opinions network dynamics among a select group of users and furtherdetect the latent communities. With a sparsity regularization, we can identifyimportant nodes in the network.',\n",
              " 'Preventing data exfiltration from computer systems typically depends onperimeter defences, but these are becoming increasingly fragile. Instead wesuggest an approach in which each at-risk document is supplemented by many fakeversions of itself. An attacker must either exfiltrate all of them; or try todiscover which is the real one while operating within the penetrated system,and both are difficult. Creating and maintaining many fakes is relativelyinexpensive, so the advantage that typically accrues to an attacker now lieswith the defender. We show that algorithmically generated fake documents arereasonably difficult to detect using algorithmic analytics.',\n",
              " \"Landsat imagery is an unparalleled freely available data source that allowsreconstructing horizontal and vertical urban form. This paper addresses thechallenge of using Landsat data, particularly its 30m spatial resolution, formonitoring three-dimensional urban densification. We compare temporal andspatial transferability of an adapted DeepLab model with a simple fullyconvolutional network (FCN) and a texture-based random forest (RF) model to mapurban density in the two morphological dimensions: horizontal (compact, open,sparse) and vertical (high rise, low rise). We test whether a model trained onthe 2014 data can be applied to 2006 and 1995 for Denmark, and examine whetherwe could use the model trained on the Danish data to accurately map otherEuropean cities. Our results show that an implementation of deep networks andthe inclusion of multi-scale contextual information greatly improve theclassification and the model's ability to generalize across space and time.DeepLab provides more accurate horizontal and vertical classifications than FCNwhen sufficient training data is available. By using DeepLab, the F1 score canbe increased by 4 and 10 percentage points for detecting vertical urban growthcompared to FCN and RF for Denmark. For mapping the other European cities withtraining data from Denmark, DeepLab also shows an advantage of 6 percentagepoints over RF for both the dimensions. The resulting maps across the years1985 to 2018 reveal different patterns of urban growth between Copenhagen andAarhus, the two largest cities in Denmark, illustrating that those cities haveused various planning policies in addressing population growth and housingsupply challenges. In summary, we propose a transferable deep learning approachfor automated, long-term mapping of urban form from Landsat images.\",\n",
              " \"Monitoring long-term landslide activity is important for risk assessment andland management. Despite the widespread use of open-access 30m Landsat imagery,their utility for landslide detection is often limited when separatinglandslides from other anthropogenic disturbances. Here, we produce landslidemaps retrospectively from 1998 to 2017 for landslide-prone and highly populatedTaiwan (35,874 km2). To improve classification accuracy of landslides, weintegrate nighttime light imagery from the Defense Meteorological SatelliteProgram (DMSP) and the Visible Infrared Imaging Radiometer Suite (VIIRS), withmulti-seasonal daytime optical Landsat time-series, and digital elevation datafrom the Advanced Spaceborne Thermal Emission and Reflection Radiometer(ASTER). We employed a non-parametric machine-learning classifier, randomforest, to classify the satellite imagery. The classifier was trained with datafrom three years (2005, 2010, and 2015), and was validated with an independentreference sample from twelve years. Our results demonstrated that combiningnighttime light data and multi-seasonal imagery significantly improved theclassification (p<0.001), compared to conventional methods based onsingle-season optical imagery. The results confirmed that the developedclassification model enabled mapping of landslides across Taiwan over a longperiod with annual overall accuracy varying between 96% and 97%, user's andproducer's accuracies between 73% and 86%. Spatiotemporal analysis of thelandslide inventories from 1998 to 2017 revealed different temporal patterns oflandslide activities, showing those areas where landslides were persistent andother areas where landslides tended to reoccur after vegetation regrowth. Insum, we provide a robust method to detect long-term landslide activities basedon freely available satellite imagery, which can be applied elsewhere.\",\n",
              " 'In this two-part paper, we consider SDL constructions of optical queues witha limited number of recirculations through the optical switches and the fiberdelay lines. We show that the constructions of certain types of optical queues,including linear compressors, linear decompressors, and 2-to-1 FIFOmultiplexers, under a simple packet routing scheme and under the constraint ofa limited number of recirculations can be transformed into equivalent integerrepresentation problems under a corresponding constraint. Given $M$ and $k$,the problem of finding an \\\\emph{optimal} construction, in the sense ofmaximizing the maximum delay (resp., buffer size), among our constructions oflinear compressors/decompressors (resp., 2-to-1 FIFO multiplexers) isequivalent to the problem of finding an optimal sequence ${\\\\dbf^*}_1^M$ in$\\\\Acal_M$ (resp., $\\\\Bcal_M$) such that $B({\\\\dbf^*}_1^M;k)=\\\\max_{\\\\dbf_1^M\\\\in\\\\Acal_M}B(\\\\dbf_1^M;k)$ (resp., $B({\\\\dbf^*}_1^M;k)=\\\\max_{\\\\dbf_1^M\\\\in\\\\Bcal_M}B(\\\\dbf_1^M;k)$), where $\\\\Acal_M$ (resp., $\\\\Bcal_M$) is the set of allsequences of fiber delays allowed in our constructions of linearcompressors/decompressors (resp., 2-to-1 FIFO multiplexers). In Part I, wepropose a class of \\\\emph{greedy} constructions of linearcompressors/decompressors and 2-to-1 FIFO multiplexers by specifying a class$\\\\Gcal_{M,k}$ of sequences such that $\\\\Gcal_{M,k}\\\\subseteq \\\\Bcal_M\\\\subseteq\\\\Acal_M$ and each sequence in $\\\\Gcal_{M,k}$ is obtained recursively in a greedymanner. We then show that every optimal construction must be a greedyconstruction. In Part II, we further show that there are at most two optimalconstructions and give a simple algorithm to obtain the optimalconstruction(s).',\n",
              " 'Long-range dependencies modeling, widely used in capturing spatiotemporalcorrelation, has shown to be effective in CNN dominated computer vision tasks.Yet neither stacks of convolutional operations to enlarge receptive fields norrecent nonlocal modules is computationally efficient. In this paper, we presenta generic family of lightweight global descriptors for modeling theinteractions between positions across different dimensions (e.g., channels,frames). This descriptor enables subsequent convolutions to access theinformative global features with negligible computational complexity andparameters. Benchmark experiments show that the proposed method can completestate-of-the-art long-range mechanisms with a significant reduction in extracomputing cost. Code available athttps://github.com/HolmesShuan/Compact-Global-Descriptor.',\n",
              " 'Knowledge distillation(KD) is a widely-used technique to train compact modelsin object detection. However, there is still a lack of study on how to distillbetween heterogeneous detectors. In this paper, we empirically find that betterFPN features from a heterogeneous teacher detector can help the studentalthough their detection heads and label assignments are different. However,directly aligning the feature maps to distill detectors suffers from twoproblems. First, the difference in feature magnitude between the teacher andthe student could enforce overly strict constraints on the student. Second, theFPN stages and channels with large feature magnitude from the teacher modelcould dominate the gradient of distillation loss, which will overwhelm theeffects of other features in KD and introduce much noise. To address the aboveissues, we propose to imitate features with Pearson Correlation Coefficient tofocus on the relational information from the teacher and relax constraints onthe magnitude of the features. Our method consistently outperforms the existingdetection KD methods and works for both homogeneous and heterogeneousstudent-teacher pairs. Furthermore, it converges faster. With a powerfulMaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOSachieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\\\\% and 4.8\\\\% higher thanthe baseline, respectively.',\n",
              " 'Using recent experimental data and various theoretical calculations on formfactors, we reanalyze the effective parameters $a_1$ and $a_2$ and their ratio.',\n",
              " 'This article is based on studies of the existing literature, focusing on thestates-of-the-arts on virtual reality (VR) and its potential uses in learning.Different platforms have been used to improve the learning effects of VR thatoffers exciting opportunities in various fields. As more and more students wantin a distance, part-time, or want to continue their education, VR has attractedconsiderable attention in learning, training, and traditional education. VRbased learning enables operators to bring together all disciplinary resourcesin a common playground. The VR base multimedia platform has successfullydemonstrated great potential of education and training. In this paper, we willdiscuss existing systems and their uses and address the technical challengesand future directions.',\n",
              " 'Unidirectional spin Hall magnetoresistance (USMR) has been widely reported inthe heavy metal / ferromagnet (HM/FM) bilayer systems. We observe the USMR inthe Pt/{\\\\alpha}-Fe2O3 bilayers where the {\\\\alpha}-Fe2O3 is an antiferromagnetic(AFM) insulator. Systematic field and temperature dependent measurementsconfirm the magnonic origin of the USMR. The appearance of AFM-USMR is drivenby the imbalance of creation and annihilation of AFM magnons by spin orbittorque due to thermal random field. However, unlike its ferromagneticcounterpart, theoretical modeling reveals that the USMR in Pt/{\\\\alpha}-Fe2O3 isdetermined by the antiferromagtic magnon number, and with a non-monotonic fielddependence. Our findings extend the generality of the USMR which pave the waysfor the highly sensitive detection of AF spin state.',\n",
              " 'In this paper we study the problem of reducing the evaluation costs ofqueries on finite databases in presence of integrity constraints, by designingand materializing views. Given a database schema, a set of queries defined onthe schema, a set of integrity constraints, and a storage limit, to find asolution to this problem means to find a set of views that satisfies thestorage limit, provides equivalent rewritings of the queries under theconstraints (this requirement is weaker than equivalence in the absence ofconstraints), and reduces the total costs of evaluating the queries. Thisproblem, database reformulation, is important for many applications, includingdata warehousing and query optimization. We give complexity results andalgorithms for database reformulation in presence of constraints, forconjunctive queries, views, and rewritings and for several types ofconstraints, including functional and inclusion dependencies. To obtain bettercomplexity results, we introduce an unchase technique, which reduces theproblem of query equivalence under constraints to equivalence in the absence ofconstraints without increasing query size.',\n",
              " 'We consider the problems of finding and determining certain query answers andof determining containment between queries; each problem is formulated inpresence of materialized views and dependencies under the closed-worldassumption. We show a tight relationship between the problems in this setting.Further, we introduce algorithms for solving each problem for those inputswhere all the queries and views are conjunctive, and the dependencies areembedded weakly acyclic. We also determine the complexity of each problem underthe security-relevant complexity measure introduced by Zhang and Mendelzon in2005. The problems studied in this paper are fundamental in ensuring correctspecification of database access-control policies, in particular in case offine-grained access control. Our approaches can also be applied in the areas ofinference control, secure data publishing, and database auditing.',\n",
              " 'In visual exploration and analysis of data, determining how to select andtransform the data for visualization is a challenge for data-unfamiliar orinexperienced users. Our main hypothesis is that for many data sets and commonanalysis tasks, there are relatively few \"data slices\" that result in effectivevisualizations. By focusing human users on appropriate and suitably transformedparts of the underlying data sets, these data slices can help the users carrytheir task to correct completion.  To verify this hypothesis, we develop a framework that permits us to captureexemplary data slices for a user task, and to explore and parsevisual-exploration sequences into a format that makes them distinct and easy tocompare. We develop a recommendation system, DataSlicer, that matches a\"currently viewed\" data slice with the most promising \"next effective\" dataslices for the given exploration task. We report the results of controlledexperiments with an implementation of the DataSlicer system, using four commonanalytical task types. The experiments demonstrate statistically significantimprovements in accuracy and exploration speed versus users without access toour system.',\n",
              " 'We consider the problem of finding equivalent minimal-size reformulations ofSQL queries in presence of embedded dependencies [1]. Our focus is onselect-project-join (SPJ) queries with equality comparisons, also known as safeconjunctive (CQ) queries, possibly with grouping and aggregation. For SPJqueries, the semantics of the SQL standard treat query answers as multisets(a.k.a. bags), whereas the stored relations may be treated either as sets,which is called bag-set semantics for query evaluation, or as bags, which iscalled bag semantics. (Under set semantics, both query answers and storedrelations are treated as sets.)  In the context of the above Query-Reformulation Problem, we develop acomprehensive framework for equivalence of CQ queries under bag and bag-setsemantics in presence of embedded dependencies, and make a number of conceptualand technical contributions. Specifically, we develop equivalence tests for CQqueries in presence of arbitrary sets of embedded dependencies under bag andbag-set semantics, under the condition that chase [9] under set semantics(set-chase) on the inputs terminates. We also present equivalence tests foraggregate CQ queries in presence of embedded dependencies. We use ourequivalence tests to develop sound and complete (whenever set-chase on theinputs terminates) algorithms for solving instances of the Query-ReformulationProblem with CQ queries under each of bag and bag-set semantics, as well as forinstances of the problem with aggregate queries.',\n",
              " 'In this paper, we focus on the problem of determining whether two conjunctive(\"CQ\") queries posed on relational data are combined-semantics equivalent [9].We continue the tradition of [2,5,9] of studying this problem using the tool ofcontainment between queries. We introduce a syntactic necessary and sufficientcondition for equivalence of queries belonging to a large natural language of\"explicit-wave\" combined-semantics CQ queries; this language encompasses (butis not limited to) all set, bag, and bag-set queries, and appears to cover allcombined-semantics CQ queries that are expressible in SQL. Our result solves inthe positive the decidability problem of determining combined-semanticsequivalence for pairs of explicit-wave CQ queries. That is, for an arbitrarypair of combined-semantics CQ queries, it is decidable (i) to determine whethereach of the queries is explicit wave, and (ii) to determine, in case bothqueries are explicit wave, whether or not they are combined-semanticsequivalent, by using our syntactic criterion. (The problem of determiningequivalence for general combined-semantics CQ queries remains open. Even so,our syntactic sufficient containment condition could still be used to determinethat two general CQ queries are combined-semantics equivalent.) Our equivalencetest, as well as our general sufficient condition for containment ofcombined-semantics CQ queries, reduce correctly to the special cases reportedin [2,5] for set, bag, and bag-set semantics. Our containment and equivalenceconditions also properly generalize the results of [9], provided that thelatter are restricted to the language of (combined-semantics) CQ queries.',\n",
              " 'Assessing and improving the quality of data in data-intensive systems arefundamental challenges that have given rise to numerous applications targetingtransformation and cleaning of data. However, while schema design, datacleaning, and data migration are nowadays reasonably well understood inisolation, not much attention has been given to the interplay between the toolsthat address issues in these areas. Our focus is on the problem of determiningwhether there exist sequences of data-transforming procedures that, whenapplied to the (untransformed) input data, would yield data satisfying theconditions required for performing the task in question. Our goal is to developa framework that would address this problem, starting with the relationalsetting.  In this paper we abstract data-processing tools as black-box procedures. Thisabstraction describes procedures by a specification of which parts of thedatabase might be modified by the procedure, as well as by the constraints thatspecify the required states of the database before and after applying theprocedure. We then proceed to study fundamental algorithmic questions arisingin this context, such as understanding when one can guarantee that sequences ofprocedures apply to original or transformed data, when they succeed atimproving the data, and when knowledge bases can represent the outcomes ofprocedures. Finally, we turn to the problem of determining whether theapplication of a sequence of procedures to a database results in thesatisfaction of properties specified by either queries or constraints. We showthat this problem is decidable for some broad and realistic classes ofprocedures and properties, even when procedures are allowed to alter the schemaof instances.',\n",
              " 'Assessing and improving the quality of data are fundamental challenges fordata-intensive systems that have given rise to applications targetingtransformation and cleaning of data. However, while schema design, datacleaning, and data migration are now reasonably well understood in isolation,not much attention has been given to the interplay between the tools addressingissues in these areas. We focus on the problem of determining whether theavailable data-processing procedures can be used together to bring about thedesired quality of the given data. For instance, consider an organizationintroducing new data-analysis tasks. Depending on the tasks, it may be apriority to determine whether the data can be processed and transformed usingthe available data-processing tools to satisfy certain properties or qualityassurances needed for the success of the task. Here, while the organization maycontrol some of its tools, some other tools may be external or proprietary,with only basic information available on how they process data. The problem isthen, how to decide which tools to apply, and in which order, to make the dataready for the new tasks?  Toward addressing this problem, we develop a new framework that abstractsdata-processing tools as black-box procedures with only some of the propertiesexposed, such as the applicability requirements, the parts of the data that theprocedure modifies, and the conditions that the data satisfy once the procedurehas been applied. We show how common tasks such as data cleaning and datamigration are encapsulated into our framework and, as a proof of concept, westudy basic properties of the framework for the case of procedures described bystandard relational constraints. While reasoning in this framework may becomputationally infeasible in general, we show that there exist well-behavedspecial cases with potential practical applications.',\n",
              " 'WaveCluster is an important family of grid-based clustering algorithms thatare capable of finding clusters of arbitrary shapes. In this paper, weinvestigate techniques to perform WaveCluster while ensuring differentialprivacy. Our goal is to develop a general technique for achieving differentialprivacy on WaveCluster that accommodates different wavelet transforms. We showthat straightforward techniques based on synthetic data generation andintroduction of random noise when quantizing the data, though generallypreserving the distribution of data, often introduce too much noise to preserveuseful clusters. We then propose two optimized techniques, PrivTHR andPrivTHREM, which can significantly reduce data distortion during two key stepsof WaveCluster: the quantization step and the significant grid identificationstep. We conduct extensive experiments based on four datasets that areparticularly interesting in the context of clustering, and show that PrivTHRand PrivTHREM achieve high utility when privacy budgets are properly allocated.',\n",
              " 'Objective: We aim to learn potential novel cures for diseases fromunstructured text sources. More specifically, we seek to extract drug-diseasepairs of potential cures to diseases by a simple reasoning over the structureof spoken text.  Materials and Methods: We use Google Cloud to transcribe podcast episodes ofan NPR radio show. We then build a pipeline for systematically pre-processingthe text to ensure quality input to the core classification model, which feedsto a series of post-processing steps for obtaining filtered results. Ourclassification model itself uses a language model pre-trained on PubMed text.The modular nature of our pipeline allows for ease of future developments inthis area by substituting higher quality components at each stage of thepipeline. As a validation measure, we use ROBOKOP, an engine over a medicalknowledge graph with only validated pathways, as a ground truth source forchecking the existence of the proposed pairs. For the proposed pairs not foundin ROBOKOP, we provide further verification using Chemotext.  Results: We found 30.4% of our proposed pairs in the ROBOKOP database. Forexample, our model successfully identified that Omeprazole can help treatheartburn.We discuss the significance of this result, showing some examples ofthe proposed pairs.  Discussion and Conclusion: The agreement of our results with the existingknowledge source indicates a step in the right direction. Given theplug-and-play nature of our framework, it is easy to add, remove, or modifyparts to improve the model as necessary. We discuss the results showing someexamples, and note that this is a potentially new line of research that hasfurther scope to be explored. Although our approach was originally oriented onradio podcast transcripts, it is input-agnostic and could be applied to anysource of textual data and to any problem of interest.',\n",
              " 'We report on the design and experimental characterization of asurface-electrode multipole ion trap. Individual microscopic sugar particlesare confined in the trap. The trajectories of driven particle motion arecompared with a theoretical model, both to verify qualitative predictions ofthe model, and to measure the charge-to-mass ratio of the confined particle.The generation of harmonics of the driving frequency is observed as a keysignature of the nonlinear nature of the trap. We remark on possibleapplications of our traps, including to mass spectrometry.',\n",
              " \"Ion transport is an essential operation in some models of quantum informationprocessing, where fast ion shuttling with minimal motional excitation isnecessary for efficient, high-fidelity quantum logic. While fast and cold ionshuttling has been demonstrated, the dynamics and specific trajectory of an ionduring diabatic transport have not been studied in detail. Here we describe aposition-dependent optical deshelving technique useful for sampling an ion'sposition throughout its trajectory, and we demonstrate the technique on fastlinear transport of a $^{40}\\\\text{Ca}^+$ ion in a surface-electrode ion trap.At high speed, the trap's electrode filters strongly distort the transportpotential waveform. With this technique, we observe deviations from theintended constant-velocity (100 m/s) transport: we measure an average speed of83(2) m/s and a peak speed of 251(6) m/s over a distance of 120 $\\\\mu$m\",\n",
              " 'Single-cell RNA-Sequencing (scRNA-Seq) has undergone major technologicaladvances in recent years, enabling the conception of various organism-levelcell atlassing projects. With increasing numbers of datasets being deposited inpublic archives, there is a need to address the challenges of enabling thereproducibility of such data sets. Here, we describe guidelines for a minimumset of metadata to sufficiently describe scRNA-Seq experiments, ensuringreproducibility of data analyses.',\n",
              " 'We review the application of the Wigner-Weisskopf model to the two-channeldecay problem for the neutral $K$ meson system in the resolvent formalism. Theresidues in the pole approximation are not orthogonal, leading to additionalinterference terms in the $K_S-K_L 2\\\\pi$ channel. We show that these terms leadto non-trivial changes in the exit beam in comparison to the result calculatedwith the assumptions of Lee, Oehme and Yang, and Wu and Yang, corresponding tosemigroup evolution for which the pole residues are orthogonal, and henceappear to rule out the applicability of the Wigner-Weisskopf model for thecomputation of regeneration in the neutral $K$ meson system.',\n",
              " 'We review the application of the Wigner-Weisskopf model for the neutral Kmeson system in the resolvent formalism. The Wigner-Weisskopf model is notequivalent to the Lee-Oehme-Yang-Wu formulation (which provides an accuraterepresentation of the data).  The residues in the pole approximation in the Wigner-Weisskopf model are notorthogonal, leading to additional interference terms in the $K_S-K_L 2\\\\pi$channel. We show that these terms would be detectable experimentally in thedecay pattern of the beam emitted from a regenerator if the Wigner-Weisskopftheory were correct. The consistency of the data with the Lee-Oehme-Yang-Wuformulation appears to rule out the applicability of the Wigner-Weisskopftheory to the problem of neutral K meson decay.',\n",
              " 'This article describes a parallax experiment performed by undergraduatephysics students at Queensland University of Technology. The experiment isanalogous to the parallax method used in astronomy to measure distances to thelocal stars. The result of one of these experiments is presented in this paper.A target was photographed using a digital camera at five distances between 3and 8 metres from two vantage points spaced 0.6 m apart. The parallax distanceswere compared with the actual distance measured using a tape measure and theaverage error was 0.5 +/- 0.9%.',\n",
              " 'This paper describes a simple activity for plotting and characterizing thelight curve from an exoplanet transit event by way of differential photometryanalysis. Using free digital imaging software, participants analyse a series oftelescope images with the goal of calculating various exoplanet parameters,including its size, orbital radius and habitability. The activity has beendesigned for a high school or undergraduate university level and introducesfundamental concepts in astrophysics and an understanding of the basis forexoplanetary science, the transit method and digital photometry.',\n",
              " 'We investigate the ISM properties of 13 star-forming galaxies within the z~2COSMOS cluster. We show that the cluster members have [NII]/Ha and [OIII]/Hbemission-line ratios similar to z~2 field galaxies, yet systematicallydifferent emission-line ratios (by ~0.17 dex) from the majority of localstar-forming galaxies. We find no statistically significant difference in the[NII]/Ha and [OIII]/Hb line ratios or ISM pressures among the z~2 clustergalaxies and field galaxies at the same redshift. We show that our clustergalaxies have significantly larger ionization parameters (by up to an order ofmagnitude) than local star-forming galaxies. We hypothesize that these highionization parameters may be associated with large specific star formationrates (i.e. a large star formation rate per unit stellar mass). If thishypothesis is correct, then this relationship would have important implicationsfor the geometry and/or the mass of stars contained within individual starclusters as a function of redshift.',\n",
              " 'We investigate the environmental dependence of the mass-metallicity relationat z=2 with MOSFIRE/Keck as part of the ZFIRE survey. Here, we present thechemical abundance of a Virgo-like progenitor at z=2.095 that has anestablished red sequence. We identified 43 cluster ($<z>=2.095\\\\pm0.004$) and 74field galaxies ($<z>=2.195\\\\pm0.083$) for which we can measure metallicities.For the first time, we show that there is no discernible difference between themass-metallicity relation of field and cluster galaxies to within 0.02dex. Bothour field and cluster galaxy mass-metallicity relations are consistent withrecent field galaxy studies at z~2. We present hydrodynamical simulations forwhich we derive mass-metallicity relations for field and cluster galaxies. Wefind at most a 0.1dex offset towards more metal-rich simulated clustergalaxies. Our results from both simulations and observations are suggestivethat environmental effects, if present, are small and are secondary to theongoing inflow and outflow processes that are governed by galaxy halo mass.',\n",
              " 'We perform a kinematic analysis of galaxies at $z\\\\sim2$ in the COSMOS legacyfield using near-infrared (NIR) spectroscopy from Keck/MOSFIRE as part of theZFIRE survey. Our sample consists of 75 Ks-band selected star-forming galaxiesfrom the ZFOURGE survey with stellar masses ranging fromlog(M$_{\\\\star}$/M$_{\\\\odot}$)$=9.0-11.0$, 28 of which are members of a knownoverdensity at $z=2.095$. We measure H$\\\\alpha$ emission-line integratedvelocity dispersions ($\\\\sigma_{\\\\rm int}$) from 50$-$230 km s$^{-1}$, consistentwith other emission-line studies of $z\\\\sim2$ field galaxies. From these data weestimate virial, stellar, and gas masses and derive correlations between theseproperties for cluster and field galaxies at $z\\\\sim2$. We find evidence thatbaryons dominate within the central effective radius. However, we find nostatistically significant differences between the cluster and the field, andconclude that the kinematics of star-forming galaxies at $z\\\\sim2$ are notsignificantly different between the cluster and field environments.',\n",
              " 'We investigate the star formation rate (SFR) dependence on the stellar massand gas-phase metallicity relation at z=2 with MOSFIRE/Keck as part of theZFIRE survey. We have identified 117 galaxies (1.98 < z < 2.56), with$8.9\\\\leq$log(M/M$_{\\\\odot}$)$\\\\leq11.0$, for which we can measure gas-phasemetallicities. For the first time, we show discernible difference between themass-metallicity relation, using individual galaxies, when deviding the sampleby low ($<10$~M$_{\\\\odot}$yr$^{-1}$) and high ($>10$~M$_{\\\\odot}$yr$^{-1}$) SFRs.At fixed mass, low star-forming galaxies tend to have higher metallicity thanhigh star-forming galaxies. Using a few basic assumptions, we further show thatthe gas masses and metallicities required to produce the fundamentalmass--metallicity relation, and its intrinsic scatter, are consistent withcold-mode accretion predictions obtained from the OWLS hydrodynamicalsimulations. Our results from both simulations and observations are suggestivethat cold-mode accretion is responsible for the fundamental mass-metallicityrelation at $z=2$ and demonstrates the direct relationship between cosmologicalaccretion and the fundamental properties of galaxies.',\n",
              " 'We investigate the dependance of galaxy sizes and star-formation rates (SFRs)on environment using a mass-limited sample of quiescent and star-forminggalaxies with M>10^9.5 at z=0.92 selected from the NMBS survey. Using the GEEC2spectroscopic cluster catalog and the accurate photometric redshifts from NMBS,we select quiescent and star-forming cluster (sigma=490 km/s) galaxies withintwo virial radius, Rvir, intervals of 0.5<Rvir<2 and Rvir<0.5. Galaxiesresiding outside of 2 Rvir of both the cluster centres and additional candidateover-densities are defined as our field sample. Galaxy structural parametersare measured from the COSMOS legacy HST/ACS F814W image. The sizes and Sersicindices of quiescent field and cluster galaxies have the same distributionregardless of Rvir. However, cluster star-forming galaxies within 0.5 Rvir havelower mass-normalised average sizes, by 16${\\\\pm}7\\\\%$, and a higher fraction ofSersic indices with n>1, than field star-forming galaxies. The average SFRs ofstar-forming cluster galaxies show a trend of decreasing SFR with clustocentricradius. The mass-normalised average SFR of cluster star-forming galaxies is afactor of 2-2.5 (7-9 sigma) lower than that of star-forming galaxies in thefield. While we find no significant dependence on environment for quiescentgalaxies, the properties of star-forming galaxies are affected, which could bethe result of environment acting on their gas content.',\n",
              " 'There is ongoing debate regarding the extent that environment affects galaxysize growth beyond z>1. To investigate the differences in star-forming andquiescent galaxy properties as a function of environment at z=2.1, we create amass-complete sample of 59 cluster galaxies Spitler et al. (2012) and 478 fieldgalaxies with log(M)>9 using photometric redshifts from the ZFOURGE survey. Wecompare the mass-size relation of field and cluster galaxies using measuredgalaxy semi-major axis half-light radii ($r_{1/2,maj}$) from CANDELS HST/F160Wimaging. We find consistent mass normalized (log(M)=10.7) sizes for quiescentfield galaxies ($r_{1/2,maj}=1.81\\\\pm0.29$ kpc) and quiescent cluster galaxies($r_{1/2,maj}=2.17\\\\pm0.63$ kpc). The mass normalized size of star-formingcluster galaxies ($r_{1/2,maj}=4.00\\\\pm0.26$ kpc ) is 12% larger (KS test$2.1\\\\sigma$) than star-forming field galaxies ($r_{1/2,maj}=3.57\\\\pm0.10$ kpc).From the mass-color relation we find that quiescent field galaxies with9.7<log(M)<10.4 are slightly redder (KS test $3.6\\\\sigma$) than quiescentcluster galaxies, while cluster and field quiescent galaxies with log(M)>10.4have consistent colors. We find that star-forming cluster galaxies are onaverage 20% redder than star-forming field galaxies at all masses. Furthermore,we stack galaxy images to measure average radial color profiles as a functionof mass. Negative color gradients are only present for massive star-formingfield and cluster galaxies with log(M)>10.4, the remaining galaxy masses andtypes have flat profiles. Our results suggest given the observed differences insize and color of star-forming field and cluster galaxies, that the environmenthas begun to influence/accelerate their evolution. However, the lack ofdifferences between field and cluster quiescent galaxies indicates that theenvironment has not begun to significantly influence their evolution at z~2.',\n",
              " 'We study the rest-frame ultra-violet sizes of massive (~0.8 x 10^11 M_Sun)galaxies at 3.4<z<4.2, selected from the FourStar Galaxy Evolution Survey(ZFOURGE), by fitting single Sersic profiles to HST/WFC3/F160W images from theCosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS).Massive quiescent galaxies are very compact, with a median circularizedhalf-light radius r_e = 0.63 +/- 0.18 kpc. Removing 5/16 (31%) sources withsigns of AGN activity does not change the result. Star-forming galaxies haver_e = 2.0 +/- 0.60 kpc, 3.2 +/- 1.3 x larger than quiescent galaxies. Quiescentgalaxies at z~4 are on average 6.0 +\\\\- 0.17 x smaller than at z~0 and 1.9 +/-0.7 x smaller than at z~2. Star-forming galaxies of the same stellar mass are2.4 +/- 0.7 x smaller than at z~0. Overall, the size evolution at 0<z<4 is welldescribed by a powerlaw, with r_e = 5.08 +/- 0.28 (1+z)^(-1.44+/-0.08) kpc forquiescent and r_e = 6.02 +/- 0.28 (1+z)^(-0.72+/-0.05) kpc for star-forminggalaxies. Compact star-forming galaxies are rare in our sample: we find only1/14 (7%) with r_e / (M / 10^11 M_Sun)^0.75 < 1.5, whereas 13/16 (81%) of thequiescent galaxies is compact. The number density of compact quiescent galaxiesat z~4 is 1.8 +/- 0.8 x 10^-5 Mpc^-3 and increases rapidly, by >5 x, between2<z<4. The paucity of compact star-forming galaxies at z~4 and their largerest-frame ultra-violet median sizes suggest that the formation phase ofcompact cores is very short and/or highly dust obscured.',\n",
              " 'Developing rapid methods for pathogen detection and growth monitoring at lowcell and analyte concentrations is an important goal, which numeroustechnologies are working towards solving. Rapid biosensors have already made adramatic impact on improving patient outcomes and with continued development,these technologies may also help limit the emergence of antimicrobialresistance and reduce the ever expanding risk of foodborne illnesses. Onetechnology that is being developed with these goals in mind is asynchronousmagnetic bead rotation (AMBR) biosensors. Self-assembled AMBR biosensors havebeen demonstrated at water/air and water/oil interfaces, and here, for thefirst time, we report on self-assembled AMBR biosensors used at a solidinterface. The solid interface configuration was used to measure the growth ofEscherichia coli with two distinct phenomena at low cell concentrations:firstly, the AMBR rotational period decreased and secondly, the rotationalperiod increased after several division times. Taking advantage of this lowcell concentration behavior, a 20% signal change from the growth of E. coliO157:H7 was detected in 91 \\\\pm 4 minutes, with a starting concentration of 5 x103 CFU/mL. Such a rapid cell growth sensor could dramatically improve thedetection time and sensitivity in applications requiring phenotypic testing oftarget cells.',\n",
              " 'In this paper, we propose a general analytical framework for informationspreading in mobile networks based on a new performance metric, mobileconductance, which allows us to separate the details of mobility models fromthe study of mobile spreading time. We derive a general result for theinformation spreading time in mobile networks in terms of this new metric, andinstantiate it through several popular mobility models. Large scale networksimulation is conducted to verify our analysis.',\n",
              " 'A novel Rateless-coding-assisted Multi-Packet Relaying (RMPR) protocol isproposed for large-size data spreading in mobile wireless networks. With thislightweight and robust protocol, the packet redundancy is reduced by a factorof $\\\\sqrt n$, while the spreading time is reduced at least by a factor of $\\\\ln(n)$. Closed-form bounds and explicit non-asymptotic results are presented,which are further validated through simulations. Besides, the packetduplication phenomenon in the network setting is analyzed for the first time.',\n",
              " 'In this paper, our recently proposed mobile-conductance based analyticalframework is extended to the sparse settings, thus offering a unified tool foranalyzing information spreading in mobile networks. A penalty factor isidentified for information spreading in sparse networks as compared to theconnected scenario, which is then intuitively interpreted and verified bysimulations. With the analytical results obtained, the mobility-connectivitytradeoff is quantitatively analyzed to determine how much mobility may beexploited to make up for network connectivity deficiency.',\n",
              " 'This letter studies the impact of relay selection (RS) on the performance ofcooperative non-orthogonal multiple access (NOMA). In particular, a two-stageRS strategy is proposed, and analytical results are developed to demonstratethat this two-stage strategy can achieve the minimal outage probability amongall possible RS schemes, and realize the maximal diversity gain. The providedsimulation results show that cooperative NOMA with this two-stage RS schemeoutperforms that with the conventional max-min approach, and can also yield asignificant performance gain over orthogonal multiple access.',\n",
              " 'Cellular systems are vulnerable to jamming attacks, especially smart jammersthat choose their jamming policies such as the jamming channel frequencies andpower based on the ongoing communication policies and network states. In thisarticle, we present an unmanned aerial vehicle (UAV) aided cellularcommunication framework against jamming. In this scheme, UAVs use reinforcementlearning methods to choose the relay policy for mobile users in cellularsystems, if the serving base station is heavily jammed. More specifically, wepropose a deep reinforcement learning based UAV relay scheme to help cellularsystems resist smart jamming without being aware of the jamming model and thenetwork model in the dynamic game based on the previous anti-jamming relayexperiences and the observed current network status. This scheme can achievethe optimal performance after enough interactions with the jammer. Simulationresults show that this scheme can reduce the bit error rate of the messages andsave energy for the cellular system compared with the existing scheme.',\n",
              " 'Opportunistically sharing the white spaces, or the temporarily unoccupiedspectrum licensed to the primary user (PU), is a practical way to improve thespectrum utilization. In this paper, we consider the fundamental problem ofrate regions achievable for multiple secondary users (SUs) which send theirinformation to a common receiver over such a white space channel. Inparticular, the PU activities are treated as on/off side information, which canbe obtained causally or non-causally by the SUs. The system is then modeled asa multi-switch channel and its achievable rate regions are characterized insome scenarios. Explicit forms of outer and inner bounds of the rate regionsare derived by assuming additional side information, and they are shown to betight in some special cases. An optimal rate and power allocation scheme thatmaximizes the sum rate is also proposed. The numerical results reveal theimpacts of side information, channel correlation and PU activity on theachievable rates, and also verify the effectiveness of our rate and powerallocation scheme. Our work may shed some light on the fundamental limit anddesign tradeoffs in practical cognitive radio systems.',\n",
              " 'Mobile networks receive increasing research interest recently due to theirincreasingly wide applications in various areas; mobile ad hoc networks (MANET)and Vehicular ad hoc networks (VANET) are two prominent examples. Mobilityintroduces challenges as well as opportunities: it is known to improve thenetwork throughput as shown in [1]. In this paper, we analyze the effect ofmobility on the information spreading based on gossip algorithms. Ourcontributions are twofold. Firstly, we propose a new performance metric, mobileconductance, which allows us to separate the details of mobility models fromthe study of mobile spreading time. Secondly, we explore the mobileconductances of several popular mobility models, and offer insights on thecorresponding results. Large scale network simulation is conducted to verifyour analysis.',\n",
              " 'A novel Distributed Spectrum-Aware Clustering (DSAC) scheme is proposed inthe context of Cognitive Radio Sensor Networks (CRSN). DSAC aims at formingenergy efficient clusters in a self-organized fashion while restrictinginterference to Primary User (PU) systems. The spectrum-aware clusteredstructure is presented where the communications consist of intra-clusteraggregation and inter-cluster relaying. In order to save communication power,the optimal number of clusters is derived and the idea of groupwise constrainedclustering is introduced to minimize intra-cluster distance underspectrum-aware constraint. In terms of practical implementation, DSACdemonstrates preferable scalability and stability because of its low complexityand quick convergence under dynamic PU activity. Finally, simulation resultsare given to validate the proposed scheme.',\n",
              " 'In vehicular communications, traffic-related information should be spreadover the network as quickly as possible to maintain a safer transportationsystem. This motivates us to develop more efficient information propagationschemes. In this paper, we propose a novel virtual-MIMO-enabled informationdissemination scheme, in which the vehicles opportunistically form virtualantenna arrays to boost the transmission range and therefore accelerateinformation propagation along the highway. We model the information propagationprocess as a renewal reward process and investigate in detail the\\\\emph{Information Propagation Speed} (IPS) of the proposed scheme. Thecorresponding closed-form IPS is derived, which shows that the IPS increasescubically with the vehicle density but will ultimately converge to a constantupper bound. Moreover, increased mobility also facilitates the informationspreading by offering more communication opportunities. However, the limitednetwork density essentially determines the bottleneck in information spreading.Extensive simulations are carried out to verify our analysis. We also show thatthe proposed scheme exhibits a significant IPS gain over its conventionalcounterpart.',\n",
              " 'The fifth generation (5G) wireless network technology is to be standardizedby 2020, where main goals are to improve capacity, reliability, and energyefficiency, while reducing latency and massively increasing connection density.An integral part of 5G is the capability to transmit touch perception typereal-time communication empowered by applicable robotics and haptics equipmentat the network edge. In this regard, we need drastic changes in networkarchitecture including core and radio access network (RAN) for achievingend-to-end latency on the order of 1 ms. In this paper, we present a detailedsurvey on the emerging technologies to achieve low latency communicationsconsidering three different solution domains: RAN, core network, and caching.We also present a general overview of 5G cellular networks composed of softwaredefined network (SDN), network function virtualization (NFV), caching, andmobile edge computing (MEC) capable of meeting latency and other 5Grequirements.',\n",
              " 'Physical layer security (PLS) is critically important for emerging wirelesscommunication networks to maintain the confidentiality of the information oflegitimate users. In this paper, we investigate enhancing PLS in an unmannedaerial vehicle (UAV) based communication network where a UAV acting as anaerial base station (BS) provides coverage in a densely packed user area (suchas a stadium or a concert area). In particular, non-orthogonal multiple access(NOMA) together with highly-directional multi-antenna transmission techniquesin mmWave frequency bands are utilized for improving spectral efficiency. Inorder to achieve PLS against potential eavesdropper attacks, we introduce aprotected zone around the user region. However, limited resource availabilityrefrain protected zone being extended to cover the entire eavesdropper region.Hence, we propose an approach to optimize the protected zone shape (for fixedarea) at each UAV-BS hovering altitude. The associated secrecy performance isevaluated considering the secrecy outage and sum secrecy rates. Numericalresults reveal the importance of protected zone shape optimization at eachaltitude to maximize NOMA secrecy rates.',\n",
              " 'Massive multiple-input multiple-output (MIMO) is a key technology for 5Gwireless communications with a promise of significant capacity increase. Theuse of low-resolution data converters is crucial for massive MIMO to make theoverall transmission as cost- and energy-efficient as possible. In this work,we consider a downlink millimeter-wave (mmWave) transmission scenario, wheremultiple users are served simultaneously by massive MIMO with one-bitdigital-to-analog (D/A) converters. In particular, we propose a novel precoderdesign based on signal-to-leakage-plus-noise ratio (SLNR), which minimizesenergy leakage into undesired users while taking into account impairments dueto nonlinear one-bit quantization. We show that well-known regularizedzero-forcing (RZF) precoder is a particular version of the proposed SLNR-basedprecoder, which is obtained when quantization impairments are totally ignored.Numerical results underscore significant performance improvements along withthe proposed SLNR-based precoder as compared to either RZF or zero-forcing (ZF)precoders.',\n",
              " 'Cellular-connected unmanned aerial vehicles (UAVs) are recently gettingsignificant attention due to various practical use cases, e.g., surveillance,data gathering, purchase delivery, among other applications. Since UAVs are lowpower nodes, energy and spectral efficient communication is of paramountimportance. To that end, multiple access (MA) schemes can play an importantrole in achieving high energy efficiency and spectral efficiency. In this work,we introduce rate-splitting MA (RSMA) and non-orthogonal MA (NOMA) schemes in acellular-connected UAV network. In particular, we investigate the energyefficiency of the RSMA and NOMA schemes in a millimeter wave (mmWave) downlinktransmission scenario. Furthermore, we optimize precoding vectors of both theschemes by explicitly taking into account the 3GPP antenna propagationpatterns. The numerical results for this realistic transmission scheme indicatethat RSMA is superior to NOMA in terms of overall energy efficiency.',\n",
              " 'RF-powered backscatter communication is a promising new technology that canbe deployed for battery-free applications such as internet of things (IoT) andwireless sensor networks (WSN). However, since this kind of communication isbased on the ambient RF signals and battery-free devices, they are vulnerableto interference and jamming. In this paper, we model the interaction betweenthe user and a smart interferer in an ambient backscatter communication networkas a game. We design the utility functions of both the user and interferer inwhich the backscattering time is taken into the account. The convexity of bothsub-game optimization problems is proved and the closed-form expression for theequilibrium of the Stackelberg game is obtained. Due to lack of informationabout the system SNR and transmission strategy of the interferer, the optimalstrategy is obtained using the Q-learning algorithm in a dynamic iterativemanner. We further introduce hotbooting Q-learning as an effective approach toexpedite the convergence of the traditional Q-learning. Simulation results showthat our approach can obtain considerable performance improvement in comparisonto random and fixed backscattering time transmission strategies and improvesthe convergence speed of Q-Learning by about 31%.',\n",
              " 'Hybrid beamforming is key to achieving energy-efficient 5G wireless networksequipped with massive amount of antennas. Low-resolution data converters bringyet another degree of freedom to energy efficiency for the state-of-the-art 5Gtransceivers. In this work, we consider the design of hybrid precoders formassive multiple-input multiple-output (MIMO) channels in millimeter-wave(mmWave) spectrum along with one-bit digital-to-analog converters (DACs) andfinite-quantized phase shifters. In particular, we propose analternating-optimization-based precoder design which recursively computes thecovariance of the quantization distortion, and updates the precodersaccordingly. Numerical results verify that the achievable rate improves quicklythrough iterations that involve updates to the weight matrix, distortioncovariance of the quantization, and the respective precoders.',\n",
              " 'The non-orthogonal multiple access (NOMA) and millimeter-wave (mmWave)transmission enable the unmanned aerial vehicle (UAV) assisted wirelessnetworks to provide broadband connectivity over densely packed urban areas. Thepresence of malicious receivers, however, compromise the security of theUAV-to-ground communications link, thereby degrading secrecy rates. In thiswork, we consider a NOMA-based transmission strategy in a mmWave UAV-assistedwireless network, and investigate the respective secrecy-rate performancerigorously. In particular, we propose a protected-zone approach to enhance thesecrecy-rate performance by preventing the most vulnerable subregion (outsidethe user region) from the presence of malicious receivers. The respectivesecrecy rates are then derived analytically as a function of the protectedzone, which verifies great secrecy rate improvements through optimizing shapeof the protected zone in use. Furthermore, we show that the optimal protectedzone shape for mmWave links appears as a compromise between protecting theangle versus distance dimension, which would otherwise form to protect solelythe distance dimension for sub-6GHz links. We also numerically evaluate theimpact of transmission power, protected-zone size, and UAV altitude on thesecrecy-rate performance improvements as practical considerations.',\n",
              " 'Due to dense deployments of Internet of things (IoT) networks, interferencemanagement becomes a critical challenge. With the proliferation of aerial IoTdevices, such as unmanned aerial vehicles (UAVs), interference characteristicsin 3D environments will be different than those in the existing terrestrial IoTnetworks. In this paper, we consider 3D topology IoT networks with a mixture ofaerial and terrestrial links, with low-cost cross-dipole antennas at groundnodes and omni-directional antennas at aerial nodes. Considering amassive-access communication scenario, we first derive the statistics of thechannel gain at IoT receivers in closed form while taking into account theradiation patterns of both ground and aerial nodes. These are then used tocalculate the ergodic achievable rate as a function of the height of the aerialreceiver. We propose an interference mitigation scheme that utilizes 3D antennaradiation pattern with different dipole antenna settings. Our results show thatusing the proposed scheme, the ergodic achievable rate improves as the heightof aerial receivers increases. In addition, the ratio between the ground andaerial receivers that maximizes the peak rate also increases with the aerialIoT receiver height.',\n",
              " 'The integration of unmanned aerial vehicles (UAVs) into the terrestrialcellular networks is envisioned as one key technology for next-generationwireless communications. In this work, we consider the physical layer securityof the communications links in the millimeter-wave (mmWave) spectrum which aremaintained by UAVs functioning as base stations (BS). In particular, we proposea new precoding strategy which incorporates the channel state information (CSI)of the eavesdropper (Eve) compromising link security. We show that our proposedprecoder strategy eliminates any need for artificial noise (AN) transmission inunderloaded scenarios (fewer users than number of antennas). In addition, wedemonstrate that our nonlinear precoding scheme provides promising secrecy-rateperformance even for overloaded scenarios at the expense of transmittinglow-power AN.',\n",
              " 'We consider the relaying application of unmanned aerial vehicles (UAVs), inwhich UAVs are placed between two transceivers (TRs) to increase the throughputof the system. Instead of studying the placement of UAVs as pursued in existingliterature, we focus on investigating the placement of a jammer or a majorsource of interference on the ground to effectively degrade the performance ofthe system, which is measured by the maximum achievable data rate oftransmission between the TRs. We demonstrate that the optimal placement of thejammer is in general a non-convex optimization problem, for which obtaining thesolution directly is intractable. Afterward, using the inherent characteristicsof the signal-to-interference ratio (SIR) expressions, we propose a tractableapproach to find the optimal position of the jammer. Based on the proposedapproach, we investigate the optimal positioning of the jammer in both dual-hopand multi-hop UAV relaying settings. Numerical simulations are provided toevaluate the performance of our proposed method.',\n",
              " 'Federated learning (FL) has emerged as a prominent distributed learningparadigm. FL entails some pressing needs for developing novel parameterestimation approaches with theoretical guarantees of convergence, which arealso communication efficient, differentially private and Byzantine resilient inthe heterogeneous data distribution settings. Quantization-based SGD solvershave been widely adopted in FL and the recently proposed SIGNSGD with majorityvote shows a promising direction. However, no existing methods enjoy all theaforementioned properties. In this paper, we propose an intuitively-simple yettheoretically-sound method based on SIGNSGD to bridge the gap. We presentStochastic-Sign SGD which utilizes novel stochastic-sign based gradientcompressors enabling the aforementioned properties in a unified framework. Wealso present an error-feedback variant of the proposed Stochastic-Sign SGDwhich further improves the learning performance in FL. We test the proposedmethod with extensive experiments using deep neural networks on the MNISTdataset and the CIFAR-10 dataset. The experimental results corroborate theeffectiveness of the proposed method.',\n",
              " 'Software-defined internet of vehicles (SDIoV) has emerged as a promisingparadigm to realize flexible and comprehensive resource management, for nextgeneration automobile transportation systems. In this paper, a vehicular cloudcomputing-based SDIoV framework is studied wherein the joint allocation oftransmission power and graph job is formulated as a nonlinear integerprogramming problem. To effectively address the problem, astructure-preservation-based two-stage allocation scheme is proposed thatdecouples template searching from power allocation. Specifically, ahierarchical tree-based random subgraph isomorphism mechanism is applied in thefirst stage by identifying potential mappings (templates) between thecomponents of graph jobs and service providers. A structure-preservingsimulated annealing-based power allocation algorithm is adopted in the secondstage to achieve the trade-off between the job completion time and energyconsumption. Extensive simulations are conducted to verify the performance ofthe proposed algorithms.',\n",
              " \"While machine learning has achieved remarkable results in a wide variety ofdomains, the training of models often requires large datasets that may need tobe collected from different individuals. As sensitive information may becontained in the individual's dataset, sharing training data may lead to severeprivacy concerns. Therefore, there is a compelling need to developprivacy-aware machine learning methods, for which one effective approach is toleverage the generic framework of differential privacy. Considering thatstochastic gradient descent (SGD) is one of the most commonly adopted methodsfor large-scale machine learning problems, a decentralized differentiallyprivate SGD algorithm is proposed in this work. Particularly, we focus on SGDwithout replacement due to its favorable structure for practicalimplementation. Both privacy and convergence analysis are provided for theproposed algorithm. Finally, extensive experiments are performed to demonstratethe effectiveness of the proposed method.\",\n",
              " 'Recently, the privacy guarantees of information dissemination protocols haveattracted increasing research interests, among which the gossip protocolsassume vital importance in various information exchange applications. In thiswork, we study the privacy guarantees of gossip protocols in general networksin terms of differential privacy and prediction uncertainty. First, lowerbounds of the differential privacy guarantees are derived for gossip protocolsin general networks in both synchronous and asynchronous settings. Theprediction uncertainty of the source node given a uniform prior is alsodetermined. For the private gossip algorithm, the differential privacy andprediction uncertainty guarantees are derived in closed form. Moreover,considering that these two metrics may be restrictive in some scenarios, therelaxed variants are proposed. It is found that source anonymity is closelyrelated to some key network structure parameters in the general networksetting. Then, we investigate information spreading in wireless networks withunreliable communications, and quantify the tradeoff between differentialprivacy guarantees and information spreading efficiency. Finally, consideringthat the attacker may not be present at the beginning of the informationdissemination process, the scenario of delayed monitoring is studied and thecorresponding differential privacy guarantees are evaluated.',\n",
              " 'The spectrum-efficient millimeter-wave (mmWave) communications has recentlyattracted much attention as a viable solution to spectrum crunch problem. Inthis work, we propose a novel non-orthogonal multiple access (NOMA) framework,which makes use of the directional propagation characteristics of mmWavecommunications so as to improve the spectral efficiency through non-orthogonalsignaling. In particular, we consider one-bit quantized angle information as alimited yet effective feedback scheme describing the channel quality of userequipment (UE) in mmWave bands. The UE pairs for NOMA transmission are thenestablished using not only the one-bit distance feedback as a classicalapproach, but also the one-bit angle feedback. The proposed strategy istherefore referred to as two-bit NOMA. We also propose a novel hybrid strategy,called combined NOMA, for the circumstances with no UE pair through two-bitNOMA. Whenever no UE pair is available through any NOMA strategy, we resort tosingle user transmission (SUT) with proper UE selection schemes. The hybridsum-rate performance is also analyzed thoroughly with the respective outage andrate expressions. The numerical results verify that the proposed strategyoutperforms one-bit NOMA schemes with either angle- or distance-only feedback.',\n",
              " 'Vehicular cloud computing has emerged as a promising paradigm for realizinguser requirements in computation-intensive tasks in modern drivingenvironments. In this paper, a novel framework of multi-task offloading overvehicular clouds (VCs) is introduced where tasks and VCs are modeled asundirected weighted graphs. Aiming to achieve a trade-off between minimizingtask completion time and data exchange costs, task components are efficientlymapped to available virtual machines in the related VCs. The problem isformulated as a non-linear integer programming problem, mainly underconstraints of limited contact between vehicles as well as available resources,and addressed in low-traffic and rush-hour scenarios. In low-traffic cases, wedetermine optimal solutions; in rush-hour cases, a connection-restrictedrandommatching-based subgraph isomorphism algorithm is proposed that presentslow computational complexity. Evaluations of the proposed algorithms againstgreedy-based baseline methods are conducted via extensive simulations.',\n",
              " 'While stress visualization within 3-dimensional particles would greatlyadvance our understanding of the behaviors of complex particles, traditionalphotoelastic methods suffer from a lack of available technology for producingsuitable complex particles. Recently, 3D-printing has created new possibilitiesfor enhancing the scope of stress analysis within physically representativegranules. Here, we investigate and evaluate opportunities offered by3D-printing a single particle with a complex external shape with photoelasticproperties. We report the results of X-ray computed tomography and 3D-printing,combined with traditional photoelastic analysis, to visualize strain forparticles ranging from simple 2D discs to complex 3D printed coffee beans,including with internal voids. We find that the relative orientation of theprint layers and the loading force affects the optical response of the discs,but without a significant difference in their mechanical properties.Furthermore, we present semi-quantitative measurements of stresses within3D-printed complex particles. The paper outlines the potential limitations andareas of future interest for stress visualization of 3-dimensional particles.',\n",
              " \"Circular (or cyclic) proofs have received increasing attention in recentyears, and have been proposed as an alternative setting for studying(co)inductive reasoning. In particular, now several type systems based oncircular reasoning have been proposed. However, little is known about thecomplexity theoretic aspects of circular proofs, which exhibit sophisticatedloop structures atypical of more common 'recursion schemes'. This paperattempts to bridge the gap between circular proofs and implicit computationalcomplexity. Namely we introduce a circular proof system based on Bellantoni andCook's famous safe-normal function algebra, and we identify suitable prooftheoretical constraints to characterise the polynomial-time and elementarycomputable functions.\",\n",
              " 'Anonymous communication systems are subject to selective denial-of-service(DoS) attacks. Selective DoS attacks lower anonymity as they force paths to berebuilt multiple times to ensure delivery which increases the opportunity formore attack. In this paper we present a detection algorithm that filters outcompromised communication channels for one of the most widely used anonymitynetworks, Tor. Our detection algorithm uses two levels of probing to filter outpotentially compromised tunnels. We perform probabilistic analysis andextensive simulation to show the robustness of our detection algorithm. We alsoanalyze the overhead of our detection algorithm and show that we can achievesatisfactory security guarantee for reasonable communication overhead (5% ofthe total available Tor bandwidth in the worst case). Real world experimentsreveal that our detection algorithm provides good defense against selective DoSattack.',\n",
              " 'The widespread use of smart devices gives rise to both security and privacyconcerns. Fingerprinting smart devices can assist in authenticating physicaldevices, but it can also jeopardize privacy by allowing remote identificationwithout user awareness. We propose a novel fingerprinting approach that usesthe microphones and speakers of smart phones to uniquely identify an individualdevice. During fabrication, subtle imperfections arise in device microphonesand speakers which induce anomalies in produced and received sounds. We exploitthis observation to fingerprint smart devices through playback and recording ofaudio samples. We use audio-metric tools to analyze and explore differentacoustic features and analyze their ability to successfully fingerprint smartdevices. Our experiments show that it is even possible to fingerprint devicesthat have the same vendor and model; we were able to accurately distinguishover 93% of all recorded audio clips from 15 different units of the same model.Our study identifies the prominent acoustic features capable of fingerprintingdevices with high success rate and examines the effect of background noise andother variables on fingerprinting accuracy.',\n",
              " 'The Tor anonymity network has been shown vulnerable to traffic analysisattacks by autonomous systems and Internet exchanges, which can observedifferent overlay hops belonging to the same circuit. We aim to determinewhether network path prediction techniques provide an accurate picture of thethreat from such adversaries, and whether they can be used to avoid thisthreat. We perform a measurement study by running traceroutes from Tor relaysto destinations around the Internet. We use the data to evaluate the accuracyof the autonomous systems and Internet exchanges that are predicted to appearon the path using state-of-the-art path inference techniques; we also considerthe impact that prediction errors have on Tor security, and whether it ispossible to produce a useful overestimate that does not miss important threats.Finally, we evaluate the possibility of using these predictions to activelyavoid AS and IX adversaries and the challenges this creates for the design ofTor.',\n",
              " 'We consider the proof complexity of the minimal complete fragment, KS, ofstandard deep inference systems for propositional logic. To examine the size ofproofs we employ atomic flows, diagrams that trace structural changes through aproof but ignore logical information. As results we obtain a polynomialsimulation of versions of Resolution, along with some extensions. We also showthat these systems, as well as bounded-depth Frege systems, cannot polynomiallysimulate KS, by giving polynomial-size proofs of certain variants of thepropositional pigeonhole principle in KS.',\n",
              " 'Modern smartphones contain motion sensors, such as accelerometers andgyroscopes. These sensors have many useful applications; however, they can alsobe used to uniquely identify a phone by measuring anomalies in the signals,which are a result from manufacturing imperfections. Such measurements can beconducted surreptitiously in the browser and can be used to track users acrossapplications, websites, and visits.  We analyze techniques to mitigate such device fingerprinting either bycalibrating the sensors to eliminate the signal anomalies, or by adding noisethat obfuscates the anomalies. To do this, we first develop a highly accuratefingerprinting mechanism that combines multiple motion sensors and makes use of(inaudible) audio stimulation to improve detection. We then collectmeasurements from a large collection of smartphones and evaluate the impact ofcalibration and obfuscation techniques on the classifier accuracy.',\n",
              " 'Circular and non-wellfounded proofs have become an increasingly popular toolfor metalogical treatments of systems with forms of induction and/or recursion.In this work we investigate the expressivity of a variant CT of G\\\\\"odel\\'ssystem T where programs are circularly typed, rather than including an explicitrecursion combinator. In particular, we examine the abstraction complexity(i.e. type level) of C, and show that the G\\\\\"odel primitive recursivefunctionals may be typed more succinctly with circular derivations, using typesprecisely one level lower than in T. In fact we give a logical correspondencebetween the two settings, interpreting the quantifier-free type 1 theory oflevel n+1 T into that of level n C and vice-versa.  We also obtain some further results and perspectives on circular\\'derivations\\', namely strong normalisation and confluence, models based onhereditary computable functionals, continuity at type 2, and a translation toterms of $\\\\T$ computing the same functional, at all types.',\n",
              " 'This paper studies propositional proof systems in which lines are sequents ofdecision trees or branching programs - deterministic and nondeterministic. Thesystems LDT and LNDT are propositional proof systems in which lines representdeterministic or non-deterministic decision trees. Branching programs aremodeled as decision dags. Adding extension to LDT and LNDT gives systems eLDTand eLNDT in which lines represent deterministic and non-deterministicbranching programs, respectively.  Deterministic and non-deterministic branching programs correspond tolog-space (L) and nondeterministic log-space (NL). Thus the systems eLDT andeLNDT are propositional proof systems that reason with (nonuniform) L and NLproperties.  The main results of the paper are simulation and non-simulation results fortree-like and dag-like proofs in the systems LDT, LNDT, eLDT, and eLNDT. Thesesystems are also compared with Frege systems, constantdepth Frege systems andextended Frege systems',\n",
              " \"We propose a graph-based extension of Boolean logic called Boolean GraphLogic (BGL). Construing formula trees as the cotrees of cographs, we may statesemantic notions such as evaluation and entailment in purely graph-theoreticterms, whence we recover the definition of BGL. Naturally, it is conservativeover usual Boolean logic.  Our contributions are the following:  (1) We give a natural semantics of BGL based on Boolean relations, i.e. it isa multivalued semantics, and show adequacy of this semantics for thecorresponding notions of entailment. (2) We show that the complexity ofevaluation is NP-complete for arbitrary graphs (as opposed to ALOGTIME-completefor formulas), while entailment is $\\\\Pi^p_2$-complete (as opposed tocoNP-complete for formulas). (3) We give a 'recursive' algorithm for evaluationby induction on the modular decomposition of graphs. (Though this is notpolynomial-time, cf. point (2) above). (4) We characterise evaluation in agame-theoretic setting, in terms of both static and sequentical strategies,extending the classical notion of positional game forms beyond cographs. (5) Wegive an axiomatisation of BGL, inspired by deep-inference proof theory, andshow soundness and completeness for the corresponding notions of entailment.  One particular feature of the graph-theoretic setting is that it escapescertain no-go theorems such as a recent result of Das and Strassburger, thatthere is no linear axiomatisation of the linear fragment of Boolean logic(equivalently the multiplicative fragment of Japaridze's Computability Logic orBlass' game semantics for Mutliplicative Linear Logic).\",\n",
              " \"We investigate the proof complexity of systems based on positive branchingprograms, i.e. non-deterministic branching programs (NBPs) where, for any0-transition between two nodes, there is also a 1-transition. Positive NBPscompute monotone Boolean functions, just like negation-free circuits orformulas, but constitute a positive version of (non-uniform) NL, rather than Por NC1, respectively.  The proof complexity of NBPs was investigated in previous work by Buss, Dasand Knop, using extension variables to represent the dag-structure, over alanguage of (non-deterministic) decision trees, yielding the system eLNDT. Oursystem eLNDT+ is obtained by restricting their systems to a positive syntax,similarly to how the 'monotone sequent calculus' MLK is obtained from the usualsequent calculus LK by restricting to negation-free formulas.  Our main result is that eLNDT+ polynomially simulates eLNDT over positivesequents. Our proof method is inspired by a similar result for MLK by Atserias,Galesi and Pudl\\\\'ak, that was recently improved to a bona fide polynomialsimulation via works of Je\\\\v{r}\\\\'abek and Buss, Kabanets, Kolokolova andKouck\\\\'y. Along the way we formalise several properties of counting functionswithin eLNDT+ by polynomial-size proofs and, as a case study, give explicitpolynomial-size poofs of the propositional pigeonhole principle.\",\n",
              " 'We present deductive systems for various modal logics that can be obtainedfrom the constructive variant of the normal modal logic CK by addingcombinations of the axioms d, t, b, 4, and 5. This includes the constructivevariants of the standard modal logics K4, S4, and S5. We use for ourpresentation the formalism of nested sequents and give a syntactic proof of cutelimination.',\n",
              " 'The main objective of this paper is to introduced a new sequence space$l_{p}(\\\\hat{F}(r,s)),$ $ 1\\\\leq p \\\\leq \\\\infty$ by using the band matrix$\\\\hat{F}(r,s).$  We also establish a few inclusion relations concerning this space anddetermine its $\\\\alpha-,\\\\beta-,\\\\gamma-$duals. We also characterize some matrixclasses on the space $l_{p}(\\\\hat{F}(r,s))$ and examine some geometricproperties of this space.',\n",
              " 'The aim of the paper is to introduced the spaces $c_{0}^{\\\\lambda}(\\\\hat{F})$and $c^{\\\\lambda}(\\\\hat{F})$ which are the BK-spaces of non-absolute type andalso derive some inclusion relations. Further, we determine the$\\\\alpha-,\\\\beta-,\\\\gamma-$duals of those spaces and also construct their bases.We also characterize some matrix classes on the spaces$c_{0}^{\\\\lambda}(\\\\hat{F})$ and $c^{\\\\lambda}(\\\\hat{F}).$ Here we characterize thesubclasses $\\\\mathcal{K}(X,Y)$ of compact operators where $X$ is$c_{0}^{\\\\lambda}(\\\\hat{F})$ or $c^{\\\\lambda}(\\\\hat{F})$ and $Y$ is one of thespaces $c_{0},c, l_{\\\\infty}, l_{1}, bv$ by applying Hausdorff measure ofnoncompactness.',\n",
              " \"We propose a cut-free cyclic system for Transitive Closure Logic (TCL) basedon a form of hypersequents, suitable for automated reasoning via proof search.We show that previously proposed sequent systems are cut-free incomplete forbasic validities from Kleene Algebra (KA) and Propositional Dynamic Logic(PDL), over standard translations. On the other hand, our system faithfullysimulates known cyclic systems for KA and PDL, thereby inheriting theircompleteness results. A peculiarity of our system is its richer correctnesscriterion, exhibiting 'alternating traces' and necessitating a more intricatesoundness argument than for traditional cyclic proofs.\",\n",
              " \"In this work we investigate how to extract alternating time bounds from'focussed' proof systems. Our main result is the obtention of fragments ofMALLw (MALL with weakening) complete for each level of the polynomialhierarchy. In one direction we encode QBF satisfiability and in the other weencode focussed proof search, and we show that the composition of the twoencodings preserves quantifier alternation, yielding the required result. Bycarefully composing with well-known embeddings of MALLw into MALL, we obtain asimilar delineation of MALL formulas, again carving out fragments complete foreach level of the polynomial hierarchy. This refines the well-known resultsthat both MALLw and MALL are PSPACE-complete.  A key insight is that we have to refine the usual presentation of focussingto account for deterministic computations in proof search, which correspond toinvertible rules that do not branch. This is so that we may more faithfullyassociate phases of focussed proof search to their alternating time complexity.This presentation seems to uncover further dualities at the level of proofsearch than usual presentations, so could be of further proof theoreticinterest in its own right.\",\n",
              " 'Advertisers are increasingly turning to fingerprinting techniques to trackusers across the web. As web browsing activity shifts to mobile platforms,traditional browser fingerprinting techniques become less effective; however,device fingerprinting using built-in sensors offers a new avenue for attack. Westudy the feasibility of using motion sensors to perform device fingerprintingat scale, and explore countermeasures that can be used to protect privacy.  We perform a large-scale user study to demonstrate that motion sensorfingerprinting is effective with even 500 users. We also develop a model toestimate prediction accuracy for larger user populations; our model provides aconservative estimate of at least 12% classification accuracy with 100000users. We then investigate the use of motion sensors on the web and find,distressingly, that many sites send motion sensor data to servers for storageand analysis, paving the way to potential fingerprinting. Finally, we considerthe problem of developing fingerprinting countermeasures; we evaluate apreviously proposed obfuscation technique and a newly developed quantizationtechnique via a user study. We find that both techniques are able todrastically reduce fingerprinting accuracy without significantly impacting theutility of the sensors in web applications.',\n",
              " \"Comment on ``Demystifying Double Robustness: A Comparison of AlternativeStrategies for Estimating a Population Mean from Incomplete Data''[arXiv:0804.2958]\",\n",
              " 'The COVID-19 pandemic due to the novel coronavirus SARS CoV-2 has inspiredremarkable breakthroughs in development of vaccines against the virus and thelaunch of several phase 3 vaccine trials in Summer 2020 to evaluate vaccineefficacy (VE). Trials of vaccine candidates using mRNA delivery systemsdeveloped by Pfizer-BioNTech and Moderna have shown substantial VEs of 94-95%,leading the US Food and Drug Administration to issue Emergency UseAuthorizations and subsequent widespread administration of the vaccines. As thetrials continue, a key issue is the possibility that VE may wane over time.Ethical considerations dictate that all trial participants be unblinded andthose randomized to placebo be offered vaccine, leading to trial protocolamendments specifying unblinding strategies. Crossover of placebo subjects tovaccine complicates inference on waning of VE. We focus on the particularfeatures of the Moderna trial and propose a statistical framework based on apotential outcomes formulation within which we develop methods for inference onwhether or not VE wanes over time and estimation of VE at any post-vaccinationtime. The framework clarifies assumptions made regarding individual- andpopulation-level phenomena and acknowledges the possibility that subjects whoare more or less likely to become infected may be crossed over to vaccinedifferentially over time. The principles of the framework can be adaptedstraightforwardly to other trials.',\n",
              " 'In many randomized clinical trials of therapeutics for COVID-19, the primaryoutcome is an ordinal categorical variable, and interest focuses on the oddsratio (active agent vs. control) under the assumption of a proportional oddsmodel. Although at the final analysis the outcome will be determined for allsubjects, at an interim analysis, the status of some participants may not yetbe determined, e.g., because ascertainment of the outcome may not be possibleuntil some pre-specified follow-up time. Accordingly, the outcome from thesesubjects can be viewed as censored. A valid interim analysis can be based ondata only from those subjects with full follow up; however, this approach isinefficient, as it does not exploit additional information that may beavailable on those for whom the outcome is not yet available at the time of theinterim analysis. Appealing to the theory of semiparametrics, we propose anestimator for the odds ratio in a proportional odds model with censored,time-lagged categorical outcome that incorporates additional baseline andtime-dependent covariate information and demonstrate that it can result inconsiderable gains in efficiency relative to simpler approaches. A byproduct ofthe approach is a covariate-adjusted estimator for the odds ratio based on thefull data that would be available at a final analysis.',\n",
              " 'The primary analysis in two-arm clinical trials usually involves inference ona scalar treatment effect parameter; e.g., depending on the outcome, thedifference of treatment-specific means, risk difference, risk ratio, or oddsratio. Most clinical trials are monitored for the possibility of earlystopping. Because ordinarily the outcome on any given subject can beascertained only after some time lag, at the time of an interim analysis, amongthe subjects already enrolled, the outcome is known for only a subset and iseffectively censored for those who have not been enrolled sufficiently long forit to be observed. Typically, the interim analysis is based only on the datafrom subjects for whom the outcome has been ascertained. A goal of an interimanalysis is to stop the trial as soon as the evidence is strong enough to doso, suggesting that the analysis ideally should make the most efficient use ofall available data, thus including information on censoring as well as otherbaseline and time-dependent covariates in a principled way. A general groupsequential framework is proposed for clinical trials with a time-laggedoutcome. Treatment effect estimators that take account of censoring andincorporate covariate information at an interim analysis are derived usingsemiparametric theory and are demonstrated to lead to stronger evidence forearly stopping than standard approaches. The associated test statistics areshown to have the independent increments structure, so that standard softwarecan be used to obtain stopping boundaries.',\n",
              " \"In clinical practice, physicians make a series of treatment decisions overthe course of a patient's disease based on his/her baseline and evolvingcharacteristics. A dynamic treatment regime is a set of sequential decisionrules that operationalizes this process. Each rule corresponds to a decisionpoint and dictates the next treatment action based on the accrued information.Using existing data, a key goal is estimating the optimal regime, that, iffollowed by the patient population, would yield the most favorable outcome onaverage. Q- and A-learning are two main approaches for this purpose. We providea detailed account of these methods, study their performance, and illustratethem using data from a depression study.\",\n",
              " \"A treatment regime is a deterministic function that dictates personalizedtreatment based on patients' individual prognostic information. There is afast-growing interest in finding optimal treatment regimes to maximize expectedlong-term clinical outcomes of patients for complex diseases, such as cancerand AIDS. For many clinical studies with survival time as a primary endpoint, amain goal is to maximize patients' survival probabilities given treatments. Inthis article, we first propose two nonparametric estimators for survivalfunction of patients following a given treatment regime. Then, we derive theestimation of the optimal treatment regime based on a value-based searchingalgorithm within a set of treatment regimes indexed by parameters. Theasymptotic properties of the proposed estimators for survival probabilitiesunder derived optimal treatment regimes are established under suitableregularity conditions. Simulations are conducted to evaluate the numericalperformance of the proposed estimators under various scenarios. An applicationto an AIDS clinical trial data is also given to illustrate the methods.\",\n",
              " 'A treatment regime formalizes personalized medicine as a function fromindividual patient characteristics to a recommended treatment. A high-qualitytreatment regime can improve patient outcomes while reducing cost, resourceconsumption, and treatment burden. Thus, there is tremendous interest inestimating treatment regimes from observational and randomized studies.However, the development of treatment regimes for application in clinicalpractice requires the long-term, joint effort of statisticians and clinicalscientists. In this collaborative process, the statistician must integrateclinical science into the statistical models underlying a treatment regime andthe clinician must scrutinize the estimated treatment regime for scientificvalidity. To facilitate meaningful information exchange, it is important thatestimated treatment regimes be interpretable in a subject-matter context. Wepropose a simple, yet flexible class of treatment regimes whose members arerepresentable as a short list of if-then statements. Regimes in this class areimmediately interpretable and are therefore an appealing choice for broadapplication in practice. We derive a robust estimator of the optimal regimewithin this class and demonstrate its finite sample performance usingsimulation experiments. The proposed method is illustrated with data from twoclinical trials.',\n",
              " 'Sequential Multiple Assignment Randomized Trials (SMARTs) are considered thegold standard for estimation and evaluation of treatment regimes. SMARTs aretypically sized to ensure sufficient power for a simple comparison, e.g., thecomparison of two fixed treatment sequences. Estimation of an optimal treatmentregime is conducted as part of a secondary and hypothesis-generating analysiswith formal evaluation of the estimated optimal regime deferred to a follow-uptrial. However, running a follow-up trial to evaluate an estimated optimaltreatment regime is costly and time-consuming; furthermore, the estimatedoptimal regime that is to be evaluated in such a follow-up trial may be farfrom optimal if the original trial was underpowered for estimation of anoptimal regime. We derive sample size procedures for a SMART that ensure: (i)sufficient power for comparing the optimal treatment regime with standard ofcare; and (ii) the estimated optimal regime is within a given tolerance of thetrue optimal regime with high-probability. We establish asymptotic validity ofthe proposed procedures and demonstrate their finite sample performance in aseries of simulation experiments.',\n",
              " \"The sequential multiple assignment randomized trial (SMART) is the goldstandard trial design to generate data for the evaluation of multi-stagetreatment regimes. As with conventional (single-stage) randomized clinicaltrials, interim monitoring allows early stopping; however, there are fewmethods for principled interim analysis in SMARTs. Because SMARTs involvemultiple stages of treatment, a key challenge is that not all enrolledparticipants will have progressed through all treatment stages at the time ofan interim analysis. Wu et al. (2021) propose basing interim analyses on anestimator for the mean outcome under a given regime that uses data only fromparticipants who have completed all treatment stages. We propose an estimatorfor the mean outcome under a given regime that gains efficiency by usingpartial information from enrolled participants regardless of their progressionthrough treatment stages. Using the asymptotic distribution of this estimator,we derive associated Pocock and O'Brien-Fleming testing procedures for earlystopping. In simulation experiments, the estimator controls type I error andachieves nominal power while reducing expected sample size relative to themethod of Wu et al. (2021). We present an illustrative application of theproposed estimator based on a recent SMART evaluating behavioral paininterventions for breast cancer patients.\",\n",
              " 'Precision medicine is currently a topic of great interest in clinical andintervention science. One way to formalize precision medicine is through atreatment regime, which is a sequence of decision rules, one per stage ofclinical intervention, that map up-to-date patient information to a recommendedtreatment. An optimal treatment regime is defined as maximizing the mean ofsome cumulative clinical outcome if applied to a population of interest. It iswell-known that even under simple generative models an optimal treatment regimecan be a highly nonlinear function of patient information. Consequently, afocal point of recent methodological research has been the development offlexible models for estimating optimal treatment regimes. However, in manysettings, estimation of an optimal treatment regime is an exploratory analysisintended to generate new hypotheses for subsequent research and not to directlydictate treatment to new patients. In such settings, an estimated treatmentregime that is interpretable in a domain context may be of greater value thanan unintelligible treatment regime built using \"black-box\" estimation methods.We propose an estimator of an optimal treatment regime composed of a sequenceof decision rules, each expressible as a list of \"if-then\" statements that canbe presented as either a paragraph or as a simple flowchart that is immediatelyinterpretable to domain experts. The discreteness of these lists precludessmooth, i.e., gradient-based, methods of estimation and leads to non-standardasymptotics. Nevertheless, we provide a computationally efficient estimationalgorithm, prove consistency of the proposed estimator, and derive rates ofconvergence. We illustrate the proposed methods using a series of simulationexamples and application to data from a sequential clinical trial on bipolardisorder.',\n",
              " 'Scalability properties of deep neural networks raise key research questions,particularly as the problems considered become larger and more challenging.This paper expands on the idea of conditional computation introduced by Bengio,et. al., where the nodes of a deep network are augmented by a set of gatingunits that determine when a node should be calculated. By factorizing theweight matrix into a low-rank approximation, an estimation of the sign of thepre-nonlinearity activation can be efficiently obtained. For networks usingrectified-linear hidden units, this implies that the computation of a hiddenunit with an estimated negative pre-nonlinearity can be ommitted altogether, asits value will become zero when nonlinearity is applied. For sparse neuralnetworks, this can result in considerable speed gains. Experimental resultsusing the MNIST and SVHN data sets with a fully-connected deep neural networkdemonstrate the performance robustness of the proposed scheme with respect tothe error introduced by the conditional computation process.',\n",
              " 'We analyze the quantum chaotic behavior of the Yukawa-SYK model as a functionof filling and temperature, which describes random Yukawa interactions between$N$ complex fermions and $M$ bosons in zero spatial dimensions, for both thenon-Fermi liquid and insulating states at finite temperature and chemicalpotential. We solve the ladder equations for the out-of-time-order correlator(OTOC) for both the bosons and fermions. Despite the appearance of the chemicalpotential in the Hamiltonian, which explicitly introduces an additional energyscale, the OTOCs for the fermions and bosons in the non-Fermi liquid state turnout to be unaffected, and the Lyapunov exponents that diagnose chaos remainmaximal. As the chemical potential increases, the system is known to experiencea first-order transition from a critical phase to a gapped insulating phase. Wepostulate that the boundary of the region in parameter space where each phaseis (meta)stable coincides with the curve on which the Lyapunov exponent ismaximal. By calculating the exponent in the insulating phase and comparing tonumerical results on the boundaries of stability, we show that this isplausible.',\n",
              " 'We construct links of arbitrarily many components each component of which isslice and yet are not concordant to any link with even one unknotted component.The only tool we use comes from the Alexander modules.',\n",
              " 'We give an explicit construction of linearly independent families of knotsarbitrarily deep in the (n)-solvable filtration of the knot concordance groupusing the \\\\rho^1-invariant. A difference between previous constructions ofinfinite rank subgroups in the concordance group and ours is that the deepestinfecting knots in the construction we present are allowed to have vanishingTristram-Levine signatures.',\n",
              " 'We use a link invariant defined by Cimasoni-Florens to compute\\\\rho-invariants. This generalizes results of Cochran-Teichner and Friedl onknots to the setting of links. As an application, we prove with only twelvepossible exceptions that the twist knots of algebraic order two are linearlyindependent in the topological concordance group.',\n",
              " \"Any two knots admit orientation preserving homeomorphic Seifert surfaces, ascan be seen by stabilizing. There is a generalization of a Seifert surface tothe setting of links called a C-complex. In this paper, we ask when two linkswill admit orientation preserving homeomorphic C-complexes. In the case of2-component links, we find that the pairwise linking number provides a completeobstruction. In the case of links with 3 or more components and zero pairwiselinking number, Milnor's triple linking number provides a complete obstruction.\",\n",
              " 'In groundbreaking work from 2004, Cimasoni gave a geometric computation ofthe multivariable Conway potential function in terms of a generalization of aSeifert surface for a link called a C-complex. Lemma 3 of that paper provides afamily of moves which relates any two C-complexes for a fixed link. This allowsfor an approach to studying links from the point of view of C-complexes and infollowing papers it has been used to derive invariants. This lemma is false. Wepresent counterexamples, a correction with detailed proof, and an analysis ofthe consequences of this error on subsequent works that rely on this lemma.',\n",
              " 'We give a refined value group for the collection of triple linking numbers oflinks in the 3-sphere. Given two links with the same pairwise linking numberswe show that they have the same refined triple linking number collection if andonly if the links admit homeomorphic surface systems. Moreover these twoconditions hold if and only if the link exteriors are bordant over $B\\\\mathbb{Z}^n$, and if and only if the third lower central series quotients$\\\\pi/\\\\pi_3$ of the link groups are isomorphic preserving meridians andlongitudes. We also show that these conditions imply that the link groups haveisomorphic fourth lower central series quotients $\\\\pi/\\\\pi_4$, preservingmeridians.',\n",
              " 'We discuss an infinite class of metabelian Von Neumann rho-invariants. Eachone is a homomorphism from the monoid of knots to the real line. In generalthey are not well defined on the concordance group. Nonetheless, we show thatthey pass to well defined homomorphisms from the subgroup of the concordancegroup generated by anisotropic knots. Thus, the computation of even one ofthese invariants can be used to conclude that a knot is of infinite order. Weintroduce a method to give a computable bound on these invariants. Finally wecompute this bound to get a new and explicit infinite set of twist knots whichis linearly independent in the concordance group and whose every member is ofalgebraic order 2 .',\n",
              " 'A polynomial f(t) with rational coefficients is strongly irreducible iff(t^k) is irreducible for all positive integers k. Likewise, two polynomials fand g are strongly coprime if f(t^k) and g(t^l) are relatively prime for allpositive integers k and l. We provide some sufficient conditions for strongirreducibility and prove that the Alexander polynomials of twist knots arepairwise strongly coprime and that most of them are strongly irreducible. Weapply these results to describe the structure of the subgroup of the rationalknot concordance group generated by the twist knots and to provide an explicitset of knots which represent linearly independent elements deep in the solvablefiltration of the knot concordance group.',\n",
              " \"In the 60's Levine proved that if $R$ is a slice knot, then on any genus $g$Seifert surface for $R$ there is a $g$ component link $J$, called a derivativeof $R$, on which the Seifert form vanishes. Many subsequent obstructions to $R$being slice are given in terms of slice obstructions of $J$. Many of theseobstructions can be derived from a 4-manifold called a null-bordism. Recentlythe authors proved that that it is possible for $R$ to be slice without $J$being slice, disproving a conjecture of Kauffmann from the 80's. In this paperwe cut open these null-bordisms in order to derive new obstructions to beingthe derivative of a slice knot. As a proof of the strength of this approach were-derive a signature condition due to Daryl Cooper. Our results also apply todoubling operators, giving new evidence for their weak injectivity. We closewith a new sufficient condition for a genus 1 algebraically slice knot to be$1.5$-solvable.\",\n",
              " \"In the 1980's Daryl Cooper introduced the notion of a C-complex (orclasp-complex) bounded by a link and explained how to compute signatures andpolynomial invariants using a C-complex. Since then this was extended by worksof Cimasoni, Florens, Mellor, Melvin, Conway, Toffoli, Friedl, and others tocompute other link invariants. Informally a C-complex is a union of surfaceswhich are allowed to intersect each other in clasps. The purpose of the currentpaper is to study the minimal number of clasps amongst all C-complexes boundedby a fixed link $L$. This measure of complexity is related to the number ofcrossing changes needed to reduce $L$ to a boundary link. We prove that if $L$is a 2-component link with nonzero linking number, then the linking numberdetermines the minimal number of clasps amongst all C-complexes. In the case of3-component links, the triple linking number provides an additional lower boundon the number of clasps in a C-complex.\",\n",
              " 'In a groundbreaking work A. Levine proved the surprising result that thereexist knots in homology spheres which are not smoothly concordant to any knotin $S^3$, even if one allows for concordances in homology cobordisms. Sincethen subsequent works due to Hom-Levine-Lidman and Zhou have strengthened thisresult showing that there are many knots in homology spheres which are notsmoothly concordant to knots in $S^3$. In this paper we present evidence thatthe opposite is true topologically. We study the Whitney tower filtration ofconcordance due to Cochran-Orr-Teichner and prove that modulo any term in thisfiltration every knot (or link) in a homology sphere is equivalent to a knot(or link) in $S^3$. As an application we recover the main result of[Davis2019], namely that the solvable filtration similarly fails to distinguishlinks in homology spheres from links in $S^3$.',\n",
              " 'We study the dynamics of cubic polynomials restricted to their basins ofinfinity, and we enumerate topological conjugacy classes with givencombinatorics.',\n",
              " 'We provide an algorithm for computing the Euler characteristic of the curves$S_p$ in the space of cubic polynomials, consisting of all polynomials with aperiodic critical point of period $p$. The curves were introduced in [Milnor,Bonifant-Kiwi-Milnor], and the algorithm applies the main results of[DeMarco-Pilgrim]. The output is shown for periods $p \\\\leq 26$.',\n",
              " 'In this article, we prove the equivalence of dynamical stability,preperiodicity, and canonical height 0, for algebraic families of rational maps$f_t: \\\\mathbb{P}^1(\\\\mathbb{C}) \\\\to \\\\mathbb{P}^1(\\\\mathbb{C})$, parameterized by$t$ in a quasi-projective complex variety. We use this to prove one implicationin the if-and-only-if statement of Conjecture 1.10 in [Baker-DeMarco, 2013] onunlikely intersections in the moduli space of rational maps; we present theconjecture here in a more general form.',\n",
              " 'In these lecture notes, we present a connection between the complex dynamicsof a family of rational functions $f_t: \\\\mathbb{P}^1\\\\to \\\\mathbb{P}^1$,parameterized by $t$ in a Riemann surface $X$, and the arithmetic dynamics of$f_t$ on rational points $\\\\mathbb{P}^1(k)$ where $k = \\\\mathbb{C}(X)$ or$\\\\bar{\\\\mathbb{Q}}(X)$. An explicit relation between stability and canonicalheight is explained, with a proof that contains a piece of the Mordell-Weiltheorem for elliptic curves over function fields. Our main goal is to pose somequestions and conjectures about these families, guided by the principle of\"unlikely intersections\" from arithmetic geometry, as in [Zannier 2012]. Wealso include a proof that the hyperbolic postcritically-finite maps are Zariskidense in the moduli space of rational maps of any given degree $d>1$. Thesenotes are based on four lectures at KAWA 2015, in Pisa, Italy, designed for anaudience specializing in complex analysis, expanding upon the main results of[Baker-DeMarco 2013, DeMarco 2016, DeMarco-Wang-Ye 2016].',\n",
              " 'We prove a formula for the Fekete-Leja transfinite diameter of the pullbackof a set E in C^N by a regular polynomial map F, expressing it in terms of theresultant of the leading part of F and the transfinite diameter of E. We alsoestablish the nonarchimedean analogue of this formula. A key step in the proofis a formula for the transfinite diameter of the filled Julia set of F.',\n",
              " 'Let $\\\\MP_d$ denote the space of polynomials $f: \\\\C \\\\to \\\\C$ of degree $d\\\\geq2$, modulo conjugation by $\\\\Aut(\\\\C)$. Using properties of polynomial trees (asintroduced in [DM, math.DS/0608759]), we show that if $f_n$ is a divergentsequence of polynomials in $\\\\MP_d$, then any subsequential limit of themeasures of maximal entropy $m(f_n)$ will have finite support. With similartechniques, we observe that the iteration maps $\\\\{\\\\MPbar_d \\\\dashrightarrow\\\\MPbar_{d^n}: n\\\\geq 1\\\\}$ between GIT-compactifications can be resolvedsimultaneously with only finitely many blow-ups of $\\\\MPbar_d$.',\n",
              " 'We consider the problem of classifying the dynamics of complex polynomials$f: \\\\mathbb{C} \\\\to \\\\mathbb{C}$ restricted to their basins of infinity. Wesynthesize existing combinatorial tools --- tableaux, trees, and laminations--- into a new invariant of basin dynamics we call the pictograph. Forpolynomials with all critical points escaping to infinity, we obtain a completedescription of the set of topological conjugacy classes. We give an algorithmfor constructing abstract pictographs, and we provide an inductive algorithmfor counting topological conjugacy classes with a given pictograph.',\n",
              " 'We study critical orbits and bifurcations within the moduli space ofquadratic rational maps on $\\\\mathbb{P}^1$. We focus on the family of curves,$Per_1(\\\\lambda)$ for $\\\\lambda$ in $\\\\mathbb{C}$, defined by the condition thateach $f\\\\in Per_1(\\\\lambda)$ has a fixed point of multiplier $\\\\lambda$. We provethat the curve $Per_1(\\\\lambda)$ contains infinitely many postcritically-finitemaps if and only if $\\\\lambda = 0$; addressing a special case of [BD2,Conjecture 1.4]. We also show that the two critical points of a map $f$ definedistinct bifurcation measures along $Per_1(\\\\lambda)$.',\n",
              " 'Any planar shape $P\\\\subset \\\\mathbb{C}$ can be embedded isometrically as partof the boundary surface $S$ of a convex subset of $\\\\mathbb{R}^3$ such that$\\\\partial P$ supports the positive curvature of $S$. The complement $Q = S\\\\setminus P$ is the associated {\\\\em cap}. We study the cap construction whenthe curvature is harmonic measure on the boundary of$(\\\\hat{\\\\mathbb{C}}\\\\setminus P, \\\\infty)$. Of particular interest is the casewhen $P$ is a filled polynomial Julia set and the curvature is proportional tothe measure of maximal entropy.',\n",
              " 'Let $M_d$ be the moduli space of one-dimensional complex polynomial dynamicalsystems. The escape rates of the critical points determine a critical heightsmap $G: M_d \\\\to \\\\mathbb{R}^{d-1}$. For generic values of $G$, each connectedcomponent of a fiber of $G$ is the deformation space for twist deformations onthe basin of infinity. We analyze the quotient space $\\\\mathcal{T}_d^*$ obtainedby collapsing each connected component of a fiber of $G$ to a point. The space$\\\\mathcal{T}_d^*$ is a parameter-space analog of the polynomial tree $T(f)$associated to a polynomial $f:\\\\mathbb{C}\\\\to\\\\mathbb{C}$, studied by DeMarco andMcMullen, and there is a natural projection from $\\\\mathcal{T}_d^*$ to the spaceof trees $\\\\mathcal{T}_d$. We show that the projectivization$\\\\mathbb{P}\\\\mathcal{T}_d^*$ is compact and contractible; further, the shiftlocus in $\\\\mathbb{P}\\\\mathcal{T}_d^*$ has a canonical locally finite simplicialstructure. The top-dimensional simplices are in one-to-one corespondence withtopological conjugacy classes of structurally stable polynomials in the shiftlocus.',\n",
              " 'Let $Rat_d$ denote the space of holomorphic self-maps of ${\\\\bf P}^1$ ofdegree $d\\\\geq 2$, and $\\\\mu_f$ the measure of maximal entropy for $f\\\\in Rat_d$.The map of measures $f\\\\mapsto\\\\mu_f$ is known to be continuous on $Rat_d$, andit is shown here to extend continuously to the boundary of $Rat_d$ in$\\\\bar{Rat}_d \\\\simeq {\\\\bf P}^{2d+1}$, except along a locus $I(d)$ of codimension$d+1$. The set $I(d)$ is also the indeterminacy locus of the iterate map$f\\\\mapsto f^n$ for every $n\\\\geq 2$. The limiting measures are given explicitly,away from $I(d)$. The degenerations of rational maps are also described interms of metrics of non-negative curvature on the Riemann sphere: the limitsare polyhedral.',\n",
              " 'Let $M_2$ be the space of quadratic rational maps $f:{\\\\bf P}^1\\\\to{\\\\bf P}^1$,modulo the action by conjugation of the group of M\\\\\"obius transformations. Inthis paper a compactification $X$ of $M_2$ is defined, as a modification ofMilnor\\'s $\\\\bar{M}_2\\\\iso{\\\\bf CP}^2$, by choosing representatives of a conjugacyclass $[f]\\\\in M_2$ such that the measure of maximal entropy of $f$ hasconformal barycenter at the origin in ${\\\\bf R}^3$, and taking the closure inthe space of probability measures. It is shown that $X$ is the smallestcompactification of $M_2$ such that all iterate maps $[f]\\\\mapsto [f^n]\\\\inM_{2^n}$ extend continuously to $X \\\\to \\\\bar{M}_{2^n}$, where $\\\\bar{M}_d$ is thenatural compactification of $M_d$ coming from geometric invariant theory.',\n",
              " 'We study the projection $\\\\pi: M_d \\\\to B_d$ which sends an affine conjugacyclass of polynomial $f: \\\\mathbb{C}\\\\to\\\\mathbb{C}$ to the holomorphic conjugacyclass of the restriction of $f$ to its basin of infinity. When $B_d$ isequipped with a dynamically natural Gromov-Hausdorff topology, the map $\\\\pi$becomes continuous and a homeomorphism on the shift locus. Our main result isthat all fibers of $\\\\pi$ are connected. Consequently, quasiconformal andtopological basin-of-infinity conjugacy classes are also connected. The keyingredient in the proof is an analysis of model surfaces and model maps,branched covers between translation surfaces which model the local behavior ofa polynomial.',\n",
              " \"We study the postcritically-finite (PCF) maps in the moduli space of complexpolynomials $\\\\mathrm{MP}_d$. For a certain class of rational curves $C$ in$\\\\mathrm{MP}_d$, we characterize the condition that $C$ contains infinitelymany PCF maps. In particular, we show that if $C$ is parameterized bypolynomials, then there are infinitely many PCF maps in $C$ if and only ifthere is exactly one active critical point along $C$, up to symmetries; weprovide the critical orbit relation satisfied by any pair of active criticalpoints. For the curves $\\\\mathrm{Per}_1(\\\\lambda)$ in the space of cubicpolynomials, introduced by Milnor (1992), we show that$\\\\mathrm{Per}_1(\\\\lambda)$ contains infinitely many PCF maps if and only if$\\\\lambda=0$. The proofs involve a combination of number-theoretic methods(specifically, arithmetic equidistribution) and complex-analytic techniques(specifically, univalent function theory). We provide a conjecture aboutZariski density of PCF maps in subvarieties of the space of rational maps, inanalogy with the Andr\\\\'e-Oort Conjecture from arithmetic geometry.\",\n",
              " 'We give a dynamical proof of a result of Masser and Zannier [MZ2, MZ3] abouttorsion points on the Legendre family of elliptic curves. Our methods alsotreat points of small height. A key ingredient is the arithmeticequidistribution theorem on $\\\\mathbb{P}^1$ of Baker-Rumely, Chambert-Loir, andFavre-Rivera-Letelier. Torsion points on the elliptic curve coincide withpreperiodic points for the degree-4 Lattes family of rational functions. Ourmain new results concern properties of the bifurcation measures for this Lattesfamily associated to marked points.',\n",
              " 'In [DKY], it was conjectured that there is a uniform bound $B$, dependingonly on the degree $d$, so that any pair of holomorphic maps $f, g:\\\\mathbb{P}^1\\\\to\\\\mathbb{P}^1$ with degree $d$ will either share all of theirpreperiodic points or have at most $B$ in common. Here we show that thisuniform bound holds for a Zariski open and dense set in the space of all pairs,$\\\\mathrm{Rat}_d \\\\times \\\\mathrm{Rat}_d$, for each degree $d\\\\geq 2$. The proofinvolves a combination of arithmetic intersection theory and complex-dynamicalresults, especially as developed recently by Gauthier-Vigny, Yuan-Zhang, andMavraki-Schmidt. In addition, we present alternate proofs of recent results ofDeMarco-Krieger-Ye and of Poineau. In fact we prove a generalization of aconjecture of Bogomolov-Fu-Tschinkel in a mixed setting of dynamical systemsand elliptic curves.',\n",
              " 'Let a and b be algebraic numbers such that exactly one of a and b is analgebraic integer, and let f_t(z):=z^2+t be a family of polynomialsparametrized by t. We prove that the set of all algebraic numbers t for whichthere exist positive integers m and n such that f_t^m(a)=f_t^n(b) has boundedWeil height. This is a special case of a more general result supporting a newbounded height conjecture in dynamics. Our results fit into the general settingof the principle of unlikely intersections in arithmetic dynamics.',\n",
              " 'We look at degenerating meromorphic families of rational maps on$\\\\mathbb{P}^1$ -- holomorphically parameterized by a punctured disk -- and weprovide examples where the bifurcation current fails to have a boundedpotential in a neighborhood of the puncture. This is in contrast to the recentresult of Favre-Gauthier that we always have continuity across the puncture forfamilies of polynomials; and it provides a counterexample to a conjecture posedby Favre in 2016. We explain why our construction fails for polynomial familiesand for families of rational maps defined over finite extensions of therationals $\\\\mathbb{Q}$.',\n",
              " 'A polynomial skew product of C^2 is a map of the form f(z,w) = (p(z),q(z,w)), where p and q are polynomials, such that f is regular of degree d >=2. For polynomial maps of C, hyperbolicity is equivalent to the condition thatthe closure of the postcritical set is disjoint from the Julia set; further,critical points either iterate to an attracting cycle or infinity. Forpolynomial skew products, Jonsson (Math. Ann., 1999) established that f isAxiom A if and only if the closure of the postcritical set is disjoint from theright analog of the Julia set. Here we present the analogous conclusion:critical orbits either escape to infinity or accumulate on an attracting set.In addition, we construct new examples of Axiom A maps demonstrating variouspostcritical behaviors.',\n",
              " 'In this article, we combine complex-analytic and arithmetic tools to studythe preperiodic points of one-dimensional complex dynamical systems. We showthat for any fixed complex numbers a and b, and any integer d at least 2, theset of complex numbers c for which both a and b are preperiodic for z^d+c isinfinite if and only if a^d = b^d. This provides an affirmative answer to aquestion of Zannier, which itself arose from questions of Masser concerningsimultaneous torsion sections on families of elliptic curves. Using similartechniques, we prove that if two complex rational functions f and g haveinfinitely many preperiodic points in common, then they must have the sameJulia set. This generalizes a theorem of Mimar, who established the same resultassuming that f and g are defined over an algebraic extension of the rationals.The main arithmetic ingredient in the proofs is an adelic equidistributiontheorem for preperiodic points over number fields and function fields, withnon-archimedean Berkovich spaces playing an essential role.',\n",
              " 'We show that the weak limit of the maximal measures for any degeneratingsequence of rational maps on the Riemann sphere must be a countable sum ofatoms. For a 1-parameter family f_t of rational maps, we refine this result byshowing that the measures of maximal entropy have a unique limit on the Riemannsphere as the family degenerates. The family f_t may be viewed as a singlerational function on the Berkovich projective line over the completion of thefield of formal Puiseux series in t, and the limiting measure on the Riemannsphere is the \"residual measure\" associated to the equilibrium measure on theBerkovich line. For the proof, we introduce a new technique for quantizingmeasures on the Berkovich projective line and demonstrate the uniqueness ofsolutions to a quantized version of the pullback formula for the equilibriummeasure there.',\n",
              " 'In this paper, a validation and uncertainty quantification (VUQ) frameworkfor the Eulerian-Eulerian two-fluid-model based multiphase-computational fluiddynamics solver (MCFD) is formulated. The framework aims to answer thequestion: how to evaluate if a MCFD solver adequately represents the underlyingphysics of a multiphase system of interest? The proposed framework is based ontotal data-model integration (TDMI) approach that uses Bayesian method toinversely quantify the uncertainty of the solver predictions with the supportof multiple experimental datasets. The framework consists of six steps withstate-of-the-art statistical methods, including: 1). Solver evaluation and datacollection; 2). Surrogate model construction; 3). Sensitivity Analysis; 4).Parameter selection; 5). Uncertainty quantification with Bayesian inference;and 6). Validation metrics calculation. Those steps are formulated in a modularmanner and using non-intrusive methods. Such features ensure the applicabilityof the flexible framework to different scenarios and modeling of multiphaseflow and boiling heat transfer, as well as the extensibility of the frameworkto support VUQ of different MCFD solvers.',\n",
              " 'Current system thermal-hydraulic codes have limited credibility in simulatingreal plant conditions, especially when the geometry and boundary conditions areextrapolated beyond the range of test facilities. This paper proposes adata-driven approach, Feature Similarity Measurement FFSM), to establish atechnical basis to overcome these difficulties by exploring local patternsusing machine learning. The underlying local patterns in multiscale data arerepresented by a set of physical features that embody the information from aphysical system of interest, empirical correlations, and the effect of meshsize. After performing a limited number of high-fidelity numerical simulationsand a sufficient amount of fast-running coarse-mesh simulations, an errordatabase is built, and deep learning is applied to construct and explore therelationship between the local physical features and simulation errors. Casestudies based on mixed convection have been designed for demonstrating thecapability of data-driven models in bridging global scale gaps.',\n",
              " 'To realize efficient computational fluid dynamics (CFD) prediction oftwo-phase flow, a multi-scale framework was proposed in this paper by applyinga physics-guided data-driven approach. Instrumental to this framework, FeatureSimilarity Measurement (FSM) technique was developed for error estimation intwo-phase flow simulation using coarse-mesh CFD, to achieve a comparableaccuracy as fine-mesh simulations with fast-running feature. By definingphysics-guided parameters and variable gradients as physical features, FSM hasthe capability to capture the underlying local patterns in the coarse-mesh CFDsimulation. Massive low-fidelity data and respective high-fidelity data areused to explore the underlying information relevant to the main simulationerrors and the effects of phenomenological scaling. By learning from previoussimulation data, a surrogate model using deep feedforward neural network (DFNN)can be developed and trained to estimate the simulation error of coarse-meshCFD. The research documented supports the feasibility of the physics-guideddeep learning methods for coarse mesh CFD simulations which has a potential forthe efficient industrial design.',\n",
              " 'This paper is a contribution to the theoretical foundations of strategies. Wefirst present a general definition of abstract strategies which is extensionalin the sense that a strategy is defined explicitly as a set of derivations ofan abstract reduction system. We then move to a more intensional definitionsupporting the abstract view but more operational in the sense that itdescribes a means for determining such a set. We characterize the class ofextensional strategies that can be defined intensionally. We also give somehints towards a logical characterization of intensional strategies and proposea few challenging perspectives.',\n",
              " 'Some of the most problematic issues that limit the implementation ofapplications on Noisy Intermediate Scale Quantum (NISQ) machines are theadverse impacts of both incoherent and coherent errors. We conducted anin-depth study of coherent errors on a quantum hardware platform using atransverse field Ising model Hamiltonian as a sample user application. Wereport here on the results from these computations using several errormitigation protocols that profile these errors and provide an indication of thehardware qubit stability. Through a detailed set of measurements we identifyinter-day and intra-day qubit calibration drift and the impacts of quantumcircuit placement on groups of qubits in different physical locations on theprocessor. This paper also discusses how these measurements can provide abetter understanding of these types of errors and how they may improve effortsto validate the accuracy of quantum computations.',\n",
              " 'Real-time scattering calculations on a Noisy Intermediate Scale Quantum(NISQ) quantum computer are disrupted by errors that accumulate throughout thecircuits. To improve the accuracy of such physics simulations, one cansupplement the application circuits with a recent error mitigation strategyknown as Noisy Output eXtrapolation (NOX). We tested these error mitigationprotocols on a Transverse Field Ising model and improved upon previouscalculations of the phase shift. Our proof-of-concept 4-qubit applicationcircuits were run on several IBM quantum computing hardware architectures.Metrics were introduced that show between 21\\\\% and 74\\\\% error reduction forcircuit depths ranging from 14 to 37 hard cycles, confirming that the NOXtechnique applies to circuits with a broad range of failure rates. Thisobservation on different cloud-accessible devices further confirms that NOXprovides performance improvements even in the advent where circuits areexecuted in substantially time-separated batches. Finally, we provide aheuristic method to obtain systematic error bars on the mitigated results,compare them with empirical errors and discuss their effects on phase shiftestimates.',\n",
              " \"Mitigation and calibration schemes are central to maximize the computationalreach of today's Noisy Intermediate Scale Quantum (NISQ) hardware, but theseschemes are often specialized to exclusively address either coherent ordecoherent error sources. Quantifying the two types of errors hence constitutesa desirable feature when it comes to benchmarking error suppression tools. Inthis paper, we present a scalable and cycle-centric methodology for obtaining adetailed estimate of the coherent contribution to the error profile of acomputing cycle. The protocol that we suggest is based on Cycle ErrorReconstruction (CER), also known as K-body Noise Reconstruction (KNR). Thisprotocol is similar to Cycle Benchmarking (CB) in that it provides acycle-centric diagnostic based on Pauli fidelity estimation. We introduce anadditional hyper-parameter in CER by allowing the hard cycles to be foldedmultiple times before being subject to Pauli twirling. Performing CER fordifferent values of our added hyper-parameter allows estimating the coherenterror contributions through a generalization of the fidelity decay formula. Weconfirm the accuracy of our method through numerical simulations on a quantumsimulator, and perform proof-of-concept experiments on three IBM chips, namelyguadalupe, manila and montreal. In all three experiments, we measuresubstantial coherent errors biased in $Z$.\",\n",
              " 'This paper investigates the application of quantum computing technology toairline gate-scheduling quadratic assignment problems (QAP). We explore thequantum computing hardware architecture and software environment required forporting classical versions of these type of problems to quantum computers. Wediscuss the variational quantum eigensolver and the inclusion ofspace-efficient graph coloring to the Quadratic Unconstrained BinaryOptimization (QUBO). These enhanced quantum computing algorithms are testedwith an 8 gate and 24 flight test case using both the IBM quantum computingsimulator and a 27 qubit superconducting transmon IBM quantum computinghardware platform.',\n",
              " \"We discuss recent progress in Tensor Lattice Field Theory and economical,symmetry preserving, truncations suitable for quantum computations orsimulations. We focus on spin and gauge models with continuous Abeliansymmetries such as the Abelian Higgs model and emphasize noise-robustimplementations of Gauss's law. We discuss recent progress concerning thecomparison between field digitizations and character expansions, symmetrybreaking in tensor language, wave-packet preparation and possible newimplementations of Abelian models using Rydberg atoms.\",\n",
              " 'The traditional approach for studying the physics of the strong interactionsemploys a basic computational construct originally proposed by Wilson in the1970s. Over the years additional enhancements have been added to thisformulation to improve computational performance and accuracy. This formulationhas been successfully implemented on high performance computing systems and hasyielded accurate calculations for many static properties of the stronginteractions (such as the hadron mass spectrum). With the recent advances inquantum computing, the question that is now being asked is whether anequivalent type of gauge invariant formulation of a field theory can beconstructed on a quantum computer to calculate dynamical processes that cannotbe simulated on a traditional supercomputer. Using the Quantum Link Model (QLM)plus the concept of rishons, this paper will specifically focus on thechallenges implementing a basic gauge link lattice construct using SU(2)non-Abelian links for illustration. The paper will also discuss the physicsthat may potentially be simulated on a quantum computer with this construct andspeculate on the prospects for having quantum computers become a part of theset of hardware platforms for lattice gauge theory simulations in the future.',\n",
              " \"Quantum computers open the possibility of performing real-time calculationsfor quantum field theory scattering processes. We propose to use an indexaveraging the absolute value of the difference between the accuratelycalculated Trotter evolution of site occupations and their actual measurementson NISQ machines. The average is over all the qubits for a certain number ofTrotter steps. We use this metric to quantify the progress made in successivestate-of-the-art machines and error-mitigation techniques. We illustrate theconcept with the transverse Ising model in one spatial dimension with foursites using three of IBM's quantum computers (Almaden, Boeblingen, andMelbourne). We discuss the size of the Trotter steps needed to achieve physicsgoals. Using the proposed metric, we show that readout mitigation methods andRichardson extrapolations of mitigated measurements are very effective forspecific numbers of Trotter steps of a chosen size. This specific choice can beapplied to other machines and noise mitigation methods. On the other hand, areliable algorithmic mitigation would require a significantly larger number ofsmaller Trotter steps.\",\n",
              " \"Quantum phase estimation (QPE) is one of the core algorithms for quantumcomputing. It has been extensively studied and applied in a variety of quantumapplications such as the Shor's factoring algorithm, quantum samplingalgorithms and the calculation of the eigenvalues of unitary matrices. The QPEalgorithm has been combined with Kitaev's algorithm and the inverse quantumFourier transform (IQFT) which are utilized as a fundamental component of suchquantum algorithms. In this paper, we explore the computational challenges ofimplementing QPE algorithms on noisy intermediate-scale quantum (NISQ) machinesusing the IBM Q Experience (e.g., the IBMQX4, 5-qubit quantum computinghardware platform). Our experimental results indicate that the accuracy offinding the phase using these QPE algorithms is severely constrained by theNISQ computer's physical characteristics such as coherence time and errorrates. To mitigate these physical limitations, we propose implementing amodified solution by reducing the number of controlled rotation gates and phaseshift operations, thereby increasing the accuracy of the finding phase innear-term quantum computers.\",\n",
              " 'We present a method to extract the phase shift of a scattering process usingthe real-time evolution in the early and intermediate stages of the collisionin order to estimate the time delay of a wave packet. This procedure isconvenient when using noisy quantum computers for which the asymptoticout-state behavior is unreachable. We demonstrate that the challenging Fouriertransforms involved in the state preparation and measurements can beimplemented in $1+1$ dimensions with current trapped ion devices and IBMquantum computers. We compare quantum computation of the time delays obtainedin the one-particle quantum mechanics limit and the scalable quantum fieldtheory formulation with accurate numerical results. We discuss the finitevolume effects in the Wigner formula connecting time delays to phase shifts.The results reported involve two- and four-qubit calculations, and we discussthe possibility of larger scale computations in the near future.',\n",
              " 'The rise of big data systems has created a need for benchmarks to measure andcompare the capabilities of these systems. Big data benchmarks present uniquescalability challenges. The supercomputing community has wrestled with thesechallenges for decades and developed methodologies for creating rigorousscalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipelinebenchmark employs supercomputing benchmarking methodologies to create ascalable benchmark that is reflective of many real-world big data processingsystems. The PageRank pipeline benchmark builds on existing prior scalablebenchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark withmultiple integrated kernels that can be run together or independently. Eachkernel is well defined mathematically and can be implemented in any programmingenvironment. The linear algebraic nature of PageRank makes it well suited tobeing implemented using the GraphBLAS standard. The computations are simpleenough that performance predictions can be made based on simple computinghardware models. The surrounding kernels provide the context for each kernelthat allows rigorous definition of both the input and the output for eachkernel. Furthermore, since the proposed PageRank pipeline benchmark is scalablein both problem size and hardware, it can be used to measure and quantitativelycompare a wide range of present day and future systems. Serial implementationsin C++, Python, Python with Pandas, Matlab, Octave, and Julia have beenimplemented and their single threaded performance has been measured.',\n",
              " 'The design of a good algorithm to solve NP-hard combinatorial approximationproblems requires specific domain knowledge about the problems and often needsa trial-and-error problem solving approach. Graph coloring is one of theessential fields to provide an efficient solution for combinatorialapplications such as flight scheduling, frequency allocation in networking, andregister allocation. In particular, some optimization algorithms have beenproposed to solve the multi-coloring graph problems but most of the cases asimple searching method would be the best approach to find an optimal solutionfor graph coloring problems. However, this naive approach can increase thecomputation cost exponentially as the graph size and the number of colorsincrease. To mitigate such intolerable overhead, we investigate the methods totake the advantages of quantum computing properties to find a solution formulti-coloring graph problems in polynomial time. We utilize the variationalquantum eigensolver (VQE) technique and quantum approximate optimizationalgorithm (QAOA) to find solutions for three combinatorial applications by bothtransferring each problem model to the corresponding Ising model and by usingthe calculated Hamiltonian matrices. Our results demonstrate that VQE and QAOAalgorithms can find one of the best solutions for each application. Therefore,our modeling approach with hybrid quantum algorithms can be applicable forcombinatorial problems in various fields to find an optimal solution inpolynomial time.',\n",
              " 'We consider the lifetime of quasi-particles in a d-wave superconductor due toscattering from antiferromagnetic spin-fluctuations, and explicitly separatethe contribution from Umklapp processes which determines the electricalconductivity. Results for the temperature dependence of the total scatteringrate and the Umklapp scattering rate are compared with relaxation ratesobtained from thermal and microwave conductivity measurements, respectively.',\n",
              " 'The Hubbard Hamiltonian on a two-leg ladder is studied numerically usingquantum Monte Carlo and Exact Diagonalization techniques. A rung interaction,$V$, is turned on such that the resulting model has an exact SO(5) symmetrywhen $V=-U$. The evolution of the low energy excitation spectrum is presentedfrom the pure Hubbard ladder to the SO(5) ladder. It is shown that the lowenergy excitations in the pure Hubbard ladder have an approximate SO(5)symmetry.',\n",
              " \"We present mean-field and quantum Monte Carlo results that suggest theexistence of an itinerant antiferromagnetic ground state in the half-filled$U-t-t'$ model in two dimensions. In particular, working at $t'/t=-0.2$ wefound that antiferromagnetic long range order develops at $U_{c_1}/t\\\\approx2.5$, while a study of the density of states $N(\\\\omega)$ and the response to anexternal magnetic field indicates that the system becomes insulating at alarger coupling $4<U_{c_2}/t<6$.\",\n",
              " 'Quantum Monte Carlo results for the specific heat c of the two dimensionalHubbard model are presented. At half-filling it was observed that $c \\\\sim T^2$at very low temperatures. Two distinct features were also identified: a lowtemperature peak related to the spin degrees of freedom and a highertemperature broad peak related to the charge degrees of freedom. Away fromhalf-filling the spin induced feature slowly disappears as a function of holedoping while the charge feature moves to lower temperature. A comparison withexperimental results for the high temperature cuprates is discussed.',\n",
              " 'We applied the Recurrent Variational Approach to the two-leg Hubbard ladder.At half-filling, our variational Ansatz was a generalization of the resonatingvalence bond state. At finite doping, hole pairs were allowed to move in theresonating valence bond background. The results obtained by the RecurrentVariational Approach were compared with results from Density MatrixRenormalization Group.',\n",
              " 'Liquid crystal elastomer films that morph into cones are strikingly capablelifters. Thus motivated, we combine theory, numerics, and experiments toreexamine the load-bearing capacity of conical shells. We show that a conesquashed between frictionless surfaces buckles at a smaller load, even inscaling, than the classical Seide/Koiter result. Such buckling begins in aregion of greatly amplified azimuthal compression generated in an outerboundary layer with oscillatory bend. Experimentally and numerically, bucklingthen grows sub-critically over the full cone. We derive a new thin-limitformula for the critical load, $\\\\propto t^{5/2}$, and validate it numerically.We also investigate deep post-buckling, finding further instabilities producingintricate states with multiple Pogorelov-type curved ridges arranged inconcentric-circles or Archimedean spirals. Finally, we investigate the forcesexerted by such states, which limit lifting performance in active cones.',\n",
              " 'The electronic momentum distribution ${\\\\rm n({\\\\bf k})}$ of the twodimensional Hubbard model is studied for different values of the coupling ${\\\\rmU/t}$, electronic density ${\\\\rm \\\\langle n \\\\rangle}$, and temperature, usingquantum Monte Carlo techniques. A detailed analysis of the data on $8\\\\times 8$clusters shows that features consistent with hole pockets at momenta ${\\\\rm {\\\\bfk}=(\\\\pm {\\\\pi\\\\over{2}},\\\\pm {\\\\pi\\\\over{2}})}$ appear as the system is doped awayfrom half-filling. Our results are consistent with recent experimental data forthe cuprates discussed by Aebi et al. (Phys. Rev. Lett. {\\\\bf 72}, 2757 (1994)).In the range of couplings studied, the depth of the pockets is maximum at ${\\\\rm\\\\langle n \\\\rangle \\\\approx 0.9}$, and it increases with decreasing temperature.The apparent absence of hole pockets in previous numerical studies of thismodel is explained.',\n",
              " 'Flat sheets encoded with patterns of contraction/elongation morph into curvedsurfaces. If the surfaces bear Gauss curvature, the resulting actuation can bestrong and powerful. We deploy the Gauss-Bonnet theorem to deduce the Gausscurvature encoded in a pattern of uniform-magnitude contraction/elongation withspatially varying direction, as is commonly implemented in patterned liquidcrystal elastomers. This approach reveals two fundamentally distinctcontributions: a structural curvature which depends on the precise form of thepattern, and a topological curvature generated by defects in the contractiledirection. These curvatures grow as different functions thecontraction/elongation magnitude, explaining the apparent contradiction betweenprevious calculations for simple +1 defects, and smooth defect-free patterns.We verify these structural and topological contributions by conductingnumerical shell calculations on sheets encoded with simple higher-ordercontractile defects to reveal their activated morphology. Finally we calculatethe Gauss curvature generated by patterns with spatially varying magnitude anddirection, which leads to additional magnitude gradient contributions to thestructural term. We anticipate this form will be useful whenever magnitude anddirection are natural variables, including in describing the contraction of amuscle along its patterned fiber direction, or a tissue growing by elongatingits cells.',\n",
              " 'Liquid crystalline elastomers (LCEs) are shape-changing materials thatexhibit large deformations in response to applied stimuli. Local control of theorientation of LCEs spatially directs the deformation of these materials torealize spontaneous shape change in response to stimuli. Prior approaches toshape programming in LCEs utilize patterning techniques that involve thedetailed inscription of spatially varying nematic fields to produce sheets.These patterned sheets deform into elaborate geometries with complex Gaussiancurvatures. Here, we present an alternative approach to realize shape-morphingin LCEs where spatial patterning of the crosslink density locally regulates thematerial deformation magnitude on either side of a prescribed interface curve.We also present a simple mathematical model describing the behavior of thesematerials. Further experiments coupled with the mathematical model demonstratethe control of the sign of Gaussian curvature, which is used in combinationwith heat transfer effects to design LCEs that self-clean as a result oftemperature-dependent actuation properties.',\n",
              " \"Comparing experimental data for high temperature cuprate superconductors withnumerical results for electronic models, it is becoming apparent that a hoppingalong the plaquette diagonals has to be included to obtain a quantitativeagreement. According to recent estimations the value of the diagonal hopping$t'$ appears to be material dependent. However, the values for $t'$ discussedin the literature were obtained comparing theoretical results in the weakcoupling limit with experimental photoemission data and band structurecalculations. The goal of this paper is to study how $t'$ gets renormalized asthe interaction between electrons, $U$, increases. For this purpose, the effectof adding a bare diagonal hopping $t'$ to the fully interacting two dimensionalHubbard model Hamiltonian is investigated using numerical techniques. Positiveand negative values of $t'$ are analyzed. Spin-spin correlations, $n(\\\\bf{k})$,$\\\\langle n\\\\rangle$ vs $\\\\mu$, and local magnetic moments are studied for valuesof $U/t$ ranging from 0 to 6, and as a function of the electronic density. Theinfluence of the diagonal hopping in the spectral function $A(\\\\bf{k},\\\\omega)$is also discussed, and the changes in the gap present in the density of statesat half-filling are studied. We introduce a new criterion to determine probablelocations of Fermi surfaces at zero temperature from $n(\\\\bf{k})$ data obtainedat finite temperature. It appears that hole pockets at ${\\\\bf{k}}=(\\\\pi/2,\\\\pi/2)$may be induced for negative $t'$ while a positive $t'$ produces similarfeatures at ${\\\\bf{k}}=(\\\\pi,0)$ and $(0,\\\\pi)$. Comparisons with the standard 2DHubbard ($t'=0$) model indicate that a negative $t'$ hopping amplitude appearsto be dynamically generated. In general, we conclude that it is very dangerousto extract a bare parameter of the Hamiltonian $(t')$ from PES data where\",\n",
              " \"Quantum Monte Carlo (QMC) and Maximum Entropy (ME) techniques are used tostudy the spectral function $A({\\\\bf p},\\\\omega)$ of the one band Hubbard modelin strong coupling including a next-nearest-neighbor electronic hopping withamplitude $t'/t= -0.35$. These values of parameters are chosen to improve thecomparison of the Hubbard model with angle-resolved photoemission (ARPES) datafor $Sr_2 Cu O_2 Cl_2$. A narrow quasiparticle (q.p.) band is observed in theQMC analysis at the temperature of the simulation $T=t/3$, both at and awayfrom half-filling. Such a narrow band produces a large accumulation of weightin the density of states at the top of the valence band. As the electronicdensity $< n >$ decreases further away from half-filling, the chemicalpotential travels through this energy window with a large number of states, andby $< n > \\\\sim 0.70$ it has crossed it entirely. The region near momentum$(0,\\\\pi)$ and $(\\\\pi,0)$ in the spectral function is more sensitive to dopingthan momenta along the diagonal from $(0,0)$ to $(\\\\pi,\\\\pi)$. The evolution withhole density of the quasiparticle dispersion contains some of the featuresobserved in recent ARPES data in the underdoped regime. For sufficiently largehole densities the ``flat'' bands at $(\\\\pi,0)$ cross the Fermi energy, aprediction that could be tested with ARPES techniques applied to overdopedcuprates. The population of the q.p. band introduces a {\\\\it hidden} density inthe system which produces interesting consequences when the quasiparticles areassumed to interact through antiferromagnetic fluctuations and studied with theBCS gap equation formalism. In particular, a region of extended s-wave is foundto compete with d-wave in the overdoped regime, i.e. when the chemicalpotential has almost entirely crossed the q.p.\",\n",
              " \"Two-sided manufacturing-as-a-service (MaaS) marketplaces connect clientsrequesting manufacturing services to suppliers providing those services.Matching mechanisms i.e. allocation of clients' orders to suppliers is a keydesign parameter of the marketplace platform. The platform might perform anallocation to maximize its revenue or optimize for social welfare of allparticipants. However, individual participants might not get maximum value fromtheir match and reject it to form matches (called blocking groups) themselves,thereby bypassing the platform. This paper considers the bipartite matchingproblem in MaaS marketplaces in a dynamic environment and proposesapproximately stable matching solutions using mechanism design and mathematicalprogramming approaches to limit the formation of blocking groups. Matching isbased on non-strict, incomplete and interdependent preferences of participantsover contracts enabling negotiations between both sides. Empirical simulationsare used to test the mechanisms in a simulated 3D printing marketplace and toevaluate the impact of stability on its performance. It is found that stablematching results in small degradation in social welfare of the marketplace.However, it leads to a significantly better outcome in terms of stability ofallocation. Unstable matchings introduce anarchy into marketplace withparticipants rejecting its allocation leading to performance poorer than stablematchings.\",\n",
              " 'In school choice problems, the motivation for students\\' welfare (efficiency)is restrained by concerns to respect schools\\' priorities (fairness). Even thebest matching in terms of welfare among all fair matchings (SOSM) is in generalinefficient. Moreover, any mechanism that improves welfare over the SOSM ismanipulable by the students. First, we characterize the \"least manipulable\"mechanisms in this class: upper-manipulation-proofness ensures that no studentis better off through strategic manipulation over the objects that are betterthan their assigned school. Second, we use the notion that a matching is lessunfair if it yields a smaller set of students whose priorities are violated,and define minimal unfairness accordingly. We then show that the EfficiencyAdjusted Deferred Acceptance (EADA) mechanism is minimally unfair in the classof efficient and upper-manipulation-proof mechanisms. When the objective is toimprove students\\' welfare over the SOSM, this characterization implies animportant insight on the frontier of the main axioms in school choice.',\n",
              " 'National radio dynamic zone (NRDZs) are intended to be geographically boundedareas within which controlled experiments can be carried out while protectingthe nearby licensed users of the spectrum. An NRDZ will facilitate research anddevelopment of new spectrum technologies, waveforms, and protocols, in typicaloutdoor operational environments of such technologies. In this paper, weintroduce and describe an NRDZ concept that relies on a combination ofautonomous aerial and ground sensor nodes for spectrum sensing and radioenvironment monitoring (REM). We elaborate on key characteristics and featuresof an NRDZ to enable advanced wireless experimentation while also coexistingwith licensed users. Some preliminary results based on simulation andexperimental evaluations are also provided on out-of-zone leakage monitoringand real-time REMs.',\n",
              " 'Peer-to-peer (P2P) botnets use decentralized command and control networksthat make them resilient to disruptions. The P2P botnet overlay networksmanifest structures in mutual-contact graphs, also called communication graphs,formed using network traffic information. It has been shown that thesestructures can be detected using community detection techniques from graphtheory. These previous works, however, treat the communication graphs and theP2P botnet structures as static. In reality, communication graphs are dynamicas they represent the continuously changing network traffic flows. Similarly,the P2P botnets also evolve with time, as new bots join and existing bots leaveeither temporarily or permanently. In this paper we address the problem ofdetecting such evolving P2P botnet communities in dynamic communication graphs.We propose a reinforcement-based approach, suitable for large communicationgraphs, that improves precision and recall of P2P botnet community detection indynamic communication graphs.',\n",
              " 'With its promise of increasing softwarization, improving disaggregability,and creating an open-source based ecosystem in the area of Radio AccessNetworks, the idea of Open RAN has generated rising interest in the community.Even as the community races to provide and verify complete Open RAN systems,the importance of verification of systems based on Open RAN under real-worldconditions has become clear, and testbed facilities for general use have beenenvisioned, in addition to private testing facilities. Aerial robots, includingautonomous ones, are among the increasingly important and interesting clientsof RAN systems, but also present a challenge for testbeds. Based on ourexperience in architecting and operating an advanced wireless testbed withaerial robots as a primary citizen, we present considerations relevant to thedesign of Open RAN testbeds, with particular attention to making such a testbedcapable of controlled experimentation with aerial clients. We also presentrepresentative results from the NSF AERPAW testbed on Open RAN slicing,programmable vehicles, and programmable radios.',\n",
              " \"In this paper, we report experimental results in collectting and processing5G NR I/Q samples in the 3.7~GHz C-band by using software-defined radio(SDR)-mounted helikite. We use MATLAB's 5G toolbox to post-process thecollected data, to obtain the synchronization signal block (SSB) from the I/Qsamples and then go through the cell search, synchronization procedures, andreference signal received power (RSRP) and reference signal received quality(RSRQ) calculation. We plot these performance metrics for various physical cellidentities as a function of the helikite's altitude. Furthermore, building onour experience with the collected and post-processed data, we discuss potentialvulnerabilities of 5G NR systems to surveillance, jamming attacks, and postquantum era attacks.\",\n",
              " \"In this paper, we introduce AERIQ: a software-defined radio (SDR) based I/Qmeasurement and analysis framework for wireless signals for aerialexperimentation. AERIQ is integrated into controllable aerial vehicles, it isflexible, repeatable, and provides raw I/Q samples for post-processing the datato extract various key parameters of interest (KPIs) over a 3D volume. UsingSDRs, we collect I/Q data with unmanned aerial vehicles (UAVs) flying atvarious altitudes in a radio dynamic zone (RDZ) like outdoor environment, froma 4G LTE eNB that we configure to operate at 3.51 GHz. Using the raw I/Qsamples, and using Matlab's LTE Toolbox, we provide a step-by-step descriptionfor frequency offset estimation/correction, synchronization, cell search,channel estimation, and reference signal received power (RSRP). We providevarious representative results for each step, such as RSRP measurements andcorresponding analytical approximation at different UAV altitudes, coherencebandwidth and coherence time of the channel at different UAV altitudes and linkdistances, and kriging based 3D RSRP interpolation. The collected raw data aswell as the software developed for obtaining and post-processing such data areprovided publicly for potential use by other researchers. AERIQ is alsoavailable in emulation and testbed environments for external researchers toaccess and use as part of the NSF AERPAW platform at NC State University.\",\n",
              " \"Recently, unmanned aerial vehicles (UAVs) have been receiving significantattention due to their wide range of potential application areas. To supportUAV use cases with beyond visual line of sight (BVLOS) and autonomous flights,cellular networks can serve as ground connectivity points, and they can provideremote control and payload communication for UAV links. However, there arelimited data sets to study the coverage of cellular technologies for UAVflights at different altitudes and develop machine learning (ML) techniques forimproving UAV communication and navigation. In this article, we present raw LTEI/Q sample data sets from physical field experiments in the Lake Wheeler farmarea of the NSF AERPAW experimentation platform. We fly a UAV that carries asoftware-defined radio (SDR) at altitudes ranging from 30~m to 110~m andcollect raw I/Q samples from an SDR-based LTE base station on the groundoperating at 3.51 GHz. We adopt a standard metadata format for reproducing theresults from the collected data sets. The post-processing of raw I/Q samplesusing MATLAB's 4G LTE toolbox is described and various representative resultsare provided. In the end, we discuss the possible ways that our provided dataset, post-processing sample code, and sample experiment code for collecting I/Qmeasurements and vehicle control can be used by other ML researchers in thefuture.\",\n",
              " 'In a well-dispersed nanofluid with strong cluster-fluid attraction, thermalconduction paths can arise through percolating amorphous-like interfacialstructures. This results in a thermal conductivity enhancement beyond theMaxwell limit of 3*phi, with phi being the nanoparticle volume fraction. Ourfindings from non-equilibrium molecular dynamics simulations, which areamenable to experimental verification, can provide a theoretical basis for thedevelopment of future nanofluids.',\n",
              " 'Several new mechanisms have been hypothesized in the recent years tocharacterize the thermal conduction behavior in nanofluids. In this paper, weshow that a large set of nanofluid thermal conductivity data is enveloped bythe well-known Hashin and Shtrikman (HS) mean-field bounds for inhomogeneoussystems. The thermal conductivity in nanofluids, therefore, is largelydependent on whether the nanoparticles stays dispersed in the base fluid, formlinear chain-like configurations, or assume an intermediate configuration. Theexperimental data, which is strikingly analogous to those in most solidcomposites and liquid mixtures, provides a strong evidence for the classicalnature of thermal conduction in nanofluids.',\n",
              " 'This article describes the ARM Scalable Vector Extension (SVE). Several goalsguided the design of the architecture. First was the need to extend the vectorprocessing capability associated with the ARM AArch64 execution state to betteraddress the computational requirements in domains such as high-performancecomputing, data analytics, computer vision, and machine learning. Second wasthe desire to introduce an extension that can scale across multipleimplementations, both now and into the future, allowing CPU designers to choosethe vector length most suitable for their power, performance, and area targets.Finally, the architecture should avoid imposing a software development cost asthe vector length changes and where possible reduce it by improving the reachof compiler auto-vectorization technologies. SVE achieves these goals. Itallows implementations to choose a vector register length between 128 and 2,048bits. It supports a vector-length agnostic programming model that lets code runand scale automatically across all vector lengths without recompilation.Finally, it introduces several innovative features that begin to overcome someof the traditional barriers to autovectorization.',\n",
              " 'In this work, we demonstrate a framework for developing closure models inturbulent combustion using experimental multi-scalar measurements. Theframework is based on the construction of conditional means and joint scalarPDFs from experimental data based on the parameterization of composition spaceusing principal component analysis (PCA). The resulting principal components(PCs) act as both conditioning variables and transport variables. Theirchemical source terms are constructed starting from instantaneous temperatureand species measurements using a variant of the pairwise mixing stirred reactor(PMSR) approach. A multi-dimensional kernel density estimation (KDE) approachis used to construct the joint PDFs in PC space. Convolutions of these jointPDFs with conditional means are used to determine the unconditional means forthe closure terms: the mean PCs chemical source terms and the density. Thesemeans are parameterized in terms of the mean PCs using artificial neuralnetworks (ANN). The framework is demonstrated a posteriori using the data fromthe Sandia piloted turbulent jet flames D, E and F by performing RANScalculations. The radial profiles of mean and RMS of temperature and measuredspecies mass fractions agree well with the experimental means for these flames.',\n",
              " \"Experimental multi-scalar measurements in laboratory flames have providedimportant databases for the validation of turbulent combustion closure models.In this work, we present a framework for data-based closure in turbulentcombustion and establish an a priori validation of this framework. The approachis based on the construction of joint probability density functions (PDFs) andconditional statistics using experimental data based on the parameterization ofthe composition space using principal component analysis (PCA). The PCA on thedata identifies key parameters, principal components (PCs), on which jointscalar PDFs and conditional scalar means can be constructed. To enable ageneric implementation for the construction of joint scalar PDFs, we use themulti-dimensional kernel density estimation (KDE) approach. An a priorivalidation of the PCA-KDE approach is carried out using the Sandia piloted jetturbulent flames D, E and F. The analysis of the results suggests that a fewPCs are adequate to reproduce the statistics, resulting in a low-dimensionalrepresentation of the joint scalars PDFs and the scalars' conditional means. Areconstruction of the scalars' means and RMS statistics are in agreement withthe corresponding statistics extracted directly from the experimental data. Wealso propose one strategy to recover missing species and construct conditionalmeans for the reaction rates based on a variation of the pairwise-mixingstirred reactor (PMSR) model. The model is demonstrated using numericalsimulations based on the one-dimensional turbulence (ODT) model for the sameflames.\",\n",
              " 'A combustion chemistry acceleration scheme is developed based on deepoperator networks (DeepONets). The scheme is based on the identification ofcombustion reaction dynamics through a modified DeepOnet architecture such thatthe solutions of thermochemical scalars are projected to new solutions in smalland flexible time increments. The approach is designed to efficiently implementchemistry acceleration without the need for computationally expensiveintegration of stiff chemistry. An additional framework of latent-spacedynamics identification with modified DeepOnet is also proposed which enhancesthe computational efficiency and widens the applicability of the proposedscheme. The scheme is demonstrated on simple chemical kinetics of hydrogenoxidation to more complex chemical kinetics of n-dodecane high- andlow-temperature oxidations. The proposed framework accurately learns thechemical kinetics and efficiently reproduces species and temperature temporalprofiles corresponding to each application. In addition, a very large speed-upwith a great extrapolation capability is also observed with the proposedscheme.',\n",
              " 'Probability density function (PDF) based turbulent combustion modelling islimited by the need to store multi-dimensional PDF tables that can take uplarge amounts of memory. A significant saving in storage can be achieved byusing various machine-learning techniques that represent the thermo-chemicalquantities of a PDF table using mathematical functions. These functions can becomputationally more expensive than the existing interpolation methods used forthermo-chemical quantities. More importantly, the training time can amount to aconsiderable portion of the simulation time. In this work, we address theseissues by introducing an adaptive training algorithm that relies on multi-layerperception (MLP) neural networks for regression and self-organizing maps (SOMs)for clustering data to tabulate using different networks. The algorithm isdesigned to address both the multi-dimensionality of the PDF table as well asthe computational efficiency of the proposed algorithm. SOM clustering dividesthe PDF table into several parts based on similarities in data. Each cluster ofdata is trained using an MLP algorithm on simple network architectures togenerate local functions for thermo-chemical quantities. The algorithm isvalidated for the so-called DLR-A turbulent jet diffusion flame using both RANSand LES simulations and the results of the PDF tabulation are compared to thestandard linear interpolation method. The comparison yields a very goodagreement between the two tabulation techniques and establishes the MLP-SOMapproach as a viable method for PDF tabulation.',\n",
              " 'Joint probability density function (PDF)-based models in turbulent combustionprovide direct closure for turbulence-chemistry interactions. The joint PDFscapture the turbulent flame dynamics at different spatial locations and henceit is crucial to represent them accurately. The jointPDFs are parameterized onthe unconditional means of thermo-chemical state variables, which can be highdimensional. Thus, accurate construction of joint PDFs at various spatiallocations may require an exorbitant amount of data. In a previous work, weintroduced a framework that alleviated data requirements by constructing jointPDFs in a lower dimensional space using principal component analysis (PCA) inconjunction with Kernel Density Estimation (KDE). However, constructing theprincipal component (PC) joint PDFs is still computationally expensive as theyare required to be calculated at each spatial location in the turbulent flame.In this work, we propose the concept of a generalized joint PDF model using theDeep Operator Network (DeepONet). The DeepONet is a machine learning model thatis parameterized on the unconditional means of PCs at a given spatial locationand discrete PC coordinates and predicts the joint probability density valuefor the corresponding PC coordinate. We demonstrate the accuracy andgeneralizability of the DeepONet on the Sandia flames, D, E and F. The DeepONetis trained based on the PC joint PDFs observed inflame E and yields excellentpredictions of joint PDFs shapes at different spatial locations of flamesD andF, which are not seen during training',\n",
              " 'The convergence between effective medium theory and pore-network modelling isexamined. Electrical conductance on two and three-dimensional cubic resistornetworks is used as an example of transport through composite materials orporous media. Effective conductance values are calculated for the networksusing effective medium theory and pore-network models. The convergence betweenthese values is analyzed as a function of network size. Effective medium theoryresults are calculated analytically and numerically. Pore-network results arecalculated numerically using Monte Carlo sampling. The reduced standarddeviations of the Monte Carlo sampled pore-network results are examined as afunction of network size. Finally, a \"quasi-two-dimensional\" network isinvestigated to demonstrate the limitations of effective medium theory whenapplied to thin porous media. Power law fits are made to these data to developsimple models governing convergence. These can be used as a guide for futureresearch that uses both effective medium theory and pore-network models.',\n",
              " \"In order to limit the damage of malware on Mac OS X and iOS, Apple usessandboxing, a kernel-level security layer that provides tight constraints forsystem calls. Particularly used for Apple iOS, sandboxing prevents apps fromexecuting potentially dangerous actions, by defining rules in a sandboxprofile. Investigating Apple's built-in sandbox profiles is difficult as theyare compiled and stored in binary format. We present SandBlaster, a softwarebundle that is able to reverse/decompile Apple binary sandbox profiles to theiroriginal human readable SBPL (SandBox Profile Language) format. We useSandBlaster to reverse all built-in Apple iOS binary sandbox profiles for iOS7, 8 and 9. Our tool is, to the best of our knowledge, the first to provide afull reversing of the Apple sandbox, shedding light into the inner workings ofApple sandbox profiles and providing essential support for security researchersand professionals interested in Apple security mechanisms.\",\n",
              " \"Policy specification for personal user data is a hard problem, as it dependson many factors that cannot be predetermined by system developers.Simultaneously, systems are increasingly relying on users to make securitydecisions. In this paper, we propose the approach of Policy by Example (PyBE)for specifying user-specific security policies. PyBE brings the benefits of thesuccessful approach of programming by example (PBE) for program synthesis tothe policy specification domain. In PyBE, users provide policy examples thatspecify if actions should be allowed or denied in certain scenarios. PyBE thenpredicts policy decisions for new scenarios. A key aspect of PyBE is its use ofactive learning to enable users to correct potential errors in their policyspecification. To evaluate PyBE's effectiveness, we perform a feasibility studywith expert users. Our study demonstrates that PyBE correctly predicts policieswith 76% accuracy across all users, a significant improvement over naiveapproaches. Finally, we investigate the causes of inaccurate predictions tomotivate directions for future research in this promising new domain.\",\n",
              " \"Billions of users rely on the security of the Android platform to protectphones, tablets, and many different types of consumer electronics. WhileAndroid's permission model is well studied, the enforcement of the protectionpolicy has received relatively little attention. Much of this enforcement isspread across system services, taking the form of hard-coded checks withintheir implementations. In this paper, we propose Authorization Check Miner(ACMiner), a framework for evaluating the correctness of Android's accesscontrol enforcement through consistency analysis of authorization checks.ACMiner combines program and text analysis techniques to generate a rich set ofauthorization checks, mines the corresponding protection policy for eachservice entry point, and uses association rule mining at a service granularityto identify inconsistencies that may correspond to vulnerabilities. We usedACMiner to study the AOSP version of Android 7.1.1 to identify 28vulnerabilities relating to missing authorization checks. In doing so, wedemonstrate ACMiner's ability to help domain experts process thousands ofauthorization checks scattered across millions of lines of code.\",\n",
              " \"Enterprises are increasingly concerned about adversaries that slowly anddeliberately exploit resources over the course of months or even years. A keystep in this kill chain is network reconnaissance, which has historically beenactive (e.g., network scans) and therefore detectable. However, new networkingtechnology increases the possibility of passive network reconnaissance, whichwill be largely undetectable by defenders. In this paper, we propose Snaz, atechnique that uses deceptively crafted honey traffic to confound the knowledgegained through passive network reconnaissance. We present a two-playernon-zero-sum Stackelberg game model that characterizes how a defender shoulddeploy honey traffic in the presence of an adversary who is aware of Snaz. Indoing so, we demonstrate the existence of optimal defender strategies that willeither dissuade an adversary from acting on the existence of realvulnerabilities observed within network traffic, or reveal the adversary'spresence when it attempts to unknowingly attack an intrusion detection node.\",\n",
              " 'Web servers are a popular target for adversaries as they are publiclyaccessible and often vulnerable to compromise. Compromises can go unnoticed formonths, if not years, and recovery often involves a complete system rebuild. Inthis paper, we propose n-m-Variant Systems, an adversarial-resistant softwarerejuvenation framework for cloud-based web applications. We improve thestate-of-the-art by introducing a variable m that provides a knob foradministrators to tune an environment to balance resource usage, performanceoverhead, and security guarantees. Using m, security guarantees can be tunedfor seconds, minutes, days, or complete resistance. We design and implement ann-m-Variant System prototype to protect a Mediawiki PHP application servingdynamic content from an external SQL persistent storage. Our performanceevaluation shows a throughput reduction of 65% for 108 seconds of resistanceand 83% for 12 days of resistance to sophisticated adversaries, givenappropriate resource allocation. Furthermore, we use theoretical analysis andsimulation to characterize the impact of system parameters on resilience toadversaries. Through these efforts, our work demonstrates how properties ofcloud-based servers can enhance the integrity of Web servers.',\n",
              " 'Historically, enterprise network reconnaissance is an active process, ofteninvolving port scanning. However, as routers and switches become more complex,they also become more susceptible to compromise. From this vantage point, anattacker can passively identify high-value hosts such as the workstations of ITadministrators, C-suite executives, and finance personnel. The goal of thispaper is to develop a technique to deceive and dissuade such adversaries. Wepropose HoneyRoles, which uses honey connections to build metaphoricalhaystacks around the network traffic of client hosts belonging to high-valueorganizational roles. The honey connections also act as network canaries tosignal network compromise, thereby dissuading the adversary from acting oninformation observed in network flows. We design a prototype implementation ofHoneyRoles using an OpenFlow SDN controller and evaluate its security using thePRISM probabilistic model checker. Our performance evaluation shows thatHoneyRoles has a small effect on network request completion time and oursecurity analysis demonstrates that once an alert is raised, HoneyRoles canquickly identify the compromised switch with high probability. In doing so, weshow that a role-based network deception is a promising approach for defendingagainst adversaries that have compromised network devices.',\n",
              " \"Computing platforms such as smartphones frequently access Web content usingmany separate applications rather than a single Web browser application. Theseapplications often deal with sensitive user information such as financial dataor passwords, and use Secure Sockets Layer (SSL) to protect it fromunauthorized eavesdropping. However, recent studies have confirmed awide-spread misconfiguration of SSL verification in applications. This paperconsiders the difficulty faced by Android application developers when modifyingSSL code for using common features like pinning or using a self-signed SSLcertificate. For example, developing an application that accesses a test Webserver with a self-signed certificate requires additional code to remove SSLverification; however, this code is not always removed in production versionsof the application. To mitigate vulnerabilities introduced because of thecomplexity of customizing SSL code in Android applications, we propose thatcommon SSL configuration should be specified in the application's packagemanifest. We provide two concrete suggestions: 1) linking the application'sdebug state to SSL verification, and 2) pinning certificates and CAs in themanifest. We evaluate the appropriateness of these two suggestions on over13,000 applications from Google's Play Store, of which 3,302 use SSL innon-advertisement code, and find that 1,889 (57.20%) of these SSL applicationswould benefit.\",\n",
              " 'Manufacturers of smart home Internet of Things (IoT) devices are increasinglyadding voice assistant and audio monitoring features to a wide range of devicesincluding smart speakers, televisions, thermostats, security systems, anddoorbells. Consequently, many of these devices are equipped with microphones,raising significant privacy concerns: users may not always be aware of whenaudio recordings are sent to the cloud, or who may gain access to therecordings. In this paper, we present the LeakyPick architecture that enablesthe detection of the smart home devices that stream recorded audio to theInternet without the user\\'s consent. Our proof-of-concept is a LeakyPick devicethat is placed in a user\\'s smart home and periodically \"probes\" other devicesin its environment and monitors the subsequent network traffic for statisticalpatterns that indicate audio transmission. Our prototype is built on aRaspberry Pi for less than USD40 and has a measurement accuracy of 94% indetecting audio transmissions for a collection of 8 devices with voiceassistant capabilities. Furthermore, we used LeakyPick to identify 89 wordsthat an Amazon Echo Dot misinterprets as its wake-word, resulting in unexpectedaudio transmission. LeakyPick provides a cost effective approach for regularconsumers to monitor their homes for unexpected audio transmissions to thecloud.',\n",
              " 'Android filesystem access control provides a foundation for Android systemintegrity. Android utilizes a combination of mandatory (e.g., SEAndroid) anddiscretionary (e.g., UNIX permissions) access control, both to protect theAndroid platform from Android/OEM services and to protect Android/OEM servicesfrom third-party apps. However, OEMs often create vulnerabilities when theyintroduce market-differentiating features because they err when re-configuringthis complex combination of Android policies. In this paper, we propose thePolyScope tool to triage the combination of Android filesystem access controlpolicies to vet releases for vulnerabilities. The PolyScope approach leveragestwo main insights: (1) adversaries may exploit the coarse granularity ofmandatory policies and the flexibility of discretionary policies to increasethe permissions available to launch attacks, which we call permissionexpansion, and (2) system configurations may limit the ways adversaries may usetheir permissions to launch attacks, motivating computation of attackoperations. We apply PolyScope to three Google and five OEM Android releases tocompute the attack operations accurately to vet these releases forvulnerabilities, finding that permission expansion increases the permissionsavailable to launch attacks, sometimes by more than 10X, but a significantfraction of these permissions (about 15-20%) are not convertible into attackoperations. Using PolyScope, we find two previously unknown vulnerabilities,showing how PolyScope helps OEMs triage the complex combination of accesscontrol policies down to attack operations worthy of testing.',\n",
              " \"Android's filesystem access control is a crucial aspect of its systemintegrity. It utilizes a combination of mandatory access controls, such asSELinux, and discretionary access controls, like Unix permissions, along withspecialized access controls such as Android permissions to safeguard OEM andAndroid services from third-party applications. However, when OEMs introducedifferentiating features, they often create vulnerabilities due to theirinability to properly reconfigure this complex policy combination. To addressthis, we introduce the POLYSCOPE tool, which triages Android filesystem accesscontrol policies to identify attack operations - authorized operations that maybe exploited by adversaries to elevate their privileges. POLYSCOPE has threesignificant advantages over prior analyses: it allows for the independentextension and analysis of individual policy models, understands the flexibilityuntrusted parties have in modifying access control policies, and can identifyattack operations that system configurations permit. We demonstrate theeffectiveness of POLYSCOPE by examining the impact of Scoped Storage onAndroid, revealing that it reduces the number of attack operations possible onexternal storage resources by over 50%. However, because OEMs only partiallyadopt Scoped Storage, we also uncover two previously unknown vulnerabilities,demonstrating how POLYSCOPE can assess an ideal scenario where all apps complywith Scoped Storage, which can reduce the number of untrusted parties accessingattack operations by over 65% on OEM systems. POLYSCOPE thus helps Android OEMsevaluate complex access control policies to pinpoint the attack operations thatrequire further examination.\",\n",
              " 'We consider a system in which two viruses of theSusceptible-Infected-Susceptible (SIS) type compete over general, overlaidgraphs. While such systems have been the focus of many recent works, they havemostly been studied in the sense of convergence analysis, with no existingresults quantifying the non-trivial coexistence equilibria (CE) - that is, whenboth competing viruses maintain long term presence over the network. In thispaper, we prove monotonicity of the CE with respect to effective infectionrates of the two viruses, and provide the first quantitative analysis of suchequilibria in the form of upper bounds involving spectral radii of theunderlying graphs, as well as positive equilibria of related single-virussystems. Our results provide deeper insight into how the long term infectionprobabilities are affected by system parameters, which we further highlight vianumerical results.',\n",
              " 'With the ease of deployment, capabilities of evading the jammers andobscuring their existence, unmanned aerial vehicles (UAVs) are one of the mostsuitable candidates to perform surveillance. There exists a body of literaturein which the inspectors follow a deterministic trajectory to conductsurveillance, which results in a predictable environment for maliciousentities. Thus, introducing randomness to the surveillance is of particularinterest. In this work, we propose a novel framework for stochasticUAV-assisted surveillance that i) inherently considers the battery constraintsof the UAVs, ii) proposes random moving patterns modeled via random walks, andiii) adds another degree of randomness to the system via consideringprobabilistic inspections. We formulate the problem of interest, i.e.,obtaining the energy-efficient random walk and inspection policies of the UAVssubject to probabilistic constraints on inspection criteria of the sites andbattery consumption of the UAVs, which turns out to be signomial programmingthat is highly non-convex. To solve it, we propose a centralized and adistributed algorithm along with their performance guarantee. This workcontributes to both UAV-assisted surveillance and classic random walkliterature by designing random walks with random inspection policies onweighted graphs with energy limited random walkers.',\n",
              " 'The dynamics of the spread of contagions such as viruses, infectious diseasesor even rumors/opinions over contact networks (graphs) have effectively beencaptured by the well known \\\\textit{Susceptible-Infected-Susceptible} ($SIS$)epidemic model in recent years. When it comes to competition between two suchcontagions spreading on overlaid graphs, their propagation is captured byso-called \\\\textit{bi-virus} epidemic models. Analysis of such dynamical systemsinvolve the identification of equilibrium points and its convergenceproperties, which determine whether either of the viruses dies out, or bothsurvive together. We demonstrate how the existing works are unsuccessful incharacterizing a large subset of the model parameter space, including allparameters for which the competitiveness of the bi-virus system is significantenough to attain coexistence of the epidemics. In this paper, we fill in thisvoid and obtain convergence results for the entirety of the model parameterspace; giving precise conditions (necessary and sufficient) under which thesystem \\\\textit{globally converges} to a \\\\textit{trichotomy} of possibleoutcomes: a virus-free state, a single-virus state, and to a coexistence state-- the first such result.',\n",
              " \"We consider the stochastic gradient descent (SGD) algorithm driven by ageneral stochastic sequence, including i.i.d noise and random walk on anarbitrary graph, among others; and analyze it in the asymptotic sense.Specifically, we employ the notion of `efficiency ordering', a well-analyzedtool for comparing the performance of Markov Chain Monte Carlo (MCMC) samplers,for SGD algorithms in the form of Loewner ordering of covariance matricesassociated with the scaled iterate errors in the long term. Using thisordering, we show that input sequences that are more efficient for MCMCsampling also lead to smaller covariance of the errors for SGD algorithms inthe limit. This also suggests that an arbitrarily weighted MSE of SGD iteratesin the limit becomes smaller when driven by more efficient chains. Our findingis of particular interest in applications such as decentralized optimizationand swarm learning, where SGD is implemented in a random walk fashion on theunderlying communication graph for cost issues and/or data privacy. Wedemonstrate how certain non-Markovian processes, for which typical mixing-timebased non-asymptotic bounds are intractable, can outperform their Markoviancounterparts in the sense of efficiency ordering for SGD. We show the utilityof our method by applying it to gradient descent with shuffling and mini-batchgradient descent, reaffirming key results from existing literature under aunified framework. Empirically, we also observe efficiency ordering forvariants of SGD such as accelerated SGD and Adam, open up the possibility ofextending our notion of efficiency ordering to a broader family of stochasticoptimization algorithms.\",\n",
              " 'We study convergence properties of competing epidemic models of theSusceptible-Infected-Susceptible (SIS) type. The SIS epidemic model has seenwidespread popularity in modelling the spreading dynamics of contagions such asviruses, infectious diseases, or even rumors/opinions over contact networks(graphs).We analyze the case of two such viruses spreading on overlaid graphs,with non-linear rates of infection spread and recovery. We call this thenon-linear bi-virus model and, building upon recent results, obtain preciseconditions for global convergence of the solutions to a trichotomy of possibleoutcomes: a virus-free state, a single-virus state, and to a coexistence state.Our techniques are based on the theory of monotone dynamical systems (MDS), incontrast to Lyapunov based techniques that have only seen partial success indetermining convergence properties in the setting of competing epidemics. Wedemonstrate how the existing works have been unsuccessful in characterizing alarge subset of the model parameter space for bi-virus epidemics, including allscenarios leading to coexistence of the epidemics. To the best of ourknowledge, our results are the first in providing complete convergence analysisfor the bi-virus system with nonlinear infection and recovery rates on generalgraphs.',\n",
              " 'We consider random walks on discrete state spaces, such as general undirectedgraphs, where the random walkers are designed to approximate a target quantityover the network topology via sampling and neighborhood exploration in the formof Markov chain Monte Carlo (MCMC) procedures. Given any Markov chaincorresponding to a target probability distribution, we design a self-repellentrandom walk (SRRW) which is less likely to transition to nodes that were highlyvisited in the past, and more likely to transition to seldom visited nodes. Fora class of SRRWs parameterized by a positive real {\\\\alpha}, we prove that theempirical distribution of the process converges almost surely to the the target(stationary) distribution of the underlying Markov chain kernel. We thenprovide a central limit theorem and derive the exact form of the arisingasymptotic co-variance matrix, which allows us to show that the SRRW with astronger repellence (larger {\\\\alpha}) always achieves a smaller asymptoticcovariance, in the sense of Loewner ordering of co-variance matrices.Especially for SRRW-driven MCMC algorithms, we show that the decrease in theasymptotic sampling variance is of the order O(1/{\\\\alpha}), eventually goingdown to zero. Finally, we provide numerical simulations complimentary to ourtheoretical results, also empirically testing a version of SRRW with {\\\\alpha}increasing in time to combine the benefits of smaller asymptotic variance dueto large {\\\\alpha}, with empirically observed faster mixing properties of SRRWwith smaller {\\\\alpha}.',\n",
              " 'Graph sampling via crawling has been actively considered as a generic andimportant tool for collecting uniform node samples so as to consistentlyestimate and uncover various characteristics of complex networks. The so-calledsimple random walk with re-weighting (SRW-rw) and Metropolis-Hastings (MH)algorithm have been popular in the literature for such unbiased graph sampling.However, an unavoidable downside of their core random walks -- slow diffusionover the space, can cause poor estimation accuracy. In this paper, we proposenon-backtracking random walk with re-weighting (NBRW-rw) and MH algorithm withdelayed acceptance (MHDA) which are theoretically guaranteed to achieve, atalmost no additional cost, not only unbiased graph sampling but also higherefficiency (smaller asymptotic variance of the resulting unbiased estimators)than the SRW-rw and the MH algorithm, respectively. In particular, a remarkablefeature of the MHDA is its applicability for any non-uniform node sampling likethe MH algorithm, but ensuring better sampling efficiency than the MHalgorithm. We also provide simulation results to confirm our theoreticalfindings.',\n",
              " \"Recently several CSMA algorithms based on the Glauber dynamics model havebeen proposed for multihop wireless scheduling, as viable solutions to achievethe throughput optimality, yet are simple to implement. However, their delayperformances still remain unsatisfactory, mainly due to the nature of theunderlying Markov chains that imposes a fundamental constraint on how the linkstate can evolve over time. In this paper, we propose a new approach towardbetter queueing and delay performance, based on our observation that thealgorithm needs not be Markovian, as long as it can be implemented in adistributed manner, achieve the same throughput optimality, while offering farbetter delay performance for general network topologies. Our approach hingesupon utilizing past state information observed by local link and thenconstructing a high-order Markov chain for the evolution of the feasible linkschedules. We show in theory and simulation that our proposed algorithm, nameddelayed CSMA, adds virtually no additional overhead onto the existingCSMA-based algorithms, achieves the throughput optimality under the usualchoice of link weight as a function of local queue length, and also providesmuch better delay performance by effectively `de-correlating' the link stateprocess (thus removing link starvation) under any arbitrary network topology.From our extensive simulations we observe that the delay under our algorithmcan be often reduced by a factor of 20 over a wide range of scenarios, comparedto the standard Glauber-dynamics-based CSMA algorithm.\",\n",
              " 'Markov Chain Monte Carlo (MCMC) has been the de facto technique for samplingand inference of large graphs such as online social networks. At the heart ofMCMC lies the ability to construct an ergodic Markov chain that attains anygiven stationary distribution $\\\\boldsymbol{\\\\pi}$, often in the form of randomwalks or crawling agents on the graph. Most of the works around MCMC, however,presume that the graph is undirected or has reciprocal edges, and becomeinapplicable when the graph is directed and non-reciprocal. Here we develop asimilar framework for directed graphs, which we call Non-Markovian Monte Carlo(NMMC), by establishing a mapping to convert $\\\\boldsymbol{\\\\pi}$ into thequasi-stationary distribution of a carefully constructed transient Markov chainon an extended state space. As applications, we demonstrate how to achieve anygiven distribution $\\\\boldsymbol{\\\\pi}$ on a directed graph and estimate theeigenvector centrality using a set of non-Markovian, history-dependent randomwalks on the same graph in a distributed manner. We also provide numericalresults on various real-world directed graphs to confirm our theoreticalfindings, and present several practical enhancements to make our NMMC methodready for practical use in most directed graphs. To the best of our knowledge,the proposed NMMC framework for directed graphs is the first of its kind,unlocking all the limitations set by the standard MCMC methods for undirectedgraphs.',\n",
              " 'In this paper, we aim to understand the transient dynamics of asusceptible-infected (SI) epidemic spreading process on a large network. The SImodel has been largely overlooked in the literature, while it is naturally abetter fit for modeling the malware propagation in early times whenpatches/vaccines are not available, or over a wider range of timescales whenmassive patching is practically infeasible. Nonetheless, its analysis is simplynon-trivial, as its important dynamics are all transient and the usualstability/steady-state analysis no longer applies. To this end, we develop atheoretical framework that allows us to obtain an accurate closed-formapproximate solution to the original SI dynamics on any arbitrary network,which captures the temporal dynamics over all time and is tighter than theexisting approximation, and also to provide a new interpretation viareliability theory. As its applications, we further develop vaccinationpolicies with or without knowledge of already-infected nodes, to mitigate thefuture epidemic spreading to the extent possible, and demonstrate theireffectiveness through numerical simulations.',\n",
              " 'We propose a generic system model for a special category of interdependentnetworks, demand-supply networks, in which the demand and the supply nodes areassociated with heterogeneous loads and resources, respectively. Our modelsheds a light on a unique cascading failure mechanism induced by resource/loadfluctuations, which in turn opens the door to conducting stress analysis oninterdependent networks. Compared to the existing literature mainly concernedwith the node connectivity, we focus on developing effective resourceallocation methods to prevent these cascading failures from happening and tomitigate/confine them upon occurrence in the network. To prevent cascadingfailures, we identify some dangerous stress mechanisms, based on which wequantify the robustness of the network in terms of the resource configurationscheme. Afterward, we identify the optimal resource configuration under tworesource/load fluctuations scenarios: uniform and proportional fluctuations. Wefurther investigate the optimal resource configuration problem consideringheterogeneous resource sharing costs among the nodes. To mitigate/confineongoing cascading failures, we propose two network adaptations mechanisms:intentional failure and resource re-adjustment, based on which we propose analgorithm to mitigate an ongoing cascading failure while reinforcing thesurviving network with a high robustness to avoid further failures.',\n",
              " 'The Opportunistic Spectrum Access (OSA) model has been developed for thesecondary users (SUs) to exploit the stochastic dynamics of licensed channelsfor file transfer in an opportunistic manner. Common approaches to designchannel sensing strategies for throughput-oriented applications tend tomaximize the long-term throughput, with the hope that it provides reduced filetransfer time as well. In this paper, we show that this is not correct ingeneral, especially for small files. Unlike prior delay-related works thatseldom consider the heterogeneous channel rate and bursty incoming packets, ourwork explicitly considers minimizing the file transfer time of a single fileconsisting of multiple packets in a set of heterogeneous channels. We formulatea mathematical framework for the static policy, and extend to dynamic policy bymapping our file transfer problem to the stochastic shortest path problem. Weanalyze the performance of our proposed static optimal and dynamic optimalpolicies over the policy that maximizes long-term throughput. We then propose aheuristic policy that takes into account the performance-complexity tradeoffand an extension to online implementation with unknown channel parameters, andalso present the regret bound for our online algorithm. We also presentnumerical simulations that reflect our analytical results.',\n",
              " \"We study the file transfer problem in opportunistic spectrum access (OSA)model, which has been widely studied in throughput-oriented applications formax-throughput strategies and in delay-related works that commonly assumeidentical channel rates and fixed file sizes. Our work explicitly considersminimizing the file transfer time for a given file in a set ofheterogeneous-rate Bernoulli channels, showing that max-throughput policydoesn't minimize file transfer time in general. We formulate a mathematicalframework for static extend to dynamic policies by mapping our file transferproblem to a stochastic shortest path problem. We analyze the performance ofour proposed static and dynamic optimal policies over the max-throughputpolicy. We propose a mixed-integer programming formulation as an efficientalternative way to obtain the dynamic optimal policy and show a huge reductionin computation time. Then, we propose a heuristic policy that takes intoaccount the performance-complexity tradeoff and consider the onlineimplementation with unknown channel parameters. Furthermore, we presentnumerical simulations to support our analytical results and discuss the effectof switching delay on different policies. Finally, we extend the file transferproblem to Markovian channels and demonstrate the impact of the correlation ofeach channel.\",\n",
              " 'The Fiedler vector of a graph, namely the eigenvector corresponding to thesecond smallest eigenvalue of a graph Laplacian matrix, plays an important rolein spectral graph theory with applications in problems such as graphbi-partitioning and envelope reduction. Algorithms designed to estimate thisquantity usually rely on a priori knowledge of the entire graph, and employtechniques such as graph sparsification and power iterations, which haveobvious shortcomings in cases where the graph is unknown, or changingdynamically. In this paper, we develop a framework in which we construct astochastic process based on a set of interacting random walks on a graph andshow that a suitably scaled version of our stochastic process converges to theFiedler vector for a sufficiently large number of walks. Like other techniquesbased on exploratory random walks and on-the-fly computations, such as MarkovChain Monte Carlo (MCMC), our algorithm overcomes challenges typically faced bypower iteration based approaches. But, unlike any existing random walk basedmethod such as MCMCs where the focus is on the leading eigenvector, ourframework with interacting random walks converges to the Fiedler vector (secondeigenvector). We also provide numerical results to confirm our theoreticalfindings on different graphs, and show that our algorithm performs well over awide range of parameters and the number of random walks. Simulations resultsover time varying dynamic graphs are also provided to show the efficacy of ourrandom walk based technique in such settings. As an important contribution, weextend our results and show that our framework is applicable for approximatingnot just the Fiedler vector of graph Laplacians, but also the secondeigenvector of any time reversible Markov Chain kernel via interacting randomwalks.',\n",
              " 'We comment on a recent theory of dynamic wetting, that is based directly upona model for interface formation, introduced by Shikhmurzaev. We argue that thetreatment of surface tension and its relaxation, inherent in the originalmodel, is physically flawed.',\n",
              " \"The notion of interactional expertise is explained starting with its originsand discussing its many applications. Interactional expertise is the ability tounderstand a technical area purely be deeply immersed in its'practice-language' without actually practising. One of its many applicationsis to explain how large sciences are managed.\",\n",
              " 'We describe and then elaborate the model of trading zones first presented inCollins et al 2007, Trading Zones and Interactional Expertise. We believe thisexpanded version of the model includes some very important but previouslyoverlooked ways for separate language communities to communicate. We illustratethe argument with examples.',\n",
              " 'In this paper, we present three new classes of $q$-ary quantum MDS codesutilizing generalized Reed-Solomon codes satisfying Hermitian self-orthogonalproperty. Among our constructions, the minimum distance of some $q$-ary quantumMDS codes can be bigger than $\\\\frac{q}{2}+1$. Comparing to previous knownconstructions, the lengths of codes in our constructions are more flexible.',\n",
              " 'In this paper, a criterion of MDS Euclidean self-orthogonal codes ispresented. New MDS Euclidean self-dual codes and self-orthogonal codes areconstructed via this criterion. In particular, among our constructions, forlarge square $q$, about $\\\\frac{1}{8}\\\\cdot q$ new MDS Euclidean (almost)self-dual codes over $\\\\F_q$ can be produced. Moreover, we can construct about$\\\\frac{1}{4}\\\\cdot q$ new MDS Euclidean self-orthogonal codes with differenteven lengths $n$ with dimension $\\\\frac{n}{2}-1$.',\n",
              " 'In this paper, we propose a mechanism on the constructions of MDS codes witharbitrary dimensions of Euclidean hulls. Precisely, we construct (extended)generalized Reed-Solomon(GRS) codes with assigned dimensions of Euclidean hullsfrom self-orthogonal GRS codes. It turns out that our constructions are moregeneral than previous works on Euclidean hulls of (extended) GRS codes.',\n",
              " 'Galois hulls of linear codes have important applications in quantum codingtheory. In this paper, we construct some new classes of (extended) generalizedReed-Solomon (GRS) codes with Galois hulls of arbitrary dimensions. We alsopropose a general method on constructing GRS codes with Galois hulls ofarbitrary dimensions from special Euclidean orthogonal GRS codes. Finally, weconstruct several new families of entanglement-assisted quantumerror-correcting codes (EAQECCs) and MDS EAQECCs by utilizing the aboveresults.',\n",
              " 'This paper proposes a new methodology to predict and update the residualuseful lifetime of a system using a sequence of degradation images. Themethodology integrates tensor linear algebra with traditional location-scaleregression widely used in reliability and prognosis. To address the highdimensionality challenge, the degradation image streams are first projected toa low-dimensional tensor subspace that is able to preserve their information.Next, the projected image tensors are regressed against time-to-failure viapenalized location-scale tensor regression. The coefficient tensor is thendecomposed using CANDECOMP/PARAFAC (CP) and Tucker decompositions, whichenables parameter estimation in a high-dimensional setting. Two optimizationalgorithms with a global convergence property are developed for modelestimation. The effectiveness of our models is validated using a simulateddataset and infrared degradation image streams from a rotating machinery.',\n",
              " 'In this paper, we produce new classes of MDS self-dual codes via (extended)generalized Reed-Solomon codes over finite fields of odd characteristic. Amongour constructions, there are many MDS self-dual codes with new parameters whichhave never been reported. For odd prime power $q$ with $q$ square, the totalnumber of lengths for MDS self-dual codes over $\\\\mathbb{F}_q$ presented in thispaper is much more than those in all the previous results.',\n",
              " 'Systematic constructions of MDS self-dual codes is widely concerned. In thispaper, we consider the constructions of MDS Euclidean self-dual codes fromshort length. Indeed, the exact constructions of MDS Euclidean self-dual codesfrom short length ($n=3,4,5,6$) are given. In general, we construct more new of$q$-ary MDS Euclidean self-dual codes from MDS self-dual codes of known lengthvia generalized Reed-Solomon (GRS for short) codes and extended GRS codes.',\n",
              " 'This paper proposes a supervised dimension reduction methodology for tensordata which has two advantages over most image-based prognostic models. First,the model does not require tensor data to be complete which expands itsapplication to incomplete data. Second, it utilizes time-to-failure (TTF) tosupervise the extraction of low-dimensional features which makes the extractedfeatures more effective for the subsequent prognostic. Besides, an optimizationalgorithm is proposed for parameter estimation and closed-form solutions arederived under certain distributions.',\n",
              " 'The root-cause diagnostics of product quality defects in multistagemanufacturing processes often requires a joint identification of crucial stagesand process variables. To meet this requirement, this paper proposes a novelpenalized matrix regression methodology for two-dimensional variable selection.The method regresses a scalar response variable against a matrix-basedpredictor using a generalized linear model. The unknown regression coefficientmatrix is decomposed as a product of two factor matrices. The rows of the firstfactor matrix and the columns of the second factor matrix are simultaneouslypenalized to inspire sparsity. To estimate the parameters, we develop a blockcoordinate proximal descent (BCPD) optimization algorithm, which cyclicallysolves two convex sub-optimization problems. We have proved that the BCPDalgorithm always converges to a critical point with any initialization. Inaddition, we have also proved that each of the sub-optimization problems has aclosed-form solution if the response variable follows a distribution whose(negative) log-likelihood function has a Lipschitz continuous gradient. Asimulation study and a dataset from a real-world application are used tovalidate the effectiveness of the proposed method.',\n",
              " 'While inequality (9) is mathematically correct, it does not imply alignmentbetween path-averaged scalars and the hyperbolic LCSs.',\n",
              " 'We use recent developments in the theory of finite-time dynamical systems toobjectively locate the material boundaries of coherent vortices intwo-dimensional Navier--Stokes turbulence. We show that these boundaries areoptimal in the sense that any closed curve in their exterior will losecoherence under material advection. Through a detailed comparison, we find thatother available Eulerian and Lagrangian techniques significantly underestimatethe size of each coherent vortex.',\n",
              " 'Extreme events, such as rogue waves, earthquakes and stock market crashes,occur spontaneously in many dynamical systems. Because of their usually adverseconsequences, quantification, prediction and mitigation of extreme events arehighly desirable. Here, we review several aspects of extreme events inphenomena described by high-dimensional, chaotic dynamical systems. Wespecially focus on two pressing aspects of the problem: (i) Mechanismsunderlying the formation of extreme events and (ii) Real-time prediction ofextreme events. For each aspect, we explore methods relying on models, data orboth. We discuss the strengths and limitations of each approach as well aspossible future research directions.',\n",
              " 'We show how the recently developed theory of geodesic transport barriers forfluid flows can be used to uncover key invariant manifolds in externallyforced, one-degree-of-freedom mechanical systems. Specifically, invariant setsin such systems turn out to be shadowed by least-stretching geodesics of theCauchy-Green strain tensor computed from the flow map of the forced mechanicalsystem. This approach enables the finite-time visualization of generalizedstable and unstable manifolds, attractors and generalized KAM curves underarbitrary forcing, when Poincare maps are not available. We illustrate theseresults by detailed visualizations of the key finite-time invariant sets ofconservatively and dissipatively forced Duffing oscillators.',\n",
              " 'We develop a variational principle that extends the notion of a shearlesstransport barrier from steady to general unsteady two-dimensional flows andmaps defined over a finite time interval. This principle reveals thathyperbolic Lagrangian Coherent Structures (LCSs) and parabolic LCSs (or jetcores) are the two main types of shearless barriers in unsteady flows. Based onthe boundary conditions they satisfy, parabolic barriers are found to be moreobservable and robust than hyperbolic barriers, confirming widespread numericalobservations. Both types of barriers are special null-geodesics of anappropriate Lorentzian metric derived from the Cauchy--Green strain tensor.Using this fact, we devise an algorithm for the automated computation ofparabolic barriers. We illustrate our detection method on steady and unsteadynon-twist maps and on the aperiodically forced Bickley jet.',\n",
              " 'The Maxey--Riley equation describes the motion of an inertial (i.e.,finite-size) spherical particle in an ambient fluid flow. The equation is asecond-order, implicit integro-differential equation with a singular kernel,and with a forcing term that blows up at the initial time. Despite thewidespread use of the equation in applications, the basic properties of itssolutions have remained unexplored. Here we fill this gap by proving localexistence and uniqueness of weak solutions. For certain initial velocitiesbetween the particle and the fluid, the results extend to strong solutions. Wealso prove continuous differentiability of the weak and strong solutions withrespect to their initial conditions. This justifies the search for coherentstructures in inertial flows using the Cauchy--Green strain tensor.',\n",
              " 'Recent experimental and numerical observations have shown the significance ofthe Basset--Boussinesq memory term on the dynamics of small spherical rigidparticles (or inertial particles) suspended in an ambient fluid flow. Theseobservations suggest an algebraic decay to an asymptotic state, as opposed tothe exponential convergence in the absence of the memory term. Here, we provethat the observed algebraic decay is a universal property of the Maxey--Rileyequation. Specifically, the particle velocity decays algebraically in time to alimit that is $\\\\mathcal O(\\\\epsilon)$-close to the fluid velocity, where$0<\\\\epsilon\\\\ll 1$ is proportional to the square of the ratio of the particleradius to the fluid characteristic length-scale. These results follows from asharp analytic upper bound that we derive for the particle velocity. Forcompleteness, we also present a first proof of existence and uniqueness ofglobal solutions to the Maxey--Riley equation, a nonlinear system offractional-order differential equations.',\n",
              " 'We propose rotation inferred from the polar decomposition of the flowgradient as a diagnostic for elliptic (or vortex-type) invariant regions innon-autonomous dynamical systems. We consider here two- and three-dimensionalsystems, in which polar rotation can be characterized by a single angle. Forthis polar rotation angle (PRA), we derive explicit formulas using the singularvalues and vectors of the flow gradient. We find that closed level sets of thePRA reveal elliptic islands in great detail, and singular level sets of the PRAuncover centers of such islands. Both features turn out to be objective(frame-invariant) for two-dimensional systems. We illustrate the diagnosticpower of PRA for elliptic structures on several examples.',\n",
              " 'We consider the incompressible Navier--Stokes equations with periodicboundary conditions and time-independent forcing. For this type of flow, wederive adjoint equations whose trajectories converge asymptotically to theequilibrium and traveling wave solutions of the Navier--Stokes equations. Usingthe adjoint equations, arbitrary initial conditions evolve to the vicinity of a(relative) equilibrium at which point a few Newton-type iterations yield thedesired (relative) equilibrium solution. We apply this adjoint-based method toa chaotic two-dimensional Kolmogorov flow. A convergence rate of 100% isobserved, leading to the discovery of 21 new steady state and traveling wavesolutions at Reynolds number Re=40. Some of the new invariant solutions havespatially localized structures that were previously believed to only exist ondomains with large aspect ratios. We show that one of the newly found steadystate solutions underpins the temporal intermittencies, i.e., high energydissipation episodes of the flow. More precisely, it is shown that eachintermittent episode of a generic turbulent trajectory corresponds to its closepassage to this equilibrium solution.',\n",
              " 'Drawing upon the bursting mechanism in slow-fast systems, we proposeindicators for the prediction of such rare extreme events which do not requirea priori known slow and fast coordinates. The indicators are associated withfunctionals defined in terms of Optimally Time Dependent (OTD) modes. One suchfunctional has the form of the largest eigenvalue of the symmetric part of thelinearized dynamics reduced to these modes. In contrast to other choices ofsubspaces, the proposed modes are flow invariant and therefore a projectiononto them is dynamically meaningful. We illustrate the application of theseindicators on three examples: a prototype low-dimensional model, a body forcedturbulent fluid flow, and a unidirectional model of nonlinear water waves. Weuse Bayesian statistics to quantify the predictive power of the proposedindicators.',\n",
              " \"In 1966, Arnold [1] showed that the Lagrangian flow of ideal incompressiblefluids (described by Euler equations) coincide with the geodesic flow on themanifold of volume preserving diffeomorphisms of the fluid domain. Arnold'sproof and the subsequent work on this topic rely heavily on the properties ofLie groups and Lie algebras which remain unfamiliar to most fluid dynamicists.In this note, we provide a simple derivation of Arnold's result which only usesthe classical methods of calculus of variations. In particular, we show thatthe Lagrangian flow maps generated by the solutions of the incompressible Eulerequations coincide with the stationary curves of an appropriate energyfunctional when the extremization is carried out over the set ofvolume-preserving diffeomorphisms.\",\n",
              " 'We derive precursors of extreme dissipation events in a turbulent channelflow. Using a recently developed method that combines dynamics and statisticsfor the underlying attractor, we extract a characteristic state that precedeslaminarization events that subsequently lead to extreme dissipation episodes.Our approach utilizes coarse statistical information for the turbulentattractor, in the form of second order statistics, to identify high-likelihoodregions in the state space. We then search within this high probabilitymanifold for the state that leads to the most finite-time growth of the flowkinetic energy. This state has both high probability of occurrence and leads toextreme values of dissipation. We use the alignment between a given turbulentstate and this critical state as a precursor for extreme events and demonstrateits favorable properties for prediction of extreme dissipation events. Finally,we analyze the physical relevance of the derived precursor and show its robustcharacter for different Reynolds numbers. Overall, we find that our choice ofprecursor works well at the Reynolds number it is computed at and at higherReynolds number flows with similar extreme events.',\n",
              " 'Accelerated gradient descent iterations are widely used in optimization. Itis known that, in the continuous-time limit, these iterations converge to asecond-order differential equation which we refer to as the acceleratedgradient flow. Using geometric singular perturbation theory, we show that,under certain conditions, the accelerated gradient flow possesses an attractinginvariant slow manifold to which the trajectories of the flow convergeasymptotically. We obtain a general explicit expression in the form offunctional series expansions that approximates the slow manifold to anyarbitrary order of accuracy. To the leading order, the accelerated gradientflow reduced to this slow manifold coincides with the usual gradient descent.We illustrate the implications of our results on three examples.',\n",
              " \"We study the horizontal dispersion of passive tracer particles on the freesurface of gravity waves in deep water. For random linear waves with theJONSWAP spectrum, the Lagrangian particle trajectories are computed using anexact nonlinear model known as the John--Sclavounos equation. We show that thesingle-particle dispersion exhibits an unusual super-diffusive behavior. Inparticular, for large times $t$, the variance of the tracer $\\\\langle|X(t)|^2\\\\rangle$ increases as a quadratic function of time, i.e., $\\\\langle|X(t)|^2\\\\rangle\\\\sim t^2$. This dispersion is markedly faster than Taylor'ssingle-particle dispersion theory which predicts that the variance of passivetracers grows linearly with time for large $t$. Our results imply that the wavemotion significantly enhances the dispersion of fluid particles. We show thatthis super-diffusive behavior is a result of the long-term correlation of theLagrangian velocities of fluid parcels on the free surface.\",\n",
              " 'Extreme events that arise spontaneously in chaotic dynamical systems oftenhave an adverse impact on the system or the surrounding environment. As such,their mitigation is highly desirable. Here, we introduce a novel controlstrategy for mitigating extreme events in a turbulent shear flow. Thecontroller combines a probabilistic prediction of the extreme events with adeterministic actuator. The predictions are used to actuate the controller onlywhen an extreme event is imminent. When actuated, the controller only acts onthe degrees of freedom that are involved in the formation of the extremeevents, exerting minimal interference with the flow dynamics. As a result, theattractors of the controlled and uncontrolled systems share the same chaoticcore (containing the non-extreme events) and only differ in the tail of theirdistributions. We propose that such adaptive low-dimensional controllers shouldbe used to mitigate extreme events in general chaotic dynamical systems, beyondthe shear flow considered here.',\n",
              " 'In stochastic multistable systems driven by the gradient of a potential,transitions between equilibria is possible because of noise. We study theability of linear delay feedback control to mitigate these transitions,ensuring that the system stays near a desirable equilibrium. For small delays,we show that the control term has two effects: i) a stabilizing effect bydeepening the potential well around the desirable equilibrium, and ii) adestabilizing effect by intensifying the noise by a factor of$(1-\\\\tau\\\\alpha)^{-1/2}$, where $\\\\tau$ and $\\\\alpha$ denote the delay and thecontrol gain, respectively. As a result, successful mitigation depends on thecompetition between these two factors. We also derive analytical results thatelucidate the choice of the appropriate control gain and delay that ensuresuccessful mitigations. These results eliminate the need for any Monte Carlosimulations of the stochastic differential equations, and thereforesignificantly reduce the computational cost of determining the suitable controlparameters. We demonstrate the application of our results on two examples.',\n",
              " 'We study the mitigation of climate tipping point transitions using an energybalance model. The evolution of the global mean surface temperature is coupledwith the CO2 concentration through the green house effect. We model the CO2concentration with a stochastic delay differential equation (SDDE), accountingfor various carbon emission and capture scenarios. The resulting coupled systemof SDDEs exhibits a tipping point phenomena: if CO2 concentration exceeds acritical threshold (around 478ppm), the temperature experiences an abruptincrease of about six degrees Celsius. We show that the CO2 concentrationexhibits a transient growth which may cause a climate tipping point, even ifthe concentration decays asymptotically. We derive a rigorous upper bound forthe CO2 evolution which quantifies its transient and asymptotic growths, andprovides sufficient conditions for evading the climate tipping point. Combiningthis upper bound with Monte Carlo simulations of the stochastic climate model,we investigate the emission reduction and carbon capture scenarios that wouldavert the tipping point.',\n",
              " 'Hyperbolic Lagrangian Coherent Structures (LCSs) are locally most repellingor most attracting material surfaces in a finite-time dynamical system. Toidentify both types of hyperbolic LCSs at the same time instance, the standardpractice has been to compute repelling LCSs from future data and attractingLCSs from past data. This approach tacitly assumes that coherent structures inthe flow are fundamentally recurrent, and hence gives inconsistent results fortemporally aperiodic systems. Here we resolve this inconsistency by showing howboth repelling and attracting LCSs are computable at the same time instancefrom a single forward or a single backward run. These LCSs are obtained assurfaces normal to the weakest and strongest eigenvectors of the Cauchy-Greenstrain tensor.',\n",
              " 'We consider rare transitions induced by colored noise excitation inmultistable systems. We show that undesirable transitions can be mitigated by asimple time-delay feedback control if the control parameters are judiciouslychosen. We devise a parsimonious method for selecting the optimal controlparameters, without requiring any Monte Carlo simulations of the system. Thismethod relies on a new nonlinear Fokker-Planck equation whose stationaryresponse distribution is approximated by a rapidly convergent iterativealgorithm. In addition, our framework allows us to accurately predict, andsubsequently suppress, the modal drift and tail inflation in the controlledstationary distribution. We demonstrate the efficacy of our method on twoexamples, including an optical laser model perturbed by multiplicative colorednoise.',\n",
              " 'We consider reduced-order modeling of nonlinear dispersive waves described bya class of nonlinear Schrodinger (NLS) equations. We compare two nonlinearreduced-order modeling methods: (i) The reduced Lagrangian approach whichrelies on the variational formulation of NLS and (ii) The recently developedmethod of reduced-order nonlinear solutions (RONS). First, we prove thesurprising result that, although the two methods are seemingly quite different,they can be obtained from the real and imaginary parts of a singlecomplex-valued master equation. Furthermore, for the NLS equation in astationary frame, we show that the reduced Lagrangian method fails to predictthe correct group velocity of the waves whereas RONS predicts the correct groupvelocity. Finally, for the modified NLS equation, where the reduced Lagrangianapproach is inapplicable, the RONS reduced-order model accurately approximatesthe true solutions.',\n",
              " 'Reconstructing high-resolution flow fields from sparse measurements is amajor challenge in fluid dynamics. Existing methods often vectorize the flow bystacking different spatial directions on top of each other, hence confoundingthe information encoded in different dimensions. Here, we introduce atensor-based sensor placement and flow reconstruction method which retains andexploits the inherent multidimensionality of the flow. We derive estimates forthe flow reconstruction error, storage requirements and computational cost ofour method. We show, with examples, that our tensor-based method issignificantly more accurate than similar vectorized methods. Furthermore, thevariance of the error is smaller when using our tensor-based method. While thecomputational cost of our method is comparable to similar vectorized methods,it reduces the storage cost by several orders of magnitude. The reduced storagecost becomes even more pronounced as the dimension of the flow increases. Wedemonstrate the efficacy of our method on three examples: a chaotic Kolmogorovflow, in-situ and satellite measurements of the global sea surface temperature,and 3D unsteady simulated flow around a marine research vessel.',\n",
              " 'We put forward the idea of defining vortex boundaries in planar flows asclosed material barriers to the diffusive transport of vorticity. Suchdiffusive vortex boundaries minimize the leakage of vorticity from the fluidmass they enclose when compared to other nearby material curves. Building onrecent results on passive diffusion barriers, we develop an algorithm for theautomated identification of such structures from general, two-dimensionalunsteady flow data. As examples, we identify vortex boundaries as vorticitydiffusion barriers in two flows: an explicitly known laminar flow and anumerically generated turbulent Navier--Stokes flow.',\n",
              " 'The efficiency of a fluid mixing device is often limited by fundamental lawsand/or design constraints, such that a perfectly homogeneous mixture cannot beobtained in finite time. Here, we address the natural corollary question: Giventhe best available mixer, what is the optimal initial tracer pattern that leadsto the most homogeneous mixture after a prescribed finite time? For idealpassive tracers, we show that this optimal initial condition coincides with theright singular vector (corresponding to the smallest singular value) of asuitably truncated Perron-Frobenius (PF) operator. The truncation of the PFoperator is made under the assumption that there is a small length-scalethreshold $\\\\ell_\\\\nu$ under which the tracer blobs are considered, for allpractical purposes, completely mixed. We demonstrate our results on twoexamples: a prototypical model known as the sine flow and a direct numericalsimulation of two-dimensional turbulence. Evaluating the optimal initialcondition through this framework only requires the position of a dense grid offluid particles at the final instance and their preimages at the initialinstance of the prescribed time interval. As such, our framework can be readilyapplied to flows where such data is available through numerical simulations orexperimental measurements.',\n",
              " 'We consider the problem of large wave prediction in two-dimensional waterwaves. Such waves form due to the synergistic effect of dispersive mixing ofsmaller wave groups and the action of localized nonlinear wave interactionsthat leads to focusing. Instead of a direct simulation approach, we rely on thedecomposition of the wave field into a discrete set of localized wave groupswith optimal length scales and amplitudes. Due to the short-term character ofthe prediction, these wave groups do not interact and therefore their dynamicscan be characterized individually. Using direct numerical simulations of thegoverning envelope equations we precompute the expected maximum elevation foreach of those wave groups. The combination of the wave field decompositionalgorithm, which provides information about the statistics of the system, andthe precomputed map for the expected wave group elevation, which encodesdynamical information, allows (i) for understanding of how the probability ofoccurrence of rogue waves changes as the spectrum parameters vary, (ii) thecomputation of a critical length scale characterizing wave groups with highprobability of evolving to rogue waves, and (iii) the formulation of a robustand parsimonious reduced-order prediction scheme for large waves. We assess thevalidity of this scheme in several cases of ocean wave spectra.',\n",
              " 'Extreme events are ubiquitous in a wide range of dynamical systems, includingturbulent fluid flows, nonlinear waves, large scale networks and biologicalsystems. Here, we propose a variational framework for probing conditions thattrigger intermittent extreme events in high-dimensional nonlinear dynamicalsystems. We seek the triggers as the probabilistically feasible solutions of anappropriately constrained optimization problem, where the function to bemaximized is a system observable exhibiting intermittent extreme bursts. Theconstraints are imposed to ensure the physical admissibility of the optimalsolutions, i.e., significant probability for their occurrence under the naturalflow of the dynamical system. We apply the method to a body-forcedincompressible Navier--Stokes equation, known as the Kolmogorov flow. We findthat the intermittent bursts of the energy dissipation are independent of theexternal forcing and are instead caused by the spontaneous transfer of energyfrom large scales to the mean flow via nonlinear triad interactions. The globalmaximizer of the corresponding variational problem identifies the responsibletriad, hence providing a precursor for the occurrence of extreme dissipationevents. Specifically, monitoring the energy transfers within this triad, allowsus to develop a data-driven short-term predictor for the intermittent bursts ofenergy dissipation. We assess the performance of this predictor through directnumerical simulations.',\n",
              " 'The paper considers subspaces of the strictly upper triangular matrices,which are stable under Lie bracket with any upper triangular matrix. Thesesubspaces are called ad-nilpotent ideals and there are Catalan number of suchsubspaces. Each ad-nilpotent ideal meets a unique largest nilpotent orbit inthe Lie algebra of all matrices. The main result of the paper is that under anequivalence relation on ad-nilpotent ideals studied by Mizuno and others, theequivalence classes are the ad-nilpotent ideals with the same largest nilpotentorbit. We include two applications of the result, one to the higher vanishingof cohomology groups of vector bundles on the flag variety and another to theKazhdan-Lusztig cells in the affine Weyl group of the symmetric group. Finally,some combinatorial results are discussed.',\n",
              " \"We study the unit distance and distinct distances problems over the planarhypercomplex numbers: the dual numbers $\\\\mathbb{D}$ and the double numbers$\\\\mathbb{S}$. We show that the distinct distances problem in $\\\\mathbb{S}^2$behaves similarly to the original problem in $\\\\mathbb{R}^2$. The other threeproblems behave rather differently from their real analogs. We study thosethree problems by introducing various notions of multiplicity of a point set.Our analysis is based on studying the geometry of the dual plane and of thedouble plane. We also rely on classical results from discrete geometry, such asthe Szemer\\\\'edi-Trotter theorem.\",\n",
              " \"Scientific analysis often relies on the ability to make accurate predictionsof a system's dynamics. Mechanistic models, parameterized by a number ofunknown parameters, are often used for this purpose. Accurate estimation of themodel state and parameters prior to prediction is necessary, but may becomplicated by issues such as noisy data and uncertainty in parameters andinitial conditions. At the other end of the spectrum exist nonparametricmethods, which rely solely on data to build their predictions. While thesenonparametric methods do not require a model of the system, their performanceis strongly influenced by the amount and noisiness of the data. In thisarticle, we consider a hybrid approach to modeling and prediction which mergesrecent advancements in nonparametric analysis with standard parametric methods.The general idea is to replace a subset of a mechanistic model's equations withtheir corresponding nonparametric representations, resulting in a hybridmodeling and prediction scheme. Overall, we find that this hybrid approachallows for more robust parameter estimation and improved short-term predictionin situations where there is a large uncertainty in model parameters. Wedemonstrate these advantages in the classical Lorenz-63 chaotic system and innetworks of Hindmarsh-Rose neurons before application to experimentallycollected structured population data.\",\n",
              " 'In this paper we present a methodology that uses convolutional neuralnetworks (CNNs) for segmentation by iteratively growing predicted mask regionsin each coordinate direction. The CNN is used to predict class probabilityscores in a small neighborhood of the center pixel in a tile of an image. Weuse a threshold on the CNN probability scores to determine whether pixels areadded to the region and the iteration continues until no new pixels are addedto the region. Our method is able to achieve high segmentation accuracy andpreserve biologically realistic morphological features while leveraging smallamounts of training data and maintaining computational efficiency. Usingretinal blood vessel images from the DRIVE database we found that our method ismore accurate than a fully convolutional semantic segmentation CNN for severalevaluation metrics.',\n",
              " 'X-ray reflectivity and fluorescence near total reflection experiments wereperformed to examine the affinities of divalent ions ($\\\\mathrm{Ca^{2+}}$ and$\\\\mathrm{Ba^{2+}}$) from aqueous solution to a charged phosphatidic-acid (PA)surface. A phospholipid (1,2-Dimyristoyl-sn-Glycero-3-Phosphate, DMPA), spreadas a monolayer at the air/water interface, was used to form and control thecharge density at the interface. We find that for solutions of the pure salts(i.e., $\\\\mathrm{CaCl_{2}}$ and $\\\\mathrm{BaCl_{2}}$), the number of bound ionsper DMPA at the interface is saturated at concentrations that exceed$\\\\mathrm{10^{-3}M}$. For a 1:1 $\\\\mathrm{Ca^{2+}/Ba^{2+}}$ mixed solutions, wefind that the bound $\\\\mathrm{Ca^{2+}/Ba^{2+}}$ ratio at the interface is 4:1.If the only property determining charge accumulation near PA were the ioniccharges, the concentration of mixed $\\\\mathrm{Ca^{2+}/Ba^{2+}}$ at the interfacewould equal that of the bulk. Our results show a clear specific affinity of PAfor Ca compared to Ba. We provide some discussion on this issues as well assome implications for biological systems. Although our results indicate anexcess of counterion charge with respect to the surface charge, that is, chargeinversion, the analysis of both reflectivity and fluorescence do not revealexcess of co-ions (namely, $\\\\mathrm{Cl^{-}}$ or $\\\\mathrm{I}^{-}$).',\n",
              " \"Reaction diffusion equations have been used to model a wide range ofbiological phenomenon related to population spread and proliferation fromecology to cancer. It is commonly assumed that individuals in a population havehomogeneous diffusion and growth rates, however, this assumption can beinaccurate when the population is intrinsically divided into many distinctsubpopulations that compete with each other. In previous work, the task ofinferring the degree of phenotypic heterogeneity between subpopulations fromtotal population density has been performed within a framework that combinesparameter distribution estimation with reaction-diffusion models. Here, weextend this approach so that it is compatible with reaction-diffusion modelsthat include competition between subpopulations. We use a reaction-diffusionmodel of Glioblastoma multiforme, an aggressive type of brain cancer, to testour approach on simulated data that are similar to measurements that could becollected in practice. We use Prokhorov metric framework and convert thereaction-diffusion model to a random differential equation model to estimatejoint distributions of diffusion and growth rates among heterogeneoussubpopulations. We then compare the new random differential equation modelperformance against other partial differential equation models' performance. Wefind that the random differential equation is more capable at predicting thecell density compared to other models while being more time efficient. Finally,we use $k$-means clustering to predict the number of subpopulations based onthe recovered distributions.\",\n",
              " 'Plant phenotyping is typically a time-consuming and expensive endeavor,requiring large groups of researchers to meticulously measure biologicallyrelevant plant traits, and is the main bottleneck in understanding plantadaptation and the genetic architecture underlying complex traits at populationscale. In this work, we address these challenges by leveraging few-shotlearning with convolutional neural networks (CNNs) to segment the leaf body andvisible venation of 2,906 P. trichocarpa leaf images obtained in the field. Incontrast to previous methods, our approach (i) does not require experimental orimage pre-processing, (ii) uses the raw RGB images at full resolution, and(iii) requires very few samples for training (e.g., just eight images for veinsegmentation). Traits relating to leaf morphology and vein topology areextracted from the resulting segmentations using traditional open-sourceimage-processing tools, validated using real-world physical measurements, andused to conduct a genome-wide association study to identify genes controllingthe traits. In this way, the current work is designed to provide the plantphenotyping community with (i) methods for fast and accurate image-basedfeature extraction that require minimal training data, and (ii) a newpopulation-scale data set, including 68 different leaf phenotypes, for domainscientists and machine learning researchers. All of the few-shot learning code,data, and results are made publicly available.',\n",
              " 'We present methods for built-in test and calibration of phased arrays usingcode-modulated embedded test (CoMET). Our approach employs Cartesian modulationof test signals within each element using existing phase shifters, combining ofthese signals into an aggregate code-multiplexed response, downconversion andcreation of code-modulated element-to-element \"interference products\" using abuilt-in power detector, demodulation of correlations from the digitizedinterference response, and extraction of amplitude and phase per element usingan equation solver. Rotated-axis methodology is discussed for accurateextraction of phase near the original 0/90/180/270 degree axes. Our techniquesare demonstrated at board level for both receive and transmit modes using aneight-element 8-16 GHz phased array constructed using ADAR1000 chips from ADI.At 6 GHz, CoMET-extracted gain and phase are accurate to within 0.2 dB and 3degree compared to network-analyzer measurements. We then employ CoMET in acalibration loop to determinate optimum control settings at 6 GHz, outside the8-16 GHz band for which the array was designed. We achieve seven-bit phaseresolution with equalized gain. The root-mean squared gain and phase errors areimproved from 0.8 dB and 8 degree before calibration to 0.1 dB and 1.7 degreeafter calibration.',\n",
              " 'Due to ion-electron collisions, it is impossible to derive any two-fluidmodel for plasma as a direct hydrodynamic limit of the Vlasov-Poisson-Landausystem for ions and electrons. At the same time, electrons are much lighterthan their ion counterparts.  In this work, we derive the massless electron limit of theVlasov-Poisson-Landau system. This is done via a re-scaling of the electronvelocity, leading to multiple velocity scales. Importantly, we demonstrate thation-electron collisions vanish in this limit, due to special structure of theLandau collisions. We also show that this is invalid for the classicalBoltzmann kernel with hard sphere interaction. This mechanism serves as thefirst step for the derivation of the two-fluid model for ions from atwo-species kinetic equation.',\n",
              " 'We construct (modified) scattering operators for the Vlasov-Poisson system inthree dimensions, mapping small asymptotic dynamics as $t\\\\to -\\\\infty$ toasymptotic dynamics as $t\\\\to +\\\\infty$. The main novelty is the construction ofmodified wave operators, but we also obtain a new simple proof of modifiedscattering.  Our analysis is guided by the Hamiltonian structure of the Vlasov-Poissonsystem. Via a pseudo-conformal inversion we recast the question of asymptoticbehavior in terms of local in time dynamics of a new equation with singularcoefficients which is approximately integrated using a generating function.',\n",
              " 'Similarity scores in face recognition represent the proximity between pairsof images as computed by a matching algorithm. Given a large set of images andthe proximities between all pairs, a similarity score space is defined. Clusteranalysis was applied to the similarity score space to develop varioustaxonomies. Given the number of subjects in the dataset, we used hierarchicalmethods to aggregate images of the same subject. We also explored the hierarchyabove and below the subject level, including clusters that reflect gender andethnicity. Evidence supports the existence of clustering by race, gender,subject, and illumination condition.',\n",
              " 'Generative adversarial networks (GANs) are able to generate high resolutionphoto-realistic images of objects that \"do not exist.\" These synthetic imagesare rather difficult to detect as fake. However, the manner in which thesegenerative models are trained hints at a potential for information leakage fromthe supplied training data, especially in the context of synthetic faces. Thispaper presents experiments suggesting that identity information in face imagescan flow from the training corpus into synthetic samples without anyadversarial actions when building or using the existing model. This raisesprivacy-related questions, but also stimulates discussions of (a) the facemanifold\\'s characteristics in the feature space and (b) how to creategenerative models that do not inadvertently reveal identity information of realsubjects whose images were used for training. We used five different facematchers (face_recognition, FaceNet, ArcFace, SphereFace and NeurotechnologyMegaMatcher) and the StyleGAN2 synthesis model, and show that this identityleakage does exist for some, but not all methods. So, can we say that thesesynthetically generated faces truly do not exist? Databases of real andsynthetically generated faces are made available with this paper to allow fullreplicability of the results discussed in this work.',\n",
              " 'As image tampering becomes ever more sophisticated and commonplace, the needfor image forensics algorithms that can accurately and quickly detect forgeriesgrows. In this paper, we revisit the ideas of image querying and retrieval toprovide clues to better localize forgeries. We propose a method to performlarge-scale image forensics on the order of one million images using the helpof an image search algorithm and database to gather contextual clues as towhere tampering may have taken place. In this vein, we introduce five newstrongly invariant image comparison methods and test their effectiveness underheavy noise, rotation, and color space changes. Lastly, we show theeffectiveness of these methods compared to passive image forensics using Nimble[https://www.nist.gov/itl/iad/mig/nimble-challenge], a new, state-of-the-artdataset from the National Institute of Standards and Technology (NIST).',\n",
              " 'Deriving relationships between images and tracing back their history ofmodifications are at the core of Multimedia Phylogeny solutions, which aim tocombat misinformation through doctored visual media. Nonetheless, most recentimage phylogeny solutions cannot properly address cases of forged compositeimages with multiple donors, an area known as multiple parenting phylogeny(MPP). This paper presents a preliminary undirected graph construction solutionfor MPP, without any strict assumptions. The algorithm is underpinned by robustimage representative keypoints and different geometric consistency checks amongmatching regions in both images to provide regions of interest for directcomparison. The paper introduces a novel technique to geometrically filter themost promising matches as well as to aid in the shared region localizationtask. The strength of the approach is corroborated by experiments withreal-world cases, with and without image distractors (unrelated cases).',\n",
              " 'We analyze a simplistic model for run-and-tumble dynamics, motivated byobservations of complex spatio-temporal patterns in colonies of myxobacteria.In our model, agents run with fixed speed either left or right, and agents turnwith a density-dependent nonlinear turning rate, in addition to diffusiveBrownian motion. We show how a very simple nonlinearity in the turning rate canmediate the formation of self-organized stationary clusters and fronts.Phenomenologically, we demonstrate the formation of barriers, where highconcentrations of agents at the boundary of a cluster, moving towards thecenter of a cluster, prevent the agents caught in the cluster from escaping.Mathematically, we analyze stationary solutions in a four-dimensional ODE witha conserved quantity and a reversibility symmetry, using a combination ofbifurcation methods, geometric arguments, and numerical continuation. We alsopresent numerical results on the temporal stability of the solutions foundhere.',\n",
              " 'Remote photoplethysmography (rPPG) is a technique for estimating blood volumechanges from reflected light without the need for a contact sensor. We presentthe first examples of presentation attacks in the digital and physical domainson rPPG from face video. Digital attacks are easily performed by addingimperceptible periodic noise to the input videos. Physical attacks areperformed with illumination from visible spectrum LEDs placed in closeproximity to the face, while still being difficult to perceive with the humaneye. We also show that our attacks extend beyond medical applications, sincethe method can effectively generate a strong periodic pulse on 3D-printed facemasks, which presents difficulties for pulse-based face presentation attackdetection (PAD). The paper concludes with ideas for using this work to improverobustness of rPPG methods and pulse-based face PAD.',\n",
              " 'In this article, we analyze how changing the underlying 3D shape of the baseidentity in face images can distort their overall appearance, especially fromthe perspective of deep face recognition. As done in popular training dataaugmentation schemes, we graphically render real and synthetic face images withrandomly chosen or best-fitting 3D face models to generate novel views of thebase identity. We compare deep features generated from these images to assessthe perturbation these renderings introduce into the original identity. Weperform this analysis at various degrees of facial yaw with the base identitiesvarying in gender and ethnicity. Additionally, we investigate if adding someform of context and background pixels in these rendered images, when used astraining data, further improves the downstream performance of a facerecognition model. Our experiments demonstrate the significance of facial shapein accurate face matching and underpin the importance of contextual data fornetwork training.',\n",
              " 'Programming Language Processing (PLP) using machine learning has made vastimprovements in the past few years. Increasingly more people are interested inexploring this promising field. However, it is challenging for new researchersand developers to find the right components to construct their own machinelearning pipelines, given the diverse PLP tasks to be solved, the large numberof datasets and models being released, and the set of complex compilers ortools involved. To improve the findability, accessibility, interoperability andreusability (FAIRness) of machine learning components, we collect and analyze aset of representative papers in the domain of machine learning-based PLP. Wethen identify and characterize key concepts including PLP tasks, modelarchitectures and supportive tools. Finally, we show some example use cases ofleveraging the reusable components to construct machine learning pipelines tosolve a set of PLP tasks.',\n",
              " 'Camera-based physiological monitoring, especially remote photoplethysmography(rPPG), is a promising tool for health diagnostics, and state-of-the-art pulseestimators have shown impressive performance on benchmark datasets. We arguethat evaluations of modern solutions may be incomplete, as we uncover failurecases for videos without a live person, or in the presence of severe noise. Wedemonstrate that spatiotemporal deep learning models trained only with livesamples \"hallucinate\" a genuine-shaped pulse on anomalous and noisy videos,which may have negative consequences when rPPG models are used by medicalpersonnel. To address this, we offer: (a) An anomaly detection model, built ontop of the predicted waveforms. We compare models trained in open-set (unknownabnormal predictions) and closed-set (abnormal predictions known when training)settings; (b) An anomaly-aware training regime that penalizes the model forpredicting periodic signals from anomalous videos. Extensive experimentationwith eight research datasets (rPPG-specific: DDPM, CDDPM, PURE, UBFC, ARPM;deep fakes: DFDC; face presentation attack detection: HKBU-MARs; rPPG outlier:KITTI) show better accuracy of anomaly detection for deep learning modelsincorporating the proposed training (75.8%), compared to models trainedregularly (73.7%) and to hand-crafted rPPG methods (52-62%).',\n",
              " 'Subtle periodic signals such as blood volume pulse and respiration can beextracted from RGB video, enabling remote health monitoring at low cost.Advancements in remote pulse estimation -- or remote photoplethysmography(rPPG) -- are currently driven by deep learning solutions. However, modernapproaches are trained and evaluated on benchmark datasets with associatedground truth from contact-PPG sensors. We present the first non-contrastiveunsupervised learning framework for signal regression to break free from theconstraints of labelled video data. With minimal assumptions of periodicity andfinite bandwidth, our approach is capable of discovering the blood volume pulsedirectly from unlabelled videos. We find that encouraging sparse power spectrawithin normal physiological bandlimits and variance over batches of powerspectra is sufficient for learning visual features of periodic signals. Weperform the first experiments utilizing unlabelled video data not specificallycreated for rPPG to train robust pulse rate estimators. Given the limitedinductive biases and impressive empirical results, the approach istheoretically capable of discovering other periodic signals from video,enabling multiple physiological measurements without the need for ground truthsignals. Codes to fully reproduce the experiments are made available along withthe paper.',\n",
              " \"Remote Photoplethysmography (rPPG), or the remote monitoring of a subject'sheart rate using a camera, has seen a shift from handcrafted techniques to deeplearning models. While current solutions offer substantial performance gains,we show that these models tend to learn a bias to pulse wave features inherentto the training dataset. We develop augmentations to mitigate this learned biasby expanding both the range and variability of heart rates that the model seeswhile training, resulting in improved model convergence when training andcross-dataset generalization at test time. Through a 3-way cross datasetanalysis we demonstrate a reduction in mean absolute error from over 13 beatsper minute to below 3 beats per minute. We compare our method with other recentrPPG systems, finding similar performance under a variety of evaluationparameters.\",\n",
              " 'Generative Adversarial Networks (GANs) have proven to be a preferred methodof synthesizing fake images of objects, such as faces, animals, andautomobiles. It is not surprising these models can also generate ISO-compliant,yet synthetic iris images, which can be used to augment training data for irismatchers and liveness detectors. In this work, we trained one of the mostrecent GAN models (StyleGAN3) to generate fake iris images with two primarygoals: (i) to understand the GAN\\'s ability to produce \"never-before-seen\"irises, and (ii) to investigate the phenomenon of identity leakage as afunction of the GAN\\'s training time. Previous work has shown that personalbiometric data can inadvertently flow from training data into syntheticsamples, raising a privacy concern for subjects who accidentally appear in thetraining dataset. This paper presents analysis for three different irismatchers at varying points in the GAN training process to diagnose where andwhen authentic training samples are in jeopardy of leaking through thegenerative process. Our results show that while most synthetic samples do notshow signs of identity leakage, a handful of generated samples match authentic(training) samples nearly perfectly, with consensus across all matchers. Inorder to prioritize privacy, security, and trust in the machine learning modeldevelopment process, the research community must strike a delicate balancebetween the benefits of using synthetic data and the corresponding threatsagainst privacy from potential identity leakage.',\n",
              " 'Face recognition performance has improved remarkably in the last decade. Muchof this success can be attributed to the development of deep learningtechniques such as convolutional neural networks (CNNs). While CNNs have pushedthe state-of-the-art forward, their training process requires a large amount ofclean and correctly labelled training data. If a CNN is intended to toleratefacial pose, then we face an important question: should this training data bediverse in its pose distribution, or should face images be normalized to asingle pose in a pre-processing step? To address this question, we evaluate anumber of popular facial landmarking and pose correction algorithms tounderstand their effect on facial recognition performance. Additionally, weintroduce a new, automatic, single-image frontalization scheme that exceeds theperformance of current algorithms. CNNs trained using sets of differentpre-processing methods are used to extract features from the Point and ShootChallenge (PaSC) and CMU Multi-PIE datasets. We assert that the subsequentverification and recognition performance serves to quantify the effectivenessof each pose correction scheme.',\n",
              " 'Departing from traditional digital forensics modeling, which seeks to analyzesingle objects in isolation, multimedia phylogeny analyzes the evolutionaryprocesses that influence digital objects and collections over time. One of itsintegral pieces is provenance filtering, which consists of searching apotentially large pool of objects for the most related ones with respect to agiven query, in terms of possible ancestors (donors or contributors) anddescendants. In this paper, we propose a two-tiered provenance filteringapproach to find all the potential images that might have contributed to thecreation process of a given query $q$. In our solution, the first (coarse) tieraims to find the most likely \"host\" images --- the major donor or background--- contributing to a composite/doctored image. The search is then refined inthe second tier, in which we search for more specific (potentially small) partsof the query that might have been extracted from other images and spliced intothe query image. Experimental results with a dataset containing more than amillion images show that the two-tiered solution underpinned by the context ofthe query is highly useful for solving this difficult task.',\n",
              " 'Design rule checking (DRC) is getting increasingly complex in advanced nodestechnologies. It would be highly desirable to have a fast interactive DRCengine that could be used during layout. In this work, we establish the proofof feasibility for such an engine. The proposed model consists of aconvolutional neural network (CNN) trained to detect DRC violations. The modelwas trained with artificial data that was derived from a set of $50$ SRAMdesigns. The focus in this demonstration was metal 1 rules. Using thissolution, we can detect multiple DRC violations 32x faster than Booleancheckers with an accuracy of up to 92. The proposed solution can be easilyexpanded to a complete rule set.',\n",
              " 'Understanding how matter behaves at the highest densities and temperatures isa major open problem in both nuclear physics and relativistic astrophysics.This physics is often encapsulated in the so-called high-temperature nuclearequation of state, which influences compact binary mergers, core-collapsesupernovae, and many more phenomena. One such case is the type (either blackhole or neutron star) and mass of the remnant of the core collapse of a massivestar. For each of six candidate equations of state, we use a very large suiteof spherically symmetric supernova models to generate a suite of syntheticpopulations of such remnants. We then compare these synthetic populations tothe observed remnant population. We thus provide a novel constraint on thehigh-temperature nuclear equation of state and describe which EOS candidatesare more or less favored by this metric.',\n",
              " 'Along with binary neutron star mergers, the in-spiral and merger of a blackhole and a neutron star is a predicted site of $r$-process nucleosynthesis andassociated kilonovae. For the right mass ratio, very large amounts of neutronrich material may become unbound from the post-merger accretion disk. Wesimulate a suite of four post-merger disks with full-transport generalrelativistic neutrino radiation magnetohydrodynamics. We find that the outflowsfrom these disks are very close to the threshold conditions for robust$r$-process nucleosynthesis. For these conditions, the detailed properties ofthe outflow determine whether a full $r$-process can or cannot occur, implyingthat a wide range of observable phenomena are possible. We show that on averagethe disk outflow lanthanide fraction is suppressed relative to the solarisotopic pattern. In combination with the dynamical ejecta, these outflowsimply a kilonova with both blue and red components.',\n",
              " 'With an increasing number of superluminous supernovae (SLSNe) discovered thequestion of their origin remains open and causes heated debates in thesupernova community. Currently, there are three proposed mechanisms for SLSNe:(1) pair-instability supernovae (PISN), (2) magnetar-driven supernovae, and (3)models in which the supernova ejecta interacts with a circumstellar materialejected before the explosion. Based on current observations of SLSNe, the PISNorigin has been disfavoured for a number of reasons. Many PISN models provideoverly broad light curves and too reddened spectra, because of massive ejectaand a high amount of nickel. In the current study we re-examine PISN propertiesusing progenitor models computed with the GENEC code. We calculate supernovaexplosions with FLASH and light curve evolution with the radiationhydrodynamics code STELLA. We find that high-mass models (200 and 250 solarmasses) at relatively high metallicity (Z=0.001) do not retain hydrogen in theouter layers and produce relatively fast evolving PISNe Type I and might besuitable to explain some SLSNe. We also investigate uncertainties in lightcurve modelling due to codes, opacities, the nickel-bubble effect andprogenitor structure and composition.',\n",
              " 'The successful transition from core-collapse supernova simulations usingclassical neutrino transport to simulations using quantum neutrino transportwill require the development of methods for calculating neutrino flavortransformations that mitigate the computational expense. One potential approachis the use of angular moments of the neutrino field, which has the added appealthat there already exist simulation codes which make use of moments forclassical neutrino transport. Evolution equations for quantum moments based onthe quantum kinetic equations can be straightforwardly generalized from theevolution of classical moments based on the Boltzmann equation. We present anefficient implementation of neutrino transformation using quantum angularmoments in the free streaming, spherically symmetric bulb model. We compare theresults against analytic solutions and the results from more exact multi-angleneutrino flavor evolution calculations. We find that our moment-based methodsemploying scalar closures predict, with good accuracy, the onset of collectiveflavor transformations seen in the multi-angle results. However in somesituations they overestimate the coherence of neutrinos traveling alongdifferent trajectories. More sophisticated quantum closures may improve theagreement between the inexpensive moment-based methods and the multi-angleapproach.',\n",
              " 'Core-collapse supernovae are predicted to produce gravitational waves (GWs)that may be detectable by Advanced LIGO/Virgo. These GW signals carryinformation from the heart of these catacylsmic events, where matter reachesnuclear densities. Recent studies have shown that it may be possible to inferproperties of the proto-neutron star (PNS) via gravitational waves generated byhydrodynamic perturbations of the PNS. However, we lack a comprehensiveunderstanding of how these relationships may change with the properties ofcore-collapse supernovae. In this work, we build a self-consistent suite ofover 1000 exploding core-collapse supernovae from a grid of progenitor massesand metallicities combined with six different nuclear equations of state.Performing a linear perturbation analysis on each model, we compute theresonant gravitational-wave frequencies of the PNS, and we motivate atime-agnostic method for identifying characteristic frequencies of the dominantgravitational-wave emission. From this, we identify two characteristicfrequencies, of the early- and late-time signal, that measure the surfacegravity of the cold remnant neutron star, and simultaneously constrain the hotnuclear equation of state. However, we find that the details of thecore-collapse supernova model, such as the treatment of gravity or the neutrinotransport, and whether it explodes, noticeably change the magnitude andevolution of the PNS eigenfrequencies.',\n",
              " \"The youngest Galactic supernova remnant G1.9+0.3, probably the result of aType Ia supernova, shows surprising anomalies in the distribution of its ejectain space and velocity. In particular, high-velocity shocked iron is seen inseveral locations far from the remnant center, in some cases beyond prominentsilicon and sulfur emission. These asymmetries strongly suggest a highlyasymmetric explosion. We present high-resolution hydrodynamic simulations intwo and three dimensions of the evolution from ages of 100 seconds to hundredsof years of two asymmetric Type Ia models, expanding into a uniform medium. Atthe age of G1.9+0.3 (about 100 years), our 2D model shows almost no ironshocked to become visible in X-rays. Only in a much higher-density environmentcould significant iron be shocked, at which time the model's expansion speed iscompletely inconsistent with the observations of G1.9+0.3. Our 3D model,evolving the most asymmetric of a suite of Type Ia SN models from Seitenzahl etal.~(2013), shows some features resembling G1.9+0.3. We characterize itsevolution with images of composition in three classes: C and O,intermediate-mass elements (IMEs), and iron-group elements (IGEs). From ages of13 to 1800 years, we follow the evolution of the highly asymmetric initialremnant as the explosion asymmetries decrease in relative strength to bereplaced by asymmetries due to evolutionary hydrodynamic instabilities. At anage of about 100 years, our 3D model has comparable shocked masses of C+O,IMEs, and IGEs, with about 0.03 $M_\\\\odot$ each. Evolutionary changes appear tobe rapid enough that continued monitoring with the Chandra X-ray Observatorymay show significant variations.\",\n",
              " \"The nineteenth century was an important period for both Oxford mathematicsand algebra in general. While there is extensive documentation of mathematicalresearch in Oxford at this time, the same cannot be said of the teaching. Thecontent of the course presents a different picture: it shows what those who setit felt was most valuable for a young mathematician to learn, perhapsindicating what direction they expected mathematics to take in the future. Tofind out what undergraduates were taught, I have looked through examinationpapers of the years between 1828 and 1912 with a focus on algebra, as well assupporting material. In this paper I will present my findings. I will give apicture of what an Oxford undergraduate's course in algebra looked like by theend of the nineteenth century and discuss my own conclusions as to why it tooksuch a form.\",\n",
              " \"My thesis describes the life and work of the mathematician Major PercyAlexander MacMahon (1854-1929). His early life as as a soldier in the RoyalArtillery and the events which led to him embarking on a career in mathematicalresearch and teaching are dealt with in the first two chapters. Succeedingchapters explain the work in invariant theory and partition theory whichbrought him to the attention of the British mathematical community andeventually resulted in a Fellowship of the Royal Society, the presidency of theLondon Mathematical Society, and the award of three prestigious mathematicalmedals and four honorary doctorates. The development and importance of hisrecreational mathematical work is traced and discussed. MacMahon's career inthe Civil Service as Deputy Warden of the Standards at the Board of Trade isalso described. Throughout the thesis, his involvement with the BritishAssociation for the Advancement of Science and other scientific organisationsis highlighted. The thesis also examines possible reasons why MacMahon's work,held in very high regard at the time, did not lead to the lasting fame accordedto some of his contemporaries. Details of his personal and social life areincluded to give a picture of MacMahon as a real person working hard to succeedin a difficult context.\",\n",
              " \"We present an efficient implementation of a surface Green's-function methodfor atomistic modeling of surfaces within the framework of density functionaltheory using a pseudopotential localized basis set approach. In this method,the system is described as a truly semi-infinite solid with a surface regioncoupled to an electron reservoir, thereby overcoming several fundamentaldrawbacks of the traditional slab approach. The versatility of the method isdemonstrated with several applications to surface physics and chemistryproblems that are inherently difficult to address properly with the slabmethod, including metal work function calculations, band alignment in thin-filmsemiconductor heterostructures, surface states in metals and topologicalinsulators, and surfaces in external electrical fields. Results obtained withthe surface Green's-function method are compared to experimental measurementsand slab calculations to demonstrate the accuracy of the approach.\",\n",
              " 'We consider a Bayesian method for simultaneous quantile regression on a realvariable. By monotone transformation, we can make both the response variableand the predictor variable take values in the unit interval. A representationof quantile function is given by a convex combination of two monotoneincreasing functions $\\\\xi_1$ and $\\\\xi_2$ not depending on the predictionvariables. In a Bayesian approach, a prior is put on quantile functions byputting prior distributions on $\\\\xi_1$ and $\\\\xi_2$. The monotonicity constrainton the curves $\\\\xi_1$ and $\\\\xi_2$ are obtained through a spline basis expansionwith coefficients increasing and lying in the unit interval. We put a Dirichletprior distribution on the spacings of the coefficient vector. A finite randomseries based on splines obeys the shape restrictions. We compare our approachwith a Bayesian method using Gaussian process prior through an extensivesimulation study and some other Bayesian approaches proposed in the literature.An application to a data on hurricane activities in the Atlantic region isgiven. We also apply our method on region-wise population data of USA for theperiod 1985--2010.',\n",
              " 'Delayed target response in synthetic aperture radar (SAR) imaging can beobscured by the range-delay ambiguity and speckle. To analyze the range-delayambiguity, one extends the standard SAR formulation and allows both the targetreflectivity and the image to depend not only on the coordinates, but also onthe response delay. However, this still leaves the speckle unaccounted for. Yetspeckle is commonly found in SAR images of extended targets, and a statisticalapproach is usually employed to describe it. We have developed a simple modelof a delayed scatterer by modifying the random function that describes ahomogeneous extended scatterer. Our model allows us to obtain a relationbetween the deterministic parameters of the target model and statisticalmoments of the SAR image. We assume a regular shape of the antenna trajectory,and our model targets are localized in at least one space-time coordinate; thispermits analytical formulation for statistical moments of the image. Theproblem of reconstruction of coordinate-delay reflectivity function is reducedto that of discrimination between instantaneous and delayed scatterers; for thelatter problem, the maximum likelihood based image processing procedure hasbeen developed. We perform Monte-Carlo simulation and evaluate performance ofthe classification procedure for a simple dependence of scatterer reflectivityon the delay time.',\n",
              " 'Distinguishing between the instantaneous and delayed scatterers in syntheticaperture radar (SAR) images is important for target identification andcharacterization. To perform this task, one can use the autocorrelationanalysis of coordinate-delay images. However, due to the range-delay ambiguitythe difference in the correlation properties between the instantaneous anddelayed targets may be small. Moreover, the reliability of discrimination isaffected by speckle, which is ubiquitous in SAR images, and requiresstatistical treatment.  Previously, we have developed a maximum likelihood based approach fordiscriminating between the instantaneous and delayed targets in SAR images. Totest it, we employed simple statistical models. They allowed us to simulateensembles of images that depend on various parameters, including aperture widthand target contrast.  In the current paper, we enhance our previously developed methodology byestablishing confidence levels for the discrimination between the instantaneousand delayed scatterers. Our procedure takes into account the difference inthresholds for different target contrasts without making any assumptions aboutthe statistics of those contrasts.',\n",
              " 'The present Chapter discusses methods by which topological Bloch bands can beprepared in cold-atom setups. Focusing on the case of Chern bands fortwo-dimensional systems, we describe how topological properties can betriggered by driving atomic gases, either by dressing internal levels withlight or through time-periodic modulations. We illustrate these methods withconcrete examples, and we discuss recent experiments where geometrical andtopological band properties have been identified.',\n",
              " 'We propose a realistic scheme to detect topological edge states in an opticallattice subjected to a synthetic magnetic field, based on a generalization ofBragg spectroscopy sensitive to angular momentum. We demonstrate that using awell-designed laser probe, the Bragg spectra provide an unambiguous signatureof the topological edge states that establishes their chiral nature. Thissignature is present for a variety of boundaries, from a hard wall to a smoothharmonic potential added on top of the optical lattice. Experimentally, theBragg signal should be very weak. To make it detectable, we introduce a\"shelving method\", based on Raman transitions, which transfers angular momentumand changes the internal atomic state simultaneously. This scheme allows todetect the weak signal from the selected edge states on a dark background, anddrastically improves the detectivity. It also leads to the possibility todirectly visualize the topological edge states, using in situ imaging, offeringa unique and instructive view on topological insulating phases.',\n",
              " 'We investigate the properties of a two-dimensional quasicrystal in thepresence of a uniform magnetic field. In this configuration, the density ofstates (DOS) displays a Hofstadter butterfly-like structure when it isrepresented as a function of the magnetic flux per tile. We show that thelow-DOS regions of the energy spectrum are associated with chiral edge states,in direct analogy with the Chern insulators realized with periodic lattices. Weestablish the topological nature of the edge states by computing thetopological Chern number associated with the bulk of the quasicrystal. Thistopological characterization of the non-periodic lattice is achieved through alocal (real-space) topological marker. This work opens a route for theexploration of topological insulating materials in a wide range of non-periodiclattice systems, including photonic crystals and cold atoms in opticallattices.',\n",
              " 'The dissipative response of a quantum system upon a time-dependent drive canbe exploited as a probe of its geometric and topological properties. In thiswork, we explore the implications of such phenomena in the context oftwo-dimensional gases subjected to a uniform magnetic field. It is shown that afilled Landau level exhibits a quantized circular dichroism, which can betraced back to its underlying non-trivial topology. Based on selection rules,we find that this quantized circular dichroism can be suitably described interms of Rabi oscillations, whose frequencies satisfy simple quantization laws.Moreover, we discuss how these quantized dissipative responses can be probedlocally, both in the bulk and at the boundaries of the quantum Hall system.This work suggests alternative forms of topological probes in quantum systemsbased on circular dichroism.',\n",
              " \"Subjecting a physical system to a time-periodic drive can substantiallymodify its properties and applications. This Floquet-engineering approach hasbeen extensively applied to a wide range of classical and quantum settings inview of designing synthetic systems with exotic properties. Considering ageneral class of two-mode nonlinear optical devices, we show that effectiveoptical nonlinearities can be created by subjecting the light field to arepeated pulse sequence, which couples the two modes in a fast andtime-periodic manner. The strength of these drive-induced opticalnonlinearities, which include an emerging four-wave mixing, can be varied bysimply adjusting the pulse sequence. This leads to topological changes in thesystem's phase space, which can be detected through light intensity and phasemeasurements. Our proposal builds on an effective-Hamiltonian approach, whichderives from a parent quantum many-body Hamiltonian describing driveninteracting bosons. As a corollary, our results equally apply to Bose-Einsteincondensates in driven double-well potentials, where pair tunneling effectivelyarises from the periodic pulse sequence. Our scheme offers a practical route toengineer and finely tune exotic nonlinearities and interactions in photonicsand ultracold quantum gases.\",\n",
              " 'We recently proposed in a Letter [Physical Review Letters 108 255303] a novelscheme to detect topological edge states in an optical lattice, based on ageneralization of Bragg spectroscopy. The scope of the present article is toprovide a more detailed and pedagogical description of the system - theHofstadter optical lattice - and probing method. We first show the existence oftopological edge states, in an ultra-cold gas trapped in a 2D optical latticeand subjected to a synthetic magnetic field. The remarkable robustness of theedge states is verified for a variety of external confining potentials. Then,we describe a specific laser probe, made from two lasers in Laguerre-Gaussianmodes, which captures unambiguous signatures of these edge states. Inparticular, the resulting Bragg spectra provide the dispersion relation of theedge states, establishing their chiral nature. In order to make the Braggsignal experimentally detectable, we introduce a \"shelving method\", whichsimultaneously transfers angular momentum and changes the internal atomicstate. This scheme allows to directly visualize the selected edge states on adark background, offering an instructive view on topological insulating phases,not accessible in solid-state experiments.',\n",
              " 'We introduce a scheme by which flat bands with higher Chern number $\\\\vertC\\\\vert>1$ can be designed in ultracold gases through a coherent manipulation ofBloch bands. Inspired by quantum-optics methods, our approach consists increating a \"dark Bloch band\" by coupling a set of source bands through resonantprocesses. Considering a $\\\\Lambda$ system of three bands, the Chern number ofthe dark band is found to follow a simple sum rule in terms of the Chernnumbers of the source bands: $C_D\\\\!=\\\\!C_1+C_2-C_3$. Altogether, our dark-statescheme realizes a nearly flat Bloch band with predictable and tunable Chernnumber $C_D$. We illustrate our method based on a $\\\\Lambda$ system, formed ofthe bands of the Harper-Hofstadter model, which leads to a nearly flat Chernband with $C_D\\\\!=\\\\!2$. We explore a realistic sequence to load atoms into thedark Chern band, as well as a probing scheme based on Hall drift measurements.Dark Chern bands offer a practical platform where exotic fractional quantumHall states could be realized in ultracold gases.',\n",
              " 'The usual approach to considerations of apin relaxation and frequency shiftsdue to fluctuating fields is through the density matrix Slichter. Here we treatthe problem of the influence of fluctuating fields on a spin 1/2 system basedon direct solution of the Schroedinger equation in contrast to the usualtreatment. Our results are seen to be in agreement with the known results inthe literature McGregor, Slichter, Red2, CSH, as they must, but our derivationdirectly from the Schroedinger equation allows us to see the role of thenecessary assumptions in a somewhat clearer way.',\n",
              " 'Many experiments utilize the precession of trapped particles in magneticfields to perform high precision measurements. It had been presumed that afterfree precession, initially polarized particles will form a Gaussian phasedistribution in the plane of precession. We show that trapped particles in thepresence of magnetic field gradients and electric fields will often form anon-Gaussian distribution with power-law tails which are consistent withnonextensive statistics. As the exact shape of the distribution depends uponmany experimental parameters, it provides a potential new technique to directlymeasure them.',\n",
              " \"A relativistic particle undergoing successive boosts which are non collinearwill experience a rotation of its coordinate axes with respect to the boostedframe. This rotation of coordinate axes is caused by a relativistic phenomenoncalled Thomas Rotation. We assess the importance of Thomas rotation in thecalculation of physical quantities like electromagnetic fields in therelativistic regime. We calculate the electromagnetic field tensor for generalthree dimensional successive boosts in the particle's rest frame as well as thelaboratory frame. We then compare the electromagnetic field tensors obtained bya direct boost $\\\\vec{\\\\beta} + \\\\delta \\\\vec{\\\\beta}$ and successive boosts$\\\\vec{\\\\beta}$ and $\\\\Delta \\\\vec{\\\\beta}$ and check their consistency with Thomasrotation. This framework might be important to situations such as thecalculation of frequency shifts for relativistic spin-1/2 particles undergoingLarmor precession in electromagnetic fields with small field non-uniformities.\",\n",
              " \"The behavior of a spin undergoing Larmor precession in the presence offluctuating fields is of interest to workers in many fields. The fluctuatingfields cause frequency shifts and relaxation which are related to their powerspectrum, which can be determined by taking the Fourier transform of theauto-correlation functions of the field fluctuations. Recently we have shownhow to calculate these correlation functions for all values of mean free path(ballistic to diffusive motion) in finite bounded regions, using the model ofpersistent continuous time random walks (CTRW) for particles subject toscattering by fixed (frozen) scattering centers so that the speed of the movingparticles is not changed by the collisions. In this work we show how scatteringwith energy exchange from an ensemble of scatterers in thermal equilibrium canbe incorporated into the CTRW. We present results for 1,2 and 3 dimensions. Theresults agree for all these cases contrary to the previously studied 'frozen'models. Our results for the velocity autocorrelation function show a long timetail $\\\\left( \\\\sim t^{-1/2}\\\\right) $, which we also obtain from conventionaldiffusion theory, with the same power, independent of dimensionality. Ourresults are valid for any Markovian scattering kernel as well as any kernelbased on a scattering cross section $\\\\sim1/v.$\",\n",
              " 'A low-order method is presented for aerodynamic prediction of wings operatingat near-stall and post-stall flight conditions. The method is intended for usein design, modeling, and simulation. In this method, the flow separation due tostall is modeled in a vortex-lattice framework as an effective reduction in thecamber, or \"decambering.\" For each section of the wing, a parabolic decamberingflap, hinged at the separation location of the section, is calculated throughiteration to ensure that the lift and moment coefficients of the section matchwith the values from the two-dimensional viscous input curves for the effectiveangle of attack of the section. As an improvement from earlier low-ordermethods, this method also predicts the separation pattern on the wing. Resultsfrom the method, presented for unswept wings having various airfoils, aspectratios, taper ratios, and small, quasi-steady roll rates, are shown to agreewell with experimental results in the literature, and computational solutionsobtained as part of the current work.',\n",
              " 'Kinematic measurements of two simultaneous coordinates from postural swayduring quiet standing were performed employing multiple ultrasonic transducers.The use of accurate acoustic devices was required for the detection of thesmall random noise displacements. The trajectory in the anteroposterior -mediolateral plane of human chest was measured and compared with the trajectoryin anteroposterior direction from the upper and lower body. The latter wasstatistically analyzed and appeared to be strongly anti-correlated. Theanti-correlations represent strong evidence for the dominance of hip strategyduring an unperturbed one minute stance. That the hip strategy, normallyobserved for large amplitude motions, also appears in the small amplitude of aquite stance, indicates the utility of such noise measurements for exploringthe biomechanics of human balance.',\n",
              " 'We present new algorithms for the randomized construction of hierarchicallysemi-separable matrices, addressing several practical issues. The HSSconstruction algorithms use a partially matrix-free, adaptive randomizedprojection scheme to determine the maximum off-diagonal block rank. We developboth relative and absolute stopping criteria to determine the minimum dimensionof the random projection matrix that is sufficient for the desired accuracy.Two strategies are discussed to adaptively enlarge the random sample matrix:repeated doubling of the number of random vectors, and iteratively incrementingthe number of random vectors by a fixed number. The relative and absolutestopping criteria are based on probabilistic bounds for the Frobenius norm ofthe random projection of the Hankel blocks of the input matrix. We discussparallel implementation and computation and communication cost of bothvariants. Parallel numerical results for a range of applications, includingboundary element method matrices and quantum chemistry Toeplitz matrices, showthe effectiveness, scalability and numerical robustness of the proposedalgorithms.',\n",
              " 'The spread of synthetic gene drives is often discussed in the context ofpanmictic populations connected by gene flow and described with simpledeterministic models. Under such assumptions, an entire species could bealtered by releasing a single individual carrying an invasive gene drive, suchas a standard homing drive. While this remains a theoretical possibility, genedrive spread in natural populations is more complex and merits a more realisticassessment. The fate of any gene drive released in a population would beinextricably linked to the ecology of the population. Given the uncertaintyoften involved in ecological assessment of natural populations, understandingthe sensitivity of gene drive spread to important ecological factors iscritical. Here we review how different forms of density-dependence, spatialheterogeneity and mating behaviors can impact the spread of self-sustaininggene drives. We highlight specific aspects of gene drive dynamics and thetarget populations that need further research.',\n",
              " 'We introduce a convolutional neural network for inferring a compactdisentangled graphical description of objects from 2D images that can be usedfor volumetric reconstruction. The network comprises an encoder and atwin-tailed decoder. The encoder generates a disentangled graphics code. Thefirst decoder generates a volume, and the second decoder reconstructs the inputimage using a novel training regime that allows the graphics code to learn aseparate representation of the 3D object and a description of its lighting andpose conditions. We demonstrate this method by generating volumes anddisentangled graphical descriptions from images and videos of faces and chairs.',\n",
              " 'Judgments about personality based on facial appearance are strong effectorsin social decision making, and are known to have impact on areas frompresidential elections to jury decisions. Recent work has shown that it ispossible to predict perception of memorability, trustworthiness, intelligenceand other attributes in human face images. The most successful of theseapproaches require face images expertly annotated with key facial landmarks. Wedemonstrate a Convolutional Neural Network (CNN) model that is able to performthe same task without the need for landmark features, thereby greatlyincreasing efficiency. The model has high accuracy, surpassing human-levelperformance in some cases. Furthermore, we use a deconvolutional approach tovisualize important features for perception of 22 attributes and demonstrate anew method for separately visualizing positive and negative features.',\n",
              " 'Quantum circuits with hierarchical structure have been used to perform binaryclassification of classical data encoded in a quantum state. We demonstratethat more expressive circuits in the same family achieve better accuracy andcan be used to classify highly entangled quantum states, for which there is noknown efficient classical method. We compare performance for several differentparameterizations on two classical machine learning datasets, Iris and MNIST,and on a synthetic dataset of quantum states. Finally, we demonstrate thatperformance is robust to noise and deploy an Iris dataset classifier on theibmqx4 quantum computer.',\n",
              " 'Parametrized quantum circuits initialized with random initial parametervalues are characterized by barren plateaus where the gradient becomesexponentially small in the number of qubits. In this technical note wetheoretically motivate and empirically validate an initialization strategywhich can resolve the barren plateau problem for practical applications. Thetechnique involves randomly selecting some of the initial parameter values,then choosing the remaining values so that the circuit is a sequence of shallowblocks that each evaluates to the identity. This initialization limits theeffective depth of the circuits used to calculate the first parameter update sothat they cannot be stuck in a barren plateau at the start of training. Inturn, this makes some of the most compact ans\\\\\"atze usable in practice, whichwas not possible before even for rather basic problems. We show empiricallythat variational quantum eigensolvers and quantum neural networks initializedusing this strategy can be trained using a gradient based method.',\n",
              " 'An ellipsoidal volume of Rydberg molecules, entrained in a supersonicmolecular beam, evolves on a nanosecond timescale to form a strongly coupledultracold plasma. We present coupled rate-equation simulations that model theunderlying kinetic processes and molecular dissociation channels in both auniformly distributed plasma and under the conditions dictated by ourexperimental geometry. Simulations predict a fast electron-driven collisionalavalanche to plasma followed by slow electron-ion recombination. Within 20$\\\\mu$s, release of Rydberg binding energy raises the electron temperature of astatic plasma to $T_e = 100$ K. Providing for a quasi-self-similar expansion,the hot electron gas drives ion radial motion, reducing $T_e$. Thesesimulations provide a classical baseline model from which to consider quantumeffects in the evolution of charge gradients and ambipolar forces in anexperimental system undergoing responsive avalanche dynamics.',\n",
              " 'Specialized applications of nanoparticles often call for particular,well-characterized particle size distributions in solution. But, this propertycan prove difficult to measure. High-throughput methods, such as dynamic lightscattering, detect nanoparticles in solution with an efficiency that scaleswith diameter to the sixth power. This diminishes the accuracy of anydetermination that must span a range of particle sizes. The accurateclassification of broadly distributed systems thus requires very large numbersof measurements. Mass-filtered particle-sensing techniques offer a betterdynamic range, but are labor-intensive and so have low throughput. Progress inmany areas of nanotechnology requires a faster, lower-cost, and more accuratemeasure of particle size distributions, particularly for diameters smaller than20 nm. Here, we present a tailored interferometric microscope system, combinedwith a high-speed image-processing strategy, optimized for real-time particletracking that determines accurate size distributions in nominal 5, 10, and 15nm colloidal gold nanoparticle systems by automatically sensing and classifyingthousands of single particles sampled from solution at rates as high as 4,000particles per minute. We demonstrate this method by sensing the irreversiblebinding of gold nanoparticles to poly-D-lysine functionalized coverslips.Variations in the single-particle signal as a function of time and mass,calibrated by TEM, show clear evidence for the presence of diffusion-limitedtransport that most affects larger particles in solution.',\n",
              " 'We argue that the quenched ultracold plasma presents an experimental platformfor studying quantum many-body physics of disordered systems in the long-timeand finite energy-density limits. We consider an experiment that quenches aplasma of nitric oxide to an ultracold system of Rydberg molecules, ions andelectrons that exhibits a long-lived state of arrested relaxation. Thequalitative features of this state fail to conform with classical models. Here,we develop a microscopic quantum description for the arrested phase based on aneffective many-body spin Hamiltonian that includes both dipole-dipole and vander Waals interactions. This effective model appears to offer a way to envisionthe essential quantum disordered non-equilibrium physics of this system.',\n",
              " 'Adversarial learning is one of the most successful approaches to modellinghigh-dimensional probability distributions from data. The quantum computingcommunity has recently begun to generalize this idea and to look for potentialapplications. In this work, we derive an adversarial algorithm for the problemof approximating an unknown quantum pure state. Although this could be done onuniversal quantum computers, the adversarial formulation enables us to executethe algorithm on near-term quantum computers. Two parametrized circuits areoptimized in tandem: One tries to approximate the target state, the other triesto distinguish between target and approximated state. Supported by numericalsimulations, we show that resilient backpropagation algorithms performremarkably well in optimizing the two circuits. We use the bipartiteentanglement entropy to design an efficient heuristic for the stoppingcriterion. Our approach may find application in quantum state tomography.',\n",
              " 'Solving for molecular excited states remains one of the key challenges ofmodern quantum chemistry. Traditional methods are constrained by existingcomputational capabilities, limiting the complexity of the molecules that canbe studied or the accuracy of the results that can be obtained. Several quantumcomputing methods have been suggested to address this limitation. However,these typically have hardware requirements which may not be achieved in thenear term. We propose a variational quantum machine learning based method todetermine molecular excited states aiming at being as resilient as possible tothe defects of early Noisy Intermediate Scale Quantum (NISQ) computers anddemonstrate an implementation for H2 on IBMQ. Our method uses a combination oftwo parametrized quantum circuits, working in tandem, combined with aVariational Quantum Eigensolver (VQE) to iteratively find the eigenstates of amolecular Hamiltonian.',\n",
              " 'This paper demonstrates a method for tensorizing neural networks based uponan efficient way of approximating scale invariant quantum states, theMulti-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as areplacement for the fully connected layers in a convolutional neural networkand test this implementation on the CIFAR-10 and CIFAR-100 datasets. Theproposed method outperforms factorization using tensor trains, providinggreater compression for the same level of accuracy and greater accuracy for thesame level of compression. We demonstrate MERA layers with 14000 times fewerparameters and a reduction in accuracy of less than 1% compared to theequivalent fully connected layers, scaling like O(N).',\n",
              " 'Quantum systems interacting with an unknown environment are notoriouslydifficult to model, especially in presence of non-Markovian andnon-perturbative effects. Here we introduce a neural network based approach,which has the mathematical simplicity of theGorini-Kossakowski-Sudarshan-Lindblad master equation, but is able to modelnon-Markovian effects in different regimes. This is achieved by using recurrentneural networks for defining Lindblad operators that can keep track of memoryeffects. Building upon this framework, we also introduce a neural networkarchitecture that is able to reproduce the entire quantum evolution, given aninitial state. As an application we study how to train these models for quantumprocess tomography, showing that recurrent neural networks are accurate overdifferent times and regimes.',\n",
              " 'We propose an efficient method for simultaneously optimizing both thestructure and parameter values of quantum circuits with only a smallcomputational overhead. Shallow circuits that use structure optimizationperform significantly better than circuits that use parameter updates alone,making this method particularly suitable for noisy intermediate-scale quantumcomputers. We demonstrate the method for optimizing a variational quantumeigensolver for finding the ground states of Lithium Hydride and the Heisenbergmodel in simulation, and for finding the ground state of Hydrogen gas on theIBM Melbourne quantum computer.',\n",
              " 'Machine learning is seen as a promising application of quantum computation.For near-term noisy intermediate-scale quantum (NISQ) devices, parametrizedquantum circuits (PQCs) have been proposed as machine learning models due totheir robustness and ease of implementation. However, the cost function isnormally calculated classically from repeated measurement outcomes, such thatit is no longer encoded in a quantum state. This prevents the value from beingdirectly manipulated by a quantum computer. To solve this problem, we give aroutine to embed the cost function for machine learning into a quantum circuit,which accepts a training dataset encoded in superposition or an easilypreparable mixed state. We also demonstrate the ability to evaluate thegradient of the encoded cost function in a quantum state.',\n",
              " 'We introduce augmented biracks and define a (co)homology theory associated toaugmented biracks. The new homology theory extends the previously studiedYang-Baxter homology with a combinatorial formulation for the boundary map andspecializes to $N$-reduced rack homology when the birack is a rack. Weintroduce augmented birack 2-cocycle invariants of classical and virtual knotsand links and provide examples.',\n",
              " 'We introduce a notion of ternary distributive algebraic structure, giveexamples, and relate it to the notion of a quandle. Classification is given forlow order structures of this type. Constructions of such structures fromternary bialgebras are provided. We also describe ternary distributivealgebraic structures coming from groups and give examples from vector spaceswhose bases are elements of a finite ternary distributive set. We introduce acohomology theory that is analogous to Hochschild cohomology and relate it to aformal deformation theory of these structures.',\n",
              " 'The flavor evolution of neutrinos in core collapse supernovae and neutronstar mergers is a critically important unsolved problem in astrophysics.Following the electron flavor evolution of the neutrino system is essential forcalculating the thermodynamics of compact objects as well as the chemicalelements they produce. Accurately accounting for flavor transformation in theseenvironments is challenging for a number of reasons, including the large numberof neutrinos involved, the small spatial scale of the oscillation, and thenonlinearity of the system. We take a step in addressing these issues bypresenting a method which describes the neutrino fields in terms of angularmoments. Our moment method successfully describes the fast flavor neutrinotransformation phenomenon which is expected to occur in regions close to thecentral object. We apply our moment method to neutron star merger conditionsand show that we are able to capture the three phases of growth, saturation,and decoherence by comparing with particle-in-cell calculations. We alsodetermine the size of the growing fluctuations in the neutrino field.',\n",
              " 'As the early universe expands and cools the rates of the weak interactionsthat keep neutrinos in thermal equilibrium with the matter and the relatedrates of the reactions that inter-convert neutrons and protons decrease.Eventually, these rates fall below the expansion rate -- they freeze out.Likewise, the rates of the strong and electromagnetic nuclear reactions thatbuild up and tear down nuclei, though fast enough to maintain equilibrium earlyon, slow down and ultimately lead to freeze out. Together these freeze outprocesses comprise the epoch of Big Bang Nucleosynthesis (BBN). The relicsemerging from this early time include the light element abundances, for exampleof helium and deuterium, and a background of decoupled neutrinos, a \"C$\\\\nu$B\" ,roughly analogous to the Cosmic Microwave Background, the CMB. These fossilrelics encode the history of the physics operating in the early universe.Consequently, BBN has emerged as a key tool for constraining new,beyond-standard-model (BSM) physics. BBN may become an even finer probe of BSMphysics, given the anticipated higher precision in measurements of theprimordial abundances of deuterium and helium afforded by the advent of largeoptical telescopes and Stage-4 CMB experiments. The latter experiments willalso provide higher precision determinations of $N_{\\\\rm eff}$, a measure of therelativistic energy density at the photon decoupling epoch and, hence, animportant probe of the C$\\\\nu$B.',\n",
              " 'The intersection of the cosmic and neutrino frontiers is a rich field wheremuch discovery space still remains. Neutrinos play a pivotal role in the hotbig bang cosmology, influencing the dynamics of the universe over numerousdecades in cosmological history. Recent studies have made tremendous progressin understanding some properties of cosmological neutrinos, primarily theirenergy density. Upcoming cosmological probes will give higher precision on theenergy density, but could also start probing other properties of the neutrinospectra. When convolved with results from terrestrial experiments, cosmologycan become even more acute at probing new physics related to neutrinos or evenBeyond the Standard Model (BSM). Any discordance between laboratory andcosmological data sets may reveal new BSM physics or suggest alternative modelsof cosmology. We give examples of the intersection between terrestrial andcosmological probes in the neutrino sector, and briefly discuss thepossibilities of what different experiments may see in conjunction withcosmological observatories.',\n",
              " 'In both stars and in the early universe, the production of deuterium is thefirst step on the way to producing heavier nuclei. If the strong force wereslightly weaker, deuterium would not be stable, and many authors have notedthat nuclesynthesis would be compromised so that helium production could notproceed through standard reaction chains. Motivated by the possibility thatother regions of space-time could have different values for the fundamentalconstants, this paper considers stellar evolution in universes without stabledeuterium and argues that such universes can remain habitable. Even inuniverses with no stellar nucleosynthesis, stars can form and will generateenergy through gravitational contraction. We show that such stars can besufficiently luminous and long-lived to support life. Stars with initial massesthat exceed the Chandrasekhar mass cannot be supported by degeneracy pressureand explode at the end of their contraction phase. The resulting explosivenucleosynthesis can provide the universe with some heavy elements. We alsoexplore the possibility that helium can be produced in stellar cores through atriple-nucleon reaction (roughly analogous to the triple-alpha reaction). Nextwe show that with even trace amounts of heavy elements --- produced through thetriple-nucleon process or by explosive nucleosynthesis --- the CNO cycle canoperate and allow stars to function. Finally, we consider Big BangNucleosynthesis without stable deuterium and find that only trace amounts ofhelium are produced, with even smaller abundances of other nuclei. With starsevolving through gravitational contraction, explosive nucleosynthesis, thetriple-nucleon reaction, and the CNO cycle, universes with no stable deuteriumare thus potentially habitable, contrary to many previous claims.',\n",
              " 'This paper derives an upper limit on the density $\\\\rho_{\\\\scriptstyle\\\\Lambda}$of dark energy based on the requirement that cosmological structure formsbefore being frozen out by the eventual acceleration of the universe. Byallowing for variations in both the cosmological parameters and the strength ofgravity, the resulting constraint is a generalization of previous limits. Thespecific parameters under consideration include the amplitude $Q$ of theprimordial density fluctuations, the Planck mass $M_{\\\\rm pl}$, thebaryon-to-photon ratio $\\\\eta$, and the density ratio $\\\\Omega_M/\\\\Omega_b$. Inaddition to structure formation, we use considerations from stellar structureand Big Bang Nucleosynthesis (BBN) to constrain these quantities. The resultingupper limit on the dimensionless density of dark energy becomes$\\\\rho_{\\\\scriptstyle\\\\Lambda}/M_{\\\\rm pl}^4<10^{-90}$, which is $\\\\sim30$ orders ofmagnitude larger than the value in our universe$\\\\rho_{\\\\scriptstyle\\\\Lambda}/M_{\\\\rm pl}^4\\\\sim10^{-120}$. This new limit is muchless restrictive than previous constraints because additional parameters areallowed to vary. With these generalizations, a much wider range of universescan develop cosmic structure and support observers. To constrain theconstituent parameters, new BBN calculations are carried out in the regimewhere $\\\\eta$ and $G=M_{\\\\rm pl}^{-2}$ are much larger than in our universe. Ifthe BBN epoch were to process all of the protons into heavier elements, nohydrogen would be left behind to make water, and the universe would not beviable. However, our results show that some hydrogen is always left over, evenunder conditions of extremely large $\\\\eta$ and $G$, so that a wide range ofalternate universes are potentially habitable.',\n",
              " 'Motivated by the possibility that the laws of physics could be different inother regions of space-time, we consider nuclear processes in universes wherethe weak interaction is either stronger or weaker than observed. We focus onthe physics of both Big Bang Nucleosynthesis (BBN) and stellar evolution. Forsufficiently ineffective weak interactions, neutrons do not decay during BBN,and the baryon-to-photon ratio $\\\\eta$ must be smaller in order for protons tosurvive without becoming incorporated into larger nuclei. For stronger weakinteractions, neutrons decay before the onset of BBN, and the early universe isleft with nearly a pure hydrogen composition. We then consider stellarstructure and evolution for the different nuclear compositions resulting fromBBN, a wide range of weak force strengths, and the full range of stellar massesfor a given universe. We delineate the range of this parameter space thatsupports working stars, along with a determination of the dominant nuclearreactions over the different regimes. Deuterium burning dominates the energygeneration in stars when the weak force is sufficiently weak, whereasproton-proton burning into helium-3 dominates for the regime where the weakforce is much stronger than in our universe. Although stars in these universesare somewhat different, they have comparable surface temperatures,luminosities, radii, and lifetimes, so that a wide range of such universesremain potentially habitable.',\n",
              " 'Motivated by the possible existence of other universes, this paper considersthe evolution of massive stars with different values for the fundamentalconstants. We focus on variations in the triple alpha resonance energy andstudy its effects on the resulting abundances of $^{12}$C, $^{16}$O, and largernuclei. In our universe, the $0^{+}$ energy level of carbon supports a resonantnuclear reaction that dominates carbon synthesis in stellar cores and accountsfor the observed cosmic abundances. Here we define $\\\\Delta{E}_R$ to be thechange in this resonant energy level, and show how different values affect thecosmic abundances of the intermediate alpha elements. Using the state of theart computational package $MESA$, we carry out stellar evolution calculationsfor massive stars in the range $M_\\\\ast$ = $15-40M_\\\\odot$, and for a wide rangeof resonance energies. We also include both solar and low metallicity initialconditions. For negative $\\\\Delta{E}_R$ , carbon yields are increased relativeto standard stellar models, and such universes remain viable as long as theproduction of carbon nuclei remains energetically favorable, and stars remainstable, down to $\\\\Delta{E}_R\\\\approx-300$ keV. For positive $\\\\Delta{E}_R$,carbon yields decrease, but significant abundances can be produced forresonance energy increments up to $\\\\Delta{E}_R\\\\approx+500$ keV. Oxygen yieldstend to be anti-correlated with those of carbon, and the allowed range in$\\\\Delta{E}_R$ is somewhat smaller. We also present yields for neon, magnesium,and silicon. With updated stellar evolution models and a more comprehensivesurvey of parameter space, these results indicate that the range of viableuniverses is larger than suggested by earlier studies.',\n",
              " 'We perform calculations of dark photon production and decay in the earlyuniverse for ranges of dark photon masses and vacuum coupling with standardmodel photons. Simultaneously and self-consistently with dark photon productionand decay, our calculations include a complete treatment of weak decoupling andbig bang nucleosynthesis (BBN) physics. These calculations incorporate allrelevant weak, electromagnetic, and strong nuclear reactions, includingcharge-changing (isospin-changing) lepton capture and decay processes. Theyreveal a rich interplay of dark photon production, decay, and associatedout-of-equilibrium transport of entropy into the decoupling neutrino seas. Mostimportantly, the self-consistent nature of our simulations allows us to capturethe magnitude and phasing of entropy injection and dilution. Entropyinjection-induced alteration of the time-temperature-scale factor relationduring weak decoupling and BBN leads to changes in the light element abundanceyields and the total radiation content (as parametrized by $N_{\\\\rm eff}$).These changes suggest ways to extend previous dark photon BBN constraints.However, our calculations also identify ranges of dark photon mass andcouplings not yet constrained, but perhaps accessible and probable, in futureStage-4 cosmic microwave background experiments and future high precisionprimordial deuterium abundance measurements.',\n",
              " 'Deuterium represents the only bound isotope in the universe with atomic massnumber $A=2$. Motivated by the possibility of other universes, where the strongforce could be stronger, this paper considers the effects of bound diprotonsand dineutrons on stars. We find that the existence of additional stable nucleiwith $A=2$ has relatively modest effects on the universe. Previous workindicates that Big Bang Nucleosynthesis (BBN) produces more deuterium, but doesnot lead to catastrophic heavy element production. This paper revisits BBNconsiderations and confirms that the universe is left with an ample supply ofhydrogen and other light nuclei for typical cosmological parameters. Using the$MESA$ numerical package, we carry out stellar evolution calculations foruniverses with stable diprotons, with nuclear cross sections enhanced by largefactors $X$. This work focuses on $X=10^{15}-10^{18}$, but explores the widerrange $X$ = $10^{-3}-10^{18}$. For a given stellar mass, the presence of stablediprotons leads to somewhat brighter stars, with the radii and photospherictemperatures roughly comparable to thoese of red giants. The centraltemperature decreases from the characteristic value of$T_c\\\\approx1.5\\\\times10^7$ K for hydrogen burning down to the value of$T_c\\\\approx10^6$ K characteristic of deuterium burning. The stellar lifetimesare smaller for a given mass, but with the extended possible mass range, thesmallest stars live for trillions of years, far longer than the current cosmicage. Finally, the enhanced cross sections allow for small, partially degenerateobjects with mass $M_\\\\ast=1-10M_J$ to produce significant steady-stateluminosity and thereby function as stars.',\n",
              " 'The successful transition from core-collapse supernova simulations usingclassical neutrino transport to simulations using quantum neutrino transportwill require the development of methods for calculating neutrino flavortransformations that mitigate the computational expense. One potential approachis the use of angular moments of the neutrino field, which has the added appealthat there already exist simulation codes which make use of moments forclassical neutrino transport. Evolution equations for quantum moments based onthe quantum kinetic equations can be straightforwardly generalized from theevolution of classical moments based on the Boltzmann equation. We present anefficient implementation of neutrino transformation using quantum angularmoments in the free streaming, spherically symmetric bulb model. We compare theresults against analytic solutions and the results from more exact multi-angleneutrino flavor evolution calculations. We find that our moment-based methodsemploying scalar closures predict, with good accuracy, the onset of collectiveflavor transformations seen in the multi-angle results. However in somesituations they overestimate the coherence of neutrinos traveling alongdifferent trajectories. More sophisticated quantum closures may improve theagreement between the inexpensive moment-based methods and the multi-angleapproach.',\n",
              " 'Motivated by the possible existence of other universes, with different valuesfor the fundamental constants, this paper considers stellar models in universeswhere $^8$Be is stable. Many previous authors have noted that stars in ouruniverse would have difficulty producing carbon and other heavy elements in theabsence of the well-known $^{12}$C resonance at 7.6 MeV. This resonance isnecessary because $^8$Be is unstable in our universe, so that carbon must beproduced via the triple alpha reaction to achieve the requisite abundance.Although a moderate change in the energy of the resonance (200 -- 300 keV) willindeed affect carbon production, an even smaller change in the binding energyof beryllium ($\\\\sim100$ keV) would allow $^8$Be to be stable. A stable isotopewith $A=8$ would obviate the need for the triple alpha process in general, andthe $^{12}$C resonance in particular, for carbon production. This paperexplores the possibility that $^8$Be can be stable in other universes. Simplenuclear considerations indicate that bound states can be realized, with bindingenergy $\\\\sim0.1-1$ MeV, if the fundamental constants vary by a $\\\\sim {\\\\rmfew}-10$ percent. In such cases, $^8$Be can be synthesized through heliumburning, and $^{12}$C can be produced later through nuclear burning ofberyllium. This paper focuses on stellar models that burn helium intoberyllium; once the universe in question has a supply of stable beryllium,carbon production can take place during subsequent evolution in the same staror in later stellar generations. Using both a semi-analytic stellar structuremodel as well as a state-of-the-art stellar evolution code, we find that viablestellar configurations that produce beryllium exist over a wide range ofparameter space. Finally, we demonstrate that carbon can be produced duringlater evolutionary stages.',\n",
              " 'Neutrinos are the Standard Model (SM) particles which we understand theleast, often due to how weakly they interact with the other SM particles.Beyond this, very little is known about interactions among the neutrinos, i.e.,their self-interactions. The SM predicts neutrino self-interactions at a levelbeyond any current experimental capabilities, leaving open the possibility forbeyond-the-SM interactions across many energy scales. In this white paper, wereview the current knowledge of neutrino self-interactions from a vast array ofprobes, from cosmology, to astrophysics, to the laboratory. We also discusstheoretical motivations for such self-interactions, including neutrino massesand possible connections to dark matter. Looking forward, we discuss thecapabilities of searches in the next generation and beyond, highlighting thepossibility of future discovery of this beyond-the-SM physics.',\n",
              " 'This book lays out the scientific goals to be addressed by thenext-generation ground-based cosmic microwave background experiment, CMB-S4,envisioned to consist of dedicated telescopes at the South Pole, the highChilean Atacama plateau and possibly a northern hemisphere site, all equippedwith new superconducting cameras. CMB-S4 will dramatically advance cosmologicalstudies by crossing critical thresholds in the search for the B-modepolarization signature of primordial gravitational waves, in the determinationof the number and masses of the neutrinos, in the search for evidence of newlight relics, in constraining the nature of dark energy, and in testing generalrelativity on large scales.',\n",
              " \"We study the ground state magnetic properties of ferromagnetic spinorBose-Einstein condensates confined in a deep optical lattices. In the Mottinsulator regime, the ``mini-condensates'' at each lattice site behave asmesoscopic spin magnets that can interact with neighboring sites through boththe static magnetic dipolar interaction and the light-induced dipolarinteraction. We show that such an array of spin magnets can undergo aferromagnetic or anti-ferromagnetic phase transition under the magnetic dipolarinteraction depending on the dimension of the confining optical lattice. Theground-state spin configurations and related magnetic properties areinvestigated in detail.\",\n",
              " \"Scientific research funding is allocated largely through a system ofsoliciting and ranking competitive grant proposals. In these competitions, theproposals themselves are not the deliverables that the funder seeks, butinstead are used by the funder to screen for the most promising research ideas.Consequently, some of the funding program's impact on science is squanderedbecause applying researchers must spend time writing proposals instead of doingscience. To what extent does the community's aggregate investment in proposalpreparation negate the scientific impact of the funding program? Are therealternative mechanisms for awarding funds that advance science moreefficiently? We use the economic theory of contests to analyze how efficientlygrant proposal competitions advance science, and compare them with recentlyproposed, partially randomized alternatives such as lotteries. We find that theeffort researchers waste in writing proposals may be comparable to the totalscientific value of the research that the funding supports, especially whenonly a few proposals can be funded. Moreover, when professional pressuresmotivate investigators to seek funding for reasons that extend beyond the valueof the proposed science (e.g., promotion, prestige), the entire program canactually hamper scientific progress when the number of awards is small. Wesuggest that lost efficiency may be restored either by partial lotteries forfunding, or by funding researchers based on past scientific success instead ofproposals for future work.\",\n",
              " 'Peer review is an integral component of contemporary science. While peerreview focuses attention on promising and interesting science, it alsoencourages scientists to pursue some questions at the expense of others. Here,we use ideas from forecasting assessment to examine how two modes of peerreview -- ex ante review of proposals for future work and ex post review ofcompleted science -- motivate scientists to favor some questions instead ofothers. Our main result is that ex ante and ex post peer review pushinvestigators toward distinct sets of scientific questions. This tension arisesbecause ex post review allows an investigator to leverage her own scientificbeliefs to generate results that others will find surprising, whereas ex antereview does not. Moreover, ex ante review will favor different researchquestions depending on whether reviewers rank proposals in anticipation ofchanges to their own personal beliefs, or to the beliefs of their peers. Thetension between ex ante and ex post review puts investigators in a bind,because most researchers need to find projects that will survive both. Byunpacking the tension between these two modes of review, we can understand howthey shape the landscape of science and how changes to peer review might shiftscientific activity in unforeseen directions.',\n",
              " 'Cavities play a fundamental role in wave phenomena from quantum mechanics toelectromagnetism and dictate the spatiotemporal physics of lasers. In general,they are constructed by closing all \"doors\" through which waves can escape. Wereport, at room temperature, a bound state in the continuum laser thatharnesses optical modes residing in the radiation continuum but nonetheless maypossess arbitrarily high quality factors. These counterintuitive cavities arebased on resonantly trapped symmetry-compatible modes that destructivelyinterfere. Our experimental demonstration opens exciting avenues towardscoherent sources with intriguing topological properties for optical trapping,biological imaging, and quantum communication.',\n",
              " 'Solution-processed organic-inorganic lead halide perovskites have recentlyemerged as promising gain media for tunable semiconductor lasers, and have cometo rival inorganic III-V group semiconductors as the material candidate forchip-scale lasers. Although electrically pumped lasing at room temperature isthe ultimate goal, optically pumped continuous-wave lasing at roomtemperature,a prerequisite for a laser diode,has not been achieved so far.Here, we report lasing action in a surface emitting distributed feedbackmethylammonium lead iodide (MAPbI3) perovskite laser on silicon substrate, atroom temperature under continuous-wave optical pumping, in ambient airenvironment. This outstanding performance is achieved by the ultra-low lasingthreshold of 13 W/cm2, which is enabled by the thermal nanoimprint lithographythat directly patterns perovskite into a high Q cavity with large modeconfinement, while at the same time improves perovskite emissioncharacteristics. Our results represent a major step toward the realization ofperovskite laser diodes, which is essential in the future insertion ofperovskite lasers into photonic integrated circuits, for applications inoptical computing, sensing and on-chip quantum information.',\n",
              " \"Metal halide perovskites have emerged as promising gain materials for on-chiplasers in photonic integrated circuits (PICs). However, stable continuous wave(CW) lasing behavior under optical pumping at room temperature - a prerequisitefor electrically pumped lasing - has not yet been demonstrated. To achievestable CW operation, we introduce a multiplex of strategies that includemorphological, structural and interfacial engineering of CH3NH3PbBr3 (MAPbBr3)thin films to improve perovskite's intrinsic stability, as well as high qualitycavity design to reduce the operational power. We demonstrate for the firsttime, over 90-minute-long green CW lasing with 9.4W/cm2 threshold from apolycarbonate (PC)-defect-passivated, directly patterned MAPbBr3two-dimensional photonic crystal (PhC) cavity without any substrate cooling. Wealso show our approach's effectiveness on the performance of MAPbBr3 underelectrical excitation: we observe a seven-fold current efficiency enhancementby applying our strategies to a MAPbBr3 LED. This work paves the way to therealization of electrically pumped lasing in perovskites.\",\n",
              " \"The planar distributions of satellite galaxies around the Milky Way andAndromeda have been extensively studied as potential challenges to the standardcosmological model. Using the Sloan Digital Sky Survey and the Millenniumsimulation we extend such studies to the satellite galaxies of massive galaxyclusters. We find that both observations and simulations of galaxy clustersshow an excess of anisotropic satellite distributions. On average, satellitesin clusters have a higher degree of anisotropy than their counterparts inMilky-Way-mass hosts once we account for the difference in their radialdistributions. The normal vector of the plane of satellites is strongly alignedwith the host halo's minor axis, while the alignment with the large-scalestructure is weak. At fixed cluster mass, the degree of anisotropy is higher athigher redshift. This reflects the highly anisotropic nature of satellitesaccretion points, a feature that is partly erased by the subsequent orbitalevolution of the satellites. We also find that satellite galaxies are mostlyaccreted singly so group accretion is not the explanation for the highflattening of the planes of satellites.\",\n",
              " 'We study the relation between halo concentration and mass (c-M relation)using the Seventh and Eighth Data Release of the Sloan Digital Sky Survey (SDSSDR7 and DR8) galaxy catalogue. Assuming that the satellite galaxies follow thedistribution of dark matter, we derive the halo concentration by fitting thesatellite radial profile with a Nararro Frank and White (NFW) format. Thederived c-M relation covers a wide halo mass range from $10^{11.6}$ to$10^{14.1} \\\\rm\\\\ M_\\\\odot$. We confirm the anti-correlation between the halo massand concentration as predicted in cosmological simulations. Our results are ingood agreement with those derived using galaxy dynamics and gravitationallensing for halos of $10^{11.6}-10^{12.9} \\\\rm\\\\ M_\\\\odot$, while they areslightly lower for halos of $10^{12.9}-10^{14.1}\\\\rm\\\\ M_\\\\odot$. It is becauseblue satellite galaxies are less concentrated, especially in the inner regions.Instead of using all satellite galaxies, red satellites could be better tracersof the underlying dark matter distribution in galaxy groups.',\n",
              " 'Recently, there has been an increase in the demand of virtual 3D objectsrepresenting real-life objects. A plethora of methods and systems have alreadybeen proposed for the acquisition of the geometry of real-life objects rangingfrom those which employ active sensor technology, passive sensor technology ora combination of various techniques.  In this paper we present the development of a 3D scanning system which isbased on the principle of structured-light, without having particularrequirements for specialized equipment. We discuss the intrinsic details andinherent difficulties of structured-light scanning techniques and present oursolutions. Finally, we introduce our open-source scanning software system\"3DUNDERWORLD-SLS\" which implements the proposed techniques both in CPU andGPU. We have performed extensive testing with a wide range of models and reportthe results. Furthermore, we present a comprehensive evaluation of the systemand a comparison with a high-end commercial 3D scanner.',\n",
              " 'The quest for an integrated light source that promises high energy efficiencyand fast modulation for high-performance photonic circuits has led to thedevelopment of room-temperature telecom-wavelength nanoscale laser with highspontaneous emission factors, \\\\beta. The coherence characterization of thistype of lasers is inherently difficult with the conventional measurement ofoutput light intensity versus input pump intensity due to the diminishing kinkin the measurement curve. We demonstrate the transition from chaotic tocoherent emission of a high-{\\\\beta} pulse-pump metallo-dielectric nanolaser canbe determined by examining the width of a second order intensity correlationpeak, which shrinks below and broadens above threshold. Photon fluctuationstudy, first one ever reported for this type of nanolaser, confirms thevalidity of this measurement technique. Additionally, we show that the widthvariation above threshold results from the delayed threshold phenomenon,providing the first indirect observation of dynamical hysteresis in ananolaser.',\n",
              " 'Action recognition, motion classification, gait analysis and synthesis arefundamental problems in a number of fields such as computer graphics,bio-mechanics and human computer interaction that generate a large body ofresearch. This type of data is complex because it is inherentlymultidimensional and has multiple modalities such as video, motion capturedata, accelerometer data, etc. While some of this data, such as monocular videoare easy to acquire, others are much more difficult and expensive such asmotion capture data or multi-view video. This creates a large barrier of entryin the research community for data driven research. We have embarked oncreating a new large repository of motion and action data (CAMREP) consistingof several motion and action databases. What makes this database unique is thatwe use a variety of modalities, enabling multi-modal analysis. Presently, thesize of datasets varies with some having a large number of subjects whileothers having smaller numbers. We have also acquired long capture sequences ina number of cases, making some datasets rather large.',\n",
              " 'Perovskite light-emitting diodes (PeLEDs) have drawn considerable attentionfor their favorable optoelectronic properties. Perovskite light-emittingelectrochemical cells (PeLECs) _ devices that utilize mobile ions _ haverecently been reported but have yet to reach the performance of the bestPeLEDs. We leveraged a poly(ethylene oxide) electrolyte and lithium dopant inCsPbBr3 thin films to produce PeLECs of improved brightness and efficiency. Inparticular, we found that a single layer PeLEC from CsPbBr3:PEO:LiPF6 with 0.5%wt. LiPF6 produced highly efficient (22 cd/A) and bright (~15000 cd/m2)electroluminescence. To understand this improved performance among PeLECs, wecharacterized these perovskite thin films with photoluminescence (PL)spectroscopy, scanning electron microscopy (SEM), atomic force microscopy(AFM), X-ray photoelectron spectroscopy (XPS), and X-ray diffraction (XRD).These studies revealed that this optimal LiPF6 concentration improveselectrical double layer formation, reduces the occurrence of voids, chargetraps, and pinholes, and increases grain size and packing density.',\n",
              " \"A special class of anisotropic media, hyperbolic metamaterials andmetasurfaces (HMMs), has attracted much attention in recent years due to itsunique abilities to manipulate and engineer electromagnetic waves on thesubwavelength scale. Because all HMM designs require metal dielectriccomposites, the unavoidable metal loss at optical frequencies inspired thedevelopment of active HMMs, where gain materials is incorporated to compensatethe metal loss. Here, we experimentally demonstrate an active type II HMM thatoperates at vacuum wavelength near 750 nm on a silicon platform. Different fromprevious active HMMs operating below 1 {\\\\mu}m, the dielectric constituent inour HMM is solely composed of gain medium, by utilizing solution processed andwidely tunable metal halide perovskite gain. Thanks to the facile fabrication,tunability and silicon compatibility of our active HMM, this work paves the waytowards HMM's integration into on chip components, and eventually, intophotonic integrated circuits.\",\n",
              " 'While the success of pre-trained language models has largely eliminated theneed for high-quality static word vectors in many NLP applications, suchvectors continue to play an important role in tasks where words need to bemodelled in the absence of linguistic context. In this paper, we explore howthe contextualised embeddings predicted by BERT can be used to producehigh-quality word vectors for such domains, in particular related to knowledgebase completion, where our focus is on capturing the semantic properties ofnouns. We find that a simple strategy of averaging the contextualisedembeddings of masked word mentions leads to vectors that outperform the staticword vectors learned by BERT, as well as those from standard word embeddingmodels, in property induction tasks. We notice in particular that maskingtarget words is critical to achieve this strong performance, as the resultingvectors focus less on idiosyncratic properties and more on general semanticproperties. Inspired by this view, we propose a filtering strategy which isaimed at removing the most idiosyncratic mention vectors, allowing us to obtainfurther performance gains in property induction.',\n",
              " 'Timeout bugs can cause serious availability and performance issues which areoften difficult to fix due to the lack of diagnostic information. Previous workproposed solutions for fixing specific type of timeout-related performancebugs. In this paper, we present TFix+, a self-configuring timeout bug fixingframework for automatically correcting two major kinds of timeout bugs (i.e.,misused timeout bugs and missing timeout bugs) with dynamic timeout valuepredictions. TFix+ provides two new hybrid schemes for fixing misused andmissing timeout bugs, respectively. TFix+ further provides prediction-driventimeout variable configuration based on runtime function tracing. We haveimplemented a prototype of TFix+ and conducted experiments on 16 real worldtimeout bugs. Our experimental results show that TFix+ can effectively fix 15out of tested 16 timeout bugs.',\n",
              " 'We suggest a unified spectrally matched optimal grid approach forfinite-difference and finite-element approximation of the PML. The new approachallows to combine optimal discrete absorption for both evanescent andpropagative waves.',\n",
              " 'Atomic force microscopy and second-harmonic generation data show that borondoping enhances the rate of oxidation of H-terminated silicon. Holes cause agreater increase in the reactivity of the Si-H up bonds than that of the Si-Siback bonds.',\n",
              " 'It is known that a higher concentration of free carriers leads to a higheroxide growth rate in the thermal oxidation of silicon. However, the role ofelectrons and holes in oxidation chemistry is not clear. Here, we reportreal-time second-harmonic-generation data on the oxidation of H-terminated(111)Si that reveal that high concentrations of electrons increase the chemicalreactivity of the outer-layer Si-Si back bonds relative to the Si-H up bonds.However, the thicknesses of the natural oxides of all samples stabilize near 1nm at room temperature, regardless of the chemical kinetics of the differentbonds.',\n",
              " \"We measured the lifetime of optically created valley polarization in singlelayer WS2 using transient absorption spectroscopy. The electron valleyrelaxation is very short (< 1ps). However the hole valley lifetime is at leasttwo orders of magnitude longer and exhibits a temperature dependence thatcannot be explained by single carrier spin/valley relaxation mechanisms. Ourtheoretical analysis suggests that a collective contribution of two potentialprocesses may explain the valley relaxation in single layer WS2. One processinvolves direct scattering of excitons from K to K' valleys with a spinflip-flop interaction. The other mechanism involves scattering through spindegenerate Gamma valley. This second process is thermally activated with anArrhenius behavior due to the energy barrier between Gamma and K valleys.\",\n",
              " \"The development and the use of quantum technologies are hindered by afundamental challenge: Quantum materials exhibit macroscopic quantum propertiesat extremely low temperatures due to the loss of quantum coherence at elevatedtemperatures. Here, based on our recent discovery of room temperaturesuperfluorescence in perovskites, we present the Quantum Analog of VibrationIsolation, 'QAVI', model and explain how it protects the quantum phase againstdephasing at high temperatures. We then postulate the requirements forobservation of macroscopic quantum phenomena at practical temperatures andpropose a unified model for all macroscopic quantum phase transitions. Wefurther present the general features of the temperature and density phasediagram of macroscopic quantum phase transitions that are mediated by the QAVIprocess and identify the similarities observed in the phase diagram of high Tcsuperconductors. Understanding this fundamental quantum coherence protectionmechanism is imperative to accelerate the discovery of high temperaturemacroscopic quantum phenomena, and offers significant potential for developingquantum technologies functioning under practical conditions.\",\n",
              " 'We quantitatively illustrate the fundamental limit that exciton-excitonannihilation (EEA) may impose to the light emission of monolayer transitionmetal dichalcogenide (TMDC) materials. The EEA in TMDC monolayers showsdependence on the interaction with substrates as its rate increases from 0.1cm2/s (0.05 cm2/s) to 0.3 cm2/s (0.1 cm2/s) with the substrates removed for WS2(MoS2) monolayers. It turns to be the major pathway of exciton decay anddominates the luminescence efficiency when the exciton density is beyond 1010cm-2 in suspended monolayers or 1011 cm-2 in supported monolayers. This sets anupper limit on the density of injected charges in light emission devices forthe realization of optimal luminescence efficiency. The strong EEA rate alsodictates the pumping threshold for population inversion in the monolayers to be12-18 MW/cm2 (optically) or 2.5-4x105 A/cm2 (electrically).',\n",
              " 'Transition metal dichalcogenide (TMDC) monolayers present a remarkablemultifunctional material with potential to enable the development of a widerange of novel devices. However, the functionalities observed often fall shortof the expectation, which hinders the device development. Here we demonstratethat the optical, catalytic, and thermal functionalities of TMDC monolayers canall be substantially enhanced by up to orders of magnitude with theintercalation of water molecules or small cations (H+ and Li+) between themonolayers and underlying substrates. In contrast, the same molecules orcations adsorbed on top of the monolayers show negligible effects. We alsodiscover two major roles of the intercalated species in the enhancement: dopingthe monolayers and modifying the interaction of the monolayers with thesubstrate. The result points out a versatile and convenient strategy of usingthe intercalation of molecules or ions to enhance the functionalities of TMDCmonolayers.',\n",
              " \"Excitons in semiconductors are usually non interacting and behave like anideal gas, but may condense to a strongly correlated liquid like state, i.e.electron hole liquid (EHL), at high density and appropriate temperature. EHL isa macroscopic quantum state with exotic properties and represents the ultimateattainable charge excitation density in steady states. It bears great promisefor a variety of fields such as ultrahigh power photonics and quantum scienceand technology. However, the condensation of gas like excitons to EHL has oftenbeen restricted to cryogenic temperatures, which significantly limits theprospect of EHL for use in practical applications. Herein we demonstrate theformation of EHL at room temperature in monolayer MoS2 by taking advantage ofthe monolayer's extraordinarily strong exciton binding energy. This workdemonstrates the potential for the liquid like state of charge excitations tobe a useful platform for the studies of macroscopic quantum phenomena and thedevelopment of optoelectronic devices.\",\n",
              " 'Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data iswidely-used for early detection of diseases. Classical LDA for EHR dataclassification, however, suffers from two handicaps: the ill-posed estimationof LDA parameters (e.g., covariance matrix), and the \"linear inseparability\" ofEHR data. To handle these two issues, in this paper, we propose a novelclassifier FWDA -- Fast Wishart Discriminant Analysis, that makes predictionsin an ensemble way. Specifically, FWDA first surrogates the distribution ofinverse covariance matrices using a Wishart distribution estimated from thetraining data, then \"weighted-averages\" the classification results of multipleLDA classifiers parameterized by the sampled inverse covariance matrices via aBayesian Voting scheme. The weights for voting are optimally updated to adapteach new input data, so as to enable the nonlinear classification. Theoreticalanalysis indicates that FWDA possesses a fast convergence rate and a robustperformance on high dimensional data. Extensive experiments on large-scale EHRdataset show that our approach outperforms state-of-the-art algorithms by alarge margin.',\n",
              " 'Schedulability is a fundamental problem in real-time scheduling, but it hasto be approximated due to the intrinsic computational hardness. As the mostpopular algorithm for deciding schedulability on multiprocess platforms, thespeedup factor of partitioned-EDF is challenging to analyze and is far frombeen determined. Partitioned-EDF was first proposed in 2005 by Barush andFisher [1], and was shown to have a speedup factor at most 3-1/m, meaning thatif the input of sporadic tasks is feasible on m processors with speed one,partitioned-EDF will always return succeeded on m processors with speed 3-1/m.In 2011, this upper bound was improved to 2.6322-1/m by Chen and Chakraborty[2], and no more improvements have appeared ever since then. In this paper, wedevelop a novel method to discretize and regularize sporadic tasks, whichenables us to improve, in the case of constrained deadlines, the speedup factorof partitioned-EDF to 2.5556-1/m, very close to the asymptotic lower bound 2.5in [2].',\n",
              " \"The COVID-19 pandemic has significantly impacted the tourism and hospitalitysector. Public policies such as travel restrictions and stay-at-home orders hadsignificantly affected tourist activities and service businesses' operationsand profitability. To this end, it is essential to develop an interpretableforecast model that supports managerial and organizational decision-making. Wedeveloped DemandNet, a novel deep learning framework for predicting time seriesdata under the influence of the COVID-19 pandemic. The framework starts byselecting the top static and dynamic features embedded in the time series data.Then, it includes a nonlinear model which can provide interpretable insightinto the previously seen data. Lastly, a prediction model is developed toleverage the above characteristics to make robust long-term forecasts. Weevaluated the framework using daily hotel demand and revenue data from eightcities in the US. Our findings reveal that DemandNet outperforms thestate-of-art models and can accurately predict the impact of the COVID-19pandemic on hotel demand and revenues.\",\n",
              " 'Transformers have been widely applied in text classification. Unfortunately,real-world data contain anomalies and noisy labels that cause challenges forstate-of-art Transformers. This paper proposes Protoformer, a novelself-learning framework for Transformers that can leverage problematic samplesfor text classification. Protoformer features a selection mechanism forembedding samples that allows us to efficiently extract and utilize anomaliesprototypes and difficult class prototypes. We demonstrated such capabilities ondatasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We alsoapplied the framework to several models. The results indicate that Protoformercan improve current Transformers in various empirical settings.',\n",
              " 'Time series models often deal with extreme events and anomalies, bothprevalent in real-world datasets. Such models often need to provide carefulprobabilistic forecasting, which is vital in risk management for extreme eventssuch as hurricanes and pandemics. However, it is challenging to automaticallydetect and learn to use extreme events and anomalies for large-scale datasets,which often require manual effort. Hence, we propose an anomaly-aware forecastframework that leverages the previously seen effects of anomalies to improveits prediction accuracy during and after the presence of extreme events.Specifically, the framework automatically extracts anomalies and incorporatesthem through an attention mechanism to increase its accuracy for future extremeevents. Moreover, the framework employs a dynamic uncertainty optimizationalgorithm that reduces the uncertainty of forecasts in an online manner. Theproposed framework demonstrated consistent superior accuracy with lessuncertainty on three datasets with different varieties of anomalies over thecurrent prediction models.',\n",
              " \"Motion planning and control in autonomous car racing are one of the mostchallenging and safety-critical tasks due to high speed and dynamism. Thelower-level control nodes are expected to be highly optimized due to resourceconstraints of onboard embedded processing units, although there are strictlatency requirements. Some of these guarantees can be provided at theapplication level, such as using ROS2's Real-Time executors. However, theperformance can be far from satisfactory as many modern control algorithms(such as Model Predictive Control) rely on solving complicated onlineoptimization problems at each iteration. In this paper, we present a simple yeteffective multi-threading technique to optimize the throughput ofonline-control algorithms for resource-constrained autonomous racing platforms.We achieve this by maintaining a systematic pool of worker threads solving theoptimization problem in parallel which can improve the system performance byreducing latency between control input commands. We further demonstrate theeffectiveness of our method using the Model Predictive Contouring Control(MPCC) algorithm running on Nvidia's Xavier AGX platform.\",\n",
              " \"In the past few years, we have envisioned an increasing number of businessesstart driving by big data analytics, such as Amazon recommendations and GoogleAdvertisements. At the back-end side, the businesses are powered by big dataprocessing platforms to quickly extract information and make decisions. Runningon top of a computing cluster, those platforms utilize scheduling algorithms toallocate resources. An efficient scheduler is crucial to the system performancedue to limited resources, e.g. CPU and Memory, and a large number of userdemands. However, besides requests from clients and current status of thesystem, it has limited knowledge about execution length of the running jobs,and incoming jobs' resource demands, which make assigning resources achallenging task. If most of the resources are occupied by a long-running job,other jobs will have to keep waiting until it releases them. This paperpresents a new scheduling strategy, named DRESS that particularly aims tooptimize the allocation among jobs with various demands. Specifically, itclassifies the jobs into two categories based on their requests, reserves aportion of resources for each of category, and dynamically adjusts the reservedratio by monitoring the pending requests and estimating release patterns ofrunning jobs. The results demonstrate DRESS significantly reduces thecompletion time for one category, up to 76.1% in our experiments, and in themeanwhile, maintains a stable overall system performance.\",\n",
              " 'Real-time machine learning detection algorithms are often found withinautonomous vehicle technology and depend on quality datasets. It is essentialthat these algorithms work correctly in everyday conditions as well as understrong sun glare. Reports indicate glare is one of the two most prominentenvironment-related reasons for crashes. However, existing datasets, such asLISA and the German Traffic Sign Recognition Benchmark, do not reflect theexistence of sun glare at all. This paper presents the GLARE traffic signdataset: a collection of images with U.S based traffic signs under heavy visualinterference by sunlight. GLARE contains 2,157 images of traffic signs with sunglare, pulled from 33 videos of dashcam footage of roads in the United States.It provides an essential enrichment to the widely used LISA Traffic Signdataset. Our experimental study shows that although several state-of-the-artbaseline methods demonstrate superior performance when trained and testedagainst traffic sign datasets without sun glare, they greatly suffer whentested against GLARE (e.g., ranging from 9% to 21% mean mAP, which issignificantly lower than the performances on LISA dataset). We also notice thatcurrent architectures have better detection accuracy (e.g., on average 42% meanmAP gain for mainstream algorithms) when trained on images of traffic signs insun glare.',\n",
              " 'In this project we analysed how much semantic information images carry, andhow much value image data can add to sentiment analysis of the text associatedwith the images. To better understand the contribution from images, we comparedmodels which only made use of image data, models which only made use of textdata, and models which combined both data types. We also analysed if thisapproach could help sentiment classifiers generalize to unknown sentiments.',\n",
              " 'Building on the success of recent discriminative mid-level elements, wepropose a surprisingly simple approach for object detection which performscomparable to the current state-of-the-art approaches on PASCAL VOC comp-3detection challenge (no external data). Through extensive experiments andablation analysis, we show how our approach effectively improves upon theHOG-based pipelines by adding an intermediate mid-level representation for thetask of object detection. This representation is easily interpretable andallows us to visualize what our object detector \"sees\". We also discuss theinsights our approach shares with CNN-based methods, such as sharingrepresentation between categories helps.',\n",
              " 'The field of object detection has made significant advances riding on thewave of region-based ConvNets, but their training procedure still includes manyheuristics and hyperparameters that are costly to tune. We present a simple yetsurprisingly effective online hard example mining (OHEM) algorithm for trainingregion-based ConvNet detectors. Our motivation is the same as it has alwaysbeen -- detection datasets contain an overwhelming number of easy examples anda small number of hard examples. Automatic selection of these hard examples canmake training more effective and efficient. OHEM is a simple and intuitivealgorithm that eliminates several heuristics and hyperparameters in common use.But more importantly, it yields consistent and significant boosts in detectionperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectivenessincreases as datasets become larger and more difficult, as demonstrated by theresults on the MS COCO dataset. Moreover, combined with complementary advancesin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP onPASCAL VOC 2007 and 2012 respectively.',\n",
              " \"In this paper, the generation of topological energy in models with largeextra dimensions is investigated. The origin of this energy is attributed to atopological deformation of the standard Minkowski vacuum due tocompactification of extra dimensions. This deformation is seen to give rise toan effective, finite energy density due to massive Kaluza-Klein modes ofgravitation. It's renormalized value is seen to depend on the size of the extradimensions instead of the UV cut-off of the theory. It is shown that if thisenergy density is to contribute to the observed cosmological constant, therewill be extremely stringent bounds on the number of extra dimensions and theirsize.\",\n",
              " 'Recent findings in neuroscience suggest that the human brain representsinformation in a geometric structure (for instance, through conceptual spaces).In order to communicate, we flatten the complex representation of entities andtheir attributes into a single word or a sentence. In this paper we use graphconvolutional networks to support the evolution of language and cooperation inmulti-agent systems. Motivated by an image-based referential game, we propose agraph referential game with varying degrees of complexity, and we providestrong baseline models that exhibit desirable properties in terms of languageemergence and cooperation. We show that the emerged communication protocol isrobust, that the agents uncover the true factors of variation in the game, andthat they learn to generalize beyond the samples encountered during training.',\n",
              " 'In order to communicate, humans flatten a complex representation of ideas andtheir attributes into a single word or a sentence. We investigate the impact ofrepresentation learning in artificial agents by developing graph referentialgames. We empirically show that agents parametrized by graph neural networksdevelop a more compositional language compared to bag-of-words and sequencemodels, which allows them to systematically generalize to new combinations offamiliar features.',\n",
              " 'Biased sampling methods such as the Temperature Accelerated Sliced Sampling(TASS), which can explore high dimensional collective variable (CV) space, isof great interest in free energy calculations. Such methods can efficientlysample configurational space even when a large number of CVs for biasing areused while many conventional methods are limited to two or three CVs. In thispaper, we propose a modification to the TASS method, called Parallel Bias TASSor PBTASS, wherein a multidimensional parallel metadynamics bias isincorporated on a selected set of CVs. The corresponding time-dependentreweighting equations are derived, and the method is benchmarked. Inparticular, we compare the accuracy and efficiency of PBTASS with variousmethods viz. standard TASS, Temperature Accelerated MolecularDynamics/driven-Adiabatic Free Energy Dynamics, and Parallel Bias Metadynamics.We demonstrate the capability of the PBTASS method by reconstructing theeight-dimensional free energy surface of alanine pentapeptide in vacuo from a25 ns long trajectory. Free energy barriers and free energies of high energysaddle points on the high dimensional free energy landscape of this system arereported.',\n",
              " 'In recent years, we have seen tremendous progress in the field of objectdetection. Most of the recent improvements have been achieved by targetingdeeper feedforward networks. However, many hard object categories such asbottle, remote, etc. require representation of fine details and not justcoarse, semantic representations. But most of these fine details are lost inthe early convolutional layers. What we need is a way to incorporate finerdetails from lower layers into the detection architecture. Skip connectionshave been proposed to combine high-level and low-level features, but we arguethat selecting the right features from low-level requires top-down contextualinformation. Inspired by the human visual pathway, in this paper we proposetop-down modulations as a way to incorporate fine details into the detectionframework. Our approach supplements the standard bottom-up, feedforward ConvNetwith a top-down modulation (TDM) network, connected using lateral connections.These connections are responsible for the modulation of lower layer filters,and the top-down network handles the selection and integration of contextualinformation and low-level features. The proposed TDM architecture provides asignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without anybells and whistles (e.g., multi-scale, iterative box refinement, etc.).',\n",
              " 'Multi-task learning in Convolutional Networks has displayed remarkablesuccess in the field of recognition. This success can be largely attributed tolearning shared representations from multiple supervisory tasks. However,existing multi-task approaches rely on enumerating multiple networkarchitectures specific to the tasks at hand, that do not generalize. In thispaper, we propose a principled approach to learn shared representations inConvNets using multi-task learning. Specifically, we propose a new sharingunit: \"cross-stitch\" unit. These units combine the activations from multiplenetworks and can be trained end-to-end. A network with cross-stitch units canlearn an optimal combination of shared and task-specific representations. Ourproposed method generalizes across multiple tasks and shows dramaticallyimproved performance over baseline methods for categories with few trainingexamples.',\n",
              " 'How do we learn an object detector that is invariant to occlusions anddeformations? Our current solution is to use a data-driven strategy -- collectlarge-scale datasets which have object instances under different conditions.The hope is that the final classifier can use these examples to learninvariances. But is it really possible to see all the occlusions in a dataset?We argue that like categories, occlusions and object deformations also follow along-tail. Some occlusions and deformations are so rare that they hardlyhappen; yet we want to learn a model invariant to such occurrences. In thispaper, we propose an alternative solution. We propose to learn an adversarialnetwork that generates examples with occlusions and deformations. The goal ofthe adversary is to generate examples that are difficult for the objectdetector to classify. In our framework both the original detector and adversaryare learned in a joint manner. Our experimental results indicate a 2.3% mAPboost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challengecompared to the Fast-RCNN pipeline. We also release the code for this paper.',\n",
              " 'Recently proposed stabilization mechanism of the Randall-Sundrum metric givesrise to a scalar radion, which couples universally to matter with a weakinteraction ($\\\\simeq 1$ TeV) scale. Demanding that gauge boson scattering asdescribed by the effective low enerrgy theory be unitary upto a given scaleleads to significant constraints on the mass of such a radion.',\n",
              " 'We adapted the join-training scheme of Faster RCNN framework from Caffe toTensorFlow as a baseline implementation for object detection. Our code is madepublicly available. This report documents the simplifications made to theoriginal pipeline, with justifications from ablation analysis on both PASCALVOC 2007 and COCO 2014. We further investigated the role of non-maximalsuppression (NMS) in selecting regions-of-interest (RoIs) for regionclassification, and found that a biased sampling toward small regions helpsperformance and can achieve on-par mAP to NMS-based sampling when convergedsufficiently.',\n",
              " \"In this paper we consider self-supervised representation learning to improvesample efficiency in reinforcement learning (RL). We propose a forwardprediction objective for simultaneously learning embeddings of states andaction sequences. These embeddings capture the structure of the environment'sdynamics, enabling efficient policy learning. We demonstrate that our actionembeddings alone improve the sample efficiency and peak performance ofmodel-free RL on control from low-dimensional states. By combining state andaction embeddings, we achieve efficient learning of high-quality policies ongoal-conditioned continuous control from pixel observations in only 1-2 millionenvironment steps.\",\n",
              " 'This paper formulates hypothesis verification as an RL problem. Specifically,we aim to build an agent that, given a hypothesis about the dynamics of theworld, can take actions to generate observations which can help predict whetherthe hypothesis is true or false. Existing RL algorithms fail to solve thistask, even for simple environments. In order to train the agents, we exploitthe underlying structure of many hypotheses, factorizing them as{pre-condition, action sequence, post-condition} triplets. By leveraging thisstructure we show that RL agents are able to succeed at the task. Furthermore,subsequent fine-tuning of the policies allows the agent to correctly verifyhypotheses not amenable to the above factorization.',\n",
              " 'Iron self-diffusion in nano-composite FeZr alloy has been investigated usingneutron reflectometry technique as a function of applied compressive stress. Acomposite target of (Fe+Zr) and (57Fe+Zr) was alternatively sputtered todeposit chemically homogeneous multilayer (CHM) structure,[Fe75Zr25/57Fe75Zr25]10. The multilayers were deposited on to a bent Si waferusing a 3-point bending device. Post-deposition, the bending of the substratewas released which results in an applied compressive stress on to themultilayer. In the as-deposited state, the alloy multilayer forms an amorphousphase, which crystallizes into a nano-composite phase when heated at 373 K.Bragg peaks due to isotopic contrast were observed from CHM, when measured byneutron reflectivity, while x-ray reflectivity showed a pattern correspondingto a single layer. Self-diffusion of iron was measured with the decay of theintensities at the Bragg peaks in the neutron reflectivity pattern afterthermal annealing at different temperatures. It was found that theself-diffusion of iron slows down with an increase in the strength of appliedcompressive stress.',\n",
              " 'In this work we investigate the process of iron nitride (Fe-N) phaseformation using 2 at.% Al or 2 at.% Ti as additives. The samples were preparedwith a magnetron sputtering technique using different amount of nitrogen duringthe deposition process. The nitrogen partial pressure (\\\\pn) was varied between0-50% (rest Argon) and the targets of pure Fe, [Fe+Ti] and [Fe+Al] weresputtered. The addition of small amount of Ti or Al results in improvedsoft-magnetic properties when sputtered using \\\\pn $\\\\leq$ 10\\\\p. When \\\\pn isincreased to 50\\\\p non-magnetic Fe-N phases are formed. We found that ironmononitride (FeN) phases (N at% $\\\\sim$50) are formed with Al or Ti addition at\\\\pn =50% whereas in absence of such addition \\\\eFeN phases (N\\\\pat$\\\\sim$30) areformed. It was found that the overall nitrogen content can be increasedsignificantly with Al or Ti additions. On the basis of obtained result wepropose a mechanism describing formation of Fe-N phases Al and Ti additives.',\n",
              " 'Thin films of iron and permalloy Ni80Fe20 were prepared using an Ar+N2mixture with magnetron sputtering technique at ambient temperature. Thenitrogen partial pressure, during sputtering process was varied in the range of0 to 100%, keeping the total gas flow at constant. At lower nitrogen pressuresRN2<33% both Fe and NiFe, first form a nanocrystalline structure and anincrease in nitrogen partail pressure results in formation of an amorphousstructure. At intermediate nitrogen partial pressures, nitrides of Fe and NiFewere obtained while at even higher nitrogen partial pressures, nitridesthemselves became nanocrystalline or amorphous. The surface, structural andmagnetic properties of the deposited films were studied using x-ray reflectionand diffraction, transmission electron microscopy, polarized neutronreflectivity and using a DC extraction magnetometer. The growth behavior foramorphous film was found different as compared with poly or nanocrystallinefilms. The soft-magnetic properties of FeN were improved on nanocrystallizationwhile those of NiFeN were degraded. A mechanism inducing nanocrystallizationand amorphization in Fe and NiFe due to reactive nitrogen sputtering isdiscussed in the present article.',\n",
              " 'Cobalt nitride (Co-N) thin films prepared using a reactive magnetronsputtering process by varying the relative nitrogen gas flow (\\\\pn) are studiedin this work. As \\\\pn~increases, Co(N), \\\\tcn, Co$_3$N and CoN phases are formed.An incremental increase in \\\\pn, after emergence of \\\\tcn~phase at \\\\pn=10\\\\p,results in a continuous expansion in the lattice constant ($a$) of \\\\tcn. For\\\\pn=30\\\\p, $a$ maximizes and becomes comparable to its theoretical value. Anexpansion in $a$ of \\\\tcn, results in an enhancement of magnetic moment, to theextent that it becomes even larger than pure Co. Though such higher (than puremetal) magnetic moment for Fe$_4$N thin films have been theoretically predictedand evidenced experimentally, higher (than pure Co) magnetic moment areevidenced in this work and explained in terms of large-volume high-moment modelfor tetra metal nitrides.',\n",
              " 'In this work, we studied the local structure and the magnetization of Co4Nthin films deposited by a reactive dc magnetron sputtering process. Theinterstitial incorporation of N atoms in a fcc Co lattice is expected to expandthe structure and such expansion yields interesting magnetic propertiescharacterized by a larger than Co magnetic moment and a very high value of spinpolarization ratio in Co4N. By optimizing the growth conditions, we preparedCo4N film having lattice parameter close to its theoretically predicted value.The N concentration was measured using secondary ion mass spectroscopy.Detailed magnetization measurements using bulk magnetization method andpolarized neutron reflectivity confirm that the magnetic moment of Co in Co4Nis higher than that of Co.',\n",
              " 'In this work, we studied cobalt nitride (Co-N) thin films deposited using adc magnetron sputtering method at a substrate temperature (\\\\Ts) of 523\\\\,K. Wefind that independent of the reactive gas flow (\\\\pn) used during sputtering,the phases of Co-N formed at this temperature seems to be identical having N\\\\pat~$\\\\sim$5. This is contrary to Co-N phases formed at lower \\\\Ts. For\\\\Ts$\\\\sim$300\\\\,K, an evolution of Co-N phases starting fromCo(N)$\\\\rightarrow$\\\\tcn$\\\\rightarrow$Co$_3$N$\\\\rightarrow$CoN can be seen as \\\\pnincreases to 100\\\\p, whereas when the substrate temperature increases to 523\\\\,K,the phase formed is a mixture of Co and \\\\tcn, independent of the {\\\\pn} usedduring sputtering. We used x-ray diffraction (XRD) to probe long rangeordering, x-ray absorption spectroscopy (XAS) at Co absorption edge for thelocal structure, Magneto-optical Kerr e ffect (MOKE) and polarized neutronreflectivity (PNR) to measure the magnetization of samples. Quantification of N\\\\pat~was done using secondary ion mass spectroscopy (SIMS). Measurementssuggest that the magnetic moment of Co-N samples deposited at 523\\\\,K isslightly higher than the bulk Co moment and does not get affected with the\\\\pn~used for reactive sputtering. Our results provide an important insightabout the phase formation of Co-N thin films which is discussed in this work.',\n",
              " 'The paper proposes a low-cost system to capture spatial vehicle headway dataand process the raw data by filtering outliers using a novel filtering process.Multiple sensors and modules are integrated to form the system. The sensorsused are compact, lightweight, low-cost and have low power consumption. Asingle beam 1-Dimensional Light Detection and Ranging (LIDAR) was used forcapturing the space headway data, a Global Positioning System (GPS) to map eachdata point with a timestamp and position and also a camera to capture videodata with an overlay of date, time, distance and speed in real-time. Thefiltering technique utilizes the Autoregressive Integrated Moving Average(ARIMA) prediction modeling and mean-filtering. The data captured is stored ina Raspberry Pi module. The data is later processed by using the filteringtechnique to obtain the least outliers. The overall system has enabled tocapture spatial headway data and speed of the vehicle at a very low cost andthe data obtained can be used for car-following model analysis andspeed-density analysis.',\n",
              " 'In this work, we studied the pathways for formation of stoichiometric\\\\tcn~thin films. Polycrystalline and epitaxial \\\\tcn~films were prepared usingreactive direct current magnetron (dcMS) sputtering technique. A systematicvariation in the substrate temperature (\\\\Ts) during the dcMS process revealsthat the lattice parameter (LP) decreases as \\\\Ts~increases. We found thatnearly stoichiometric \\\\tcn~films can be obtained when \\\\Ts~= 300\\\\,K. However,they emerge from the transient state of Co target ($\\\\phi$3\\\\,inch). By reducingthe target size to $\\\\phi$1\\\\,inch, now the \\\\tcn~phase formation takes place fromthe metallic state of Co target. In this case, LP of \\\\tcn~film comes out to be$\\\\sim$99\\\\p~of the value expected for \\\\tcn. This is the largest value of LPfound so far for \\\\tcn. The pathways achieved for formation of polycrystalline\\\\tcn~were adopted to grow an epitaxial \\\\tcn~film, which shows four foldmagnetic anisotropy in magneto-optic Kerr effect measurements. Detailedcharacterization using secondary ion mass spectroscopy indicates that Ndiffuses out when \\\\Ts~is raised even to 400\\\\,K. Measurement of electronicstructure using x-ray photoelectron spectroscopy and x-ray absorptionspectroscopy further confirms it. Magnetization measurements using bulkmagnetization and polarized neutron reflectivity show that the saturationmagnetization of stoichiometric \\\\tcn~film is even larger than pure Co. Sinceall our measurements indicated that N could be diffusing out, when \\\\tcn~filmsare grown at high \\\\Ts, we did actual N self-diffusion measurements in a CoNsample and found that N self-diffusion was indeed substantially higher. Theoutcome of this work clearly shows that the \\\\tcn~films grown prior to this workwere always N deficient and the pathways for formation of a stoichiometric\\\\tcn~have been achieved.',\n",
              " 'This study aims towards a systematic reciprocity of the tunable synthesisparameters - partial pressure of N$_2$ gas, ion energy (\\\\Ei) and Ti interfacein TiN thin film samples deposited using ion beam sputtering at ambienttemperature (300\\\\,K). At the optimum partial pressure of N$_2$ gas, sampleswere prepared with or without Ti interface at \\\\Ei~=~1.0 or 0.5\\\\,keV. They werecharacterized using x-ray reflectivity (XRR) to deduce thickness, roughness anddensity. The roughness of TiN thin films was found to be below 1\\\\,nm, whendeposited at the lower \\\\Ei~of 0.5\\\\,keV and when interfaced with a layer of Ti.Under these conditions, the density of TiN sample reaches to5.80($\\\\pm$0.03)\\\\,g~cm$^{-3}$, a value highest hitherto for any TiN sample.X-ray diffraction and electrical resistivity measurements were performed. Itwas found that the cumulative effect of the reduction in \\\\Ei~from 1.0 to0.5\\\\,keV and the addition of Ti interface favors (111) oriented growth leadingto dense and smooth TiN films and a substantial reduction in the electricalresistivity. The reduction in \\\\Ei~has been attributed to the surface kineticsmechanism (simulated using SRIM) where the available energy of the sputteredspecies (\\\\Esp) leaving the target at \\\\Ei~= 0.5\\\\,keV is the optimum valuefavoring the growth of defects free homogeneously distributed films. Theelectronic structure of samples was probed using N K-edge absorptionspectroscopy and the information about the crystal field and spin-orbitsplitting confirmed TiN phase formation. In essence, through this work, wedemonstrate the role of \\\\Esp~and Ti interface in achieving highly dense andsmooth TiN thin films with low resistivity without the need of a hightemperature or substrate biasing during the thin film deposition process.',\n",
              " \"To contemplate an alternative approach for the minimization of diffusion athigh temperature depositions, present findings impart viability ofroom-temperature deposited reactively sputtered ScN thin film samples. Theadopted room temperature route endows precise control over the $R_{N_2}$ flowfor a methodical structural phase evolution from Sc$\\\\to$ScN and probe thecorrelated physical aspects of the highly textured ScN samples. In the nitridedregime i.e. at $R_{N_2}$ = 2.5-100% flow, incorporation of unintentional oxygendefects were evidenced from surface sensitive soft x-ray absorptionspectroscopy study, though less compared to their metal ($R_{N_2} = 0\\\\%$) andinterstitial ($R_{N_2} = 1.6\\\\%$) counterparts, due to higher Gibb's free energyfor Sc-O-N formation with no trace of ligand field splitting around the OK-edge spectra. To eradicate the sceptism of appearance of N K-edge (401.6 eV)and Sc L-edge (402.2 eV) absorption spectra adjacent to each other, the nascentSc K-edge study has been adopted for the first time to validate complementaryinsight on the metrical parameters of the Sc-N system taken into consideration.Optical bandgaps of the polycrystalline ScN thin film samples were found tovary between 2.25-2.62 eV as obtained from the UV-Vis spectroscopy, whereas,the nano-indentation hardness and modulus of the as-deposited samples liebetween 15-34GPa and 152-476GPa, respectively following a linearly increasingtrend of resistance to plastic deformations. Besides, contrary to other early3d transition metal nitrides (TiN, VN, CrN), a comprehensive comparison ofnoticeably large homogeneity range in Sc-N has been outlined to apprehend theminuscule lattice expansion over the large $R_{N_2}$ realm.\",\n",
              " 'Raman and Photoluminescence (PL) experiments on correlated metallicLa$_{0.7}$Sr$_{0.3}$MnO$_{3}$ have been carried out using different excitationwavelengths as a function of temperature from 15 K to 300 K. Our data suggest aRaman mode centered at 1800 cm$^{-1}$ and a PL band at 2.2 eV. The intensitiesof the two peaks decrease with increasing temperature. The Raman mode can beattributed to a plasmon excitation whose frequency and linewidths areconsistent with the measured resistivities. The PL involves intersiteelectronic transitions of the manganese ions.',\n",
              " 'We have investigated the magnetic-field- and pressure-induced structural andmagnetic phases of the triple-layer ruthenate - Sr$_{4}$Ru$_{3}$O$_{10}$.Magnetic-field-induced changes in the phonon spectra reveal dramaticspin-reorientation transitions and strong magneto-elastic coupling in thismaterial. Additionally, pressure-dependent Raman measurements at differenttemperatures reveal an anomalous negative Gruneisen-parameter associated withthe B$_{1g}$ mode ($\\\\sim$ 380 cm$^{-1}$) at low temperatures (T $<$ 75K), whichcan be explained consistently with the field dependent Raman data.',\n",
              " \"We give an algorithm for finding a solution to the Carath\\\\'{e}odory-Fej\\\\'{e}rinterpolation problem on the polydisc $\\\\mathbb D^n,$ whenever it exists. Anecessary condition for the existence of a solution becomes apparent from thisalgorithm. A generalization of the well-known theorem due to Nehari has beenobtained. A proof of the Kor\\\\'{a}nyi--Puk\\\\'{a}nszky theorem is given using thespectral theorem.\",\n",
              " 'We show that the complex normed linear space $\\\\ell^1(n),$ $n>1,$ has noisometric embedding into $k\\\\times k$ complex matrices for any $k\\\\in \\\\mathbb{N}$and discuss a class of infinite dimensional operator space structures on it.',\n",
              " 'We report the first electron paramagnetic resonance studies of singlecrystals and powders of Pr_{0.6}Ca_{0.4}MnO_{3} in the 300-4.2 K range,covering the charge ordering transition at ~ 240 K and antiferromagnetictransition (T_N) at ~ 170 K. The asymmetry parameter for the Dysonian singlecrystal spectra shows anomalous increase at T_{co}. Below T_{co} the g-valueincreases continuously, suggesting a gradual strengthening of orbital ordering.The linewidth undergoes a sudden increase at T_{co} and continues to increasedown to T_N. The intensity increases as the temperature is decreased tillT_{co} due to the renormalization of magnetic susceptibility arising from thebuild up of ferromagnetic correlations. The value of the exchange constant, J,is estimated to be 154 K.',\n",
              " 'We have revisited the still unresolved puzzle of the dispersion of the Ramandisordered-induced D band as a function of laser excitation photon energy E$_L$in graphite-like materials. We propose that the D-mode is a combination of anoptic phonon at the K-point in the Brillioun zone and an acoustic phonon whosemomentum is determined uniquely by the double resonance condition. The fit ofthe experimental data with the double-resonance model yields the reducedeffective mass of 0.025m$_{e}$ for the electron-hole pairs corresponding to theA$_{2}$ transition, in agreement with other experiments. The model can alsoexplain the difference between $\\\\omega_S$ and $\\\\omega_{AS}$ for D andD$^{\\\\star}$ modes, and predicts its dependence on the Raman excitationfrequency.',\n",
              " 'Cubic structure is one of the most commonly found structures for oxides athigh temperature. As the temperature is lowered only a handful of these oxidesexhibit ferroelectricity which is rather surprising. In this paper, we use theexample of YCrO3 (YCO), an incipient ferroelectric material to show that inmost oxides there are two competing phenomenon -onset of ferroelectricity dueto rotation of CrO6 octahedra and displacement of Y atom leading to suppressionof ferroelectricity. This competition reveals that while the octahedralrotations favor a lower symmetry state, the Y atom displacement opposes itleaving YCO to exhibit only an incipient ferroelectric state. These resultswhile being in agreement with the earlier theoretical predictions can also helpsuggest a pathway to a more stable ferroelectric state in these oxides by usinga larger cationic substitution at the Y site.',\n",
              " 'In this paper, we present design, implementation, and effectiveness ofgenerating personalized suggestions for email replies. To personalize emailresponses based on users style and personality, we model the users personabased on her past responses to emails. This model is added to thelanguage-based model created across users using past responses of the all useremails.  A users model captures the typical responses of the user given a particularcontext. The context includes the email received, recipient of the email, andother external signals such as calendar activities, preferences, etc. Thecontext along with users personality (e.g., extrovert, formal, reserved, etc.)is used to suggest responses. These responses can be a mixture of multiplemodes: email replies (textual), audio clips, etc. This helps in makingresponses mimic the user as much as possible and helps the user to be moreproductive while retaining her mark in the responses.',\n",
              " 'We prove a local Douglas formula for higher order weighted Dirichlet-typeintegrals. With the help of this formula, we study the multiplier algebra ofthe associated higher order weighted Dirichlet-type spaces $\\\\mathcalH_{\\\\pmb\\\\mu},$ induced by an $m$-tuple $\\\\pmb \\\\mu =(\\\\mu_1,\\\\ldots,\\\\mu_{m})$ offinite non-negative Borel measures on the unit circle. In particular, it isshown that any weighted Dirichlet-type space of order $m,$ for $m\\\\geqslant 3,$forms an algebra under pointwise product. We also prove that every non-zeroclosed $M_z$-invariant subspace of $\\\\mathcal H_{\\\\pmb\\\\mu},$ has codimension $1$property if $m\\\\geqslant 3$ or $\\\\mu_2$ is finitely supported. As anotherapplication of local Douglas formula obtained in this article, it is shown thatfor any $m\\\\geqslant 2,$ weighted Dirichlet-type space of order $m$ does notcoincide with any de Branges-Rovnyak space $\\\\mathcal H(b)$ with equivalence ofnorms.',\n",
              " 'We report the evolution of the Raman active A_g phonon modes in thecharge-ordered manganite Pr_{0.63}Ca_{0.37}MnO_{3} as a function of temperaturefrom 300K to 25K. Our studies reveal that the linewidths of A_g(2) and A_g(4)phonons increase as temperature decreases. This anomalous temperaturedependence of phonon lineshapes, seen for the first time in charge orderedmanganites, can be quantitatively understood in terms of strong spin-phononcoupling involving t_{2g} spins and A_g phonons.',\n",
              " 'In this letter we show the presence of a spin-glass like phase in singlecrystals of magnetoelectric gallium ferrite (GaFeO3) below ~210 K viatemperature dependent ac and dc magnetization studies. Analysis of frequencydispersion of the susceptibility peak at ~210 K using the critical slowing downmodel and Vogel-Fulcher law strongly suggests the existence of a classicalspin-glass like phase. This classical spin glass behavior of GaFeO3 isunderstood in terms of an outcome of geometrical frustration arising from theinherent site disorder among the antiferromagnetically coupled Fe ions locatedat octahedral Ga and Fe sites.',\n",
              " 'Sol-gel deposited (010) textured polycrystalline thin films of galliumferrite (GaFeO3 or GFO) on n-Si(100) and Pt/Si(111) substrates arecharacterized for room temperature multiferroism. Structural characterizationusing X-ray diffraction and Raman spectroscopy confirms formation of singlephase with nano-sized crystallites. Temperature dependent magnetization studydemonstrates ferri to paramagnetic transition at ~300 K. Room temperaturepiezoresponse force microscopic analysis reveals local 180 degree phaseswitching of ferroelectric domains at very high coercive field EC, ~ 1350 kV/cmconsistent with recent experimental and first-principles studies. Our studyopens up possibility of integrating polycrystalline GFO in novel roomtemperature multiferroic devices.',\n",
              " 'We obtain a formula that relates the spherical moments of the multiplicationtuple on a Dirichlet-type space to a complex moment problem in severalvariables. This can be seen as the ball-analogue of a formula originallyinvented by Richter. We capitalize on this formula to study Dirichlet-typespaces on the unit ball and joint $2$-isometries.',\n",
              " \"The validity of the von-Neumann inequality for commuting $n$ - tuples of$3\\\\times 3$ matrices remains open for $n\\\\geq 3$. We give a partial answer tothis question, which is used to obtain a necessary condition for theCarath\\\\'{e}odory-Fej\\\\'{e}r interpolation problem on the polydisc $\\\\mathbb D^n.$In the special case of $n=2$ (which follows from Ando's theorem as well), thisnecessary condition is made explicit. An alternative approach to theCarath\\\\'{e}odory-Fej\\\\'{e}r interpolation problem, in the special case of $n=2,$adapting a theorem of Kor\\\\'{a}nyi and Puk\\\\'{a}nzsky is given. As a consequence,a class of polynomials are isolated for which a complete solution to theCarath\\\\'{e}odory-Fej\\\\'{e}r interpolation problem is easily obtained. A naturalgeneralization of the Hankel operators on the Hardy space of $H^2(\\\\mathbb T^2)$then becomes apparent. Many of our results remain valid for any $n\\\\in \\\\mathbbN,$ however, the computations are somewhat cumbersome for $n>2$ and areomitted. The inequality $\\\\lim_{n\\\\to \\\\infty}C_2(n)\\\\leq 2 K^\\\\mathbb C_G$, where$K_G^\\\\mathbb C$ is the complex Grothendieck constant and\\\\[C_2(n)=\\\\sup\\\\big\\\\{\\\\|p(\\\\boldsymbol T)\\\\|:\\\\|p\\\\|_{\\\\mathbb D^n,\\\\infty}\\\\leq 1,\\\\|\\\\boldsymbol T\\\\|_{\\\\infty} \\\\leq 1 \\\\big\\\\}\\\\] is due to Varopoulos. Here thesupremum is taken over all complex polynomials $p$ in $n$ variables of degreeat most $2$ and commuting $n$ - tuples $\\\\boldsymbol T:=(T_1,\\\\ldots,T_n)$ ofcontractions. We show that \\\\[\\\\lim_{n\\\\to \\\\infty}C_2(n)\\\\leq \\\\frac{3\\\\sqrt{3}}{4}K^\\\\mathbb C_G\\\\] obtaining a slight improvement in the inequality of Varopoulos.We show that the normed linear space $\\\\ell^1(n),$ $n>1,$ has no isometricembedding into $k\\\\times k$ complex matrices for any $k\\\\in \\\\mathbb N$ anddiscuss several infinite dimensional operator space structures on it.\",\n",
              " 'Let $\\\\mathbb C_k[Z_1,\\\\ldots, Z_n]$ denote the set of all polynomials ofdegree at most $k$ in $n$ complex variables and $\\\\mathscr{C}_n$ denote the setof all $n$ - tuple $\\\\boldsymbol T=(T_1,\\\\ldots,T_n)$ of commuting contractionson some Hilbert space $\\\\mathbb{H}.$ The interesting inequality $$K_{G}^{\\\\mathbbC}\\\\leq \\\\lim_{n\\\\to \\\\infty}C_2(n)\\\\leq 2 K^\\\\mathbb C_G,$$ where\\\\[C_k(n)=\\\\sup\\\\big\\\\{\\\\|p(\\\\boldsymbol T)\\\\|:\\\\|p\\\\|_{\\\\mathbb D^n,\\\\infty}\\\\leq 1, p\\\\in\\\\mathbb C_k[Z_1,\\\\ldots,Z_n],\\\\boldsymbol T\\\\in\\\\mathscr{C}_n \\\\big\\\\}\\\\] and$K_{G}^{\\\\mathbb C}$ is the complex Grothendieck constant, is due to Varopoulos.We answer a long--standing question by showing that the limit$\\\\lim_{n\\\\to\\\\infty} \\\\frac{C_2(n)}{K^\\\\mathbb C_G}$ is strictly bigger than $1.$Let $\\\\mathbb C_2^s[Z_1,\\\\ldots , Z_n]$ denote the set of all complex valuedhomogeneous polynomials $p(z_1,\\\\ldots,z_n)$ $=\\\\sum_{j,k=1}^{n}a_{jk}z_jz_k$ ofdegree two in $n$ - variables, where $(\\\\!(a_{jk})\\\\!)$ is a $n\\\\times n$ complexsymmetric matrix. For each $n\\\\in\\\\mathbb{N},$ define the linear map$\\\\mathscr{A}_n:\\\\big (\\\\mathbb C_2^s[Z_1,\\\\ldots , Z_n],\\\\|\\\\cdot\\\\|_{\\\\mathbb D^n,\\\\infty}\\\\big ) \\\\to \\\\big (M_n, \\\\|\\\\cdot \\\\|_{\\\\infty \\\\to 1}\\\\big )$ to be$\\\\mathscr{A}_n\\\\big (p) = (\\\\!(a_{jk})\\\\!).$ We show that the supremum (over $n$)of the norm of the operators $\\\\mathscr{A}_n;\\\\,n\\\\in\\\\mathbb{N},$ is bounded belowby the constant $\\\\pi^2/8.$ Using a class of operators, first introduced byVaropoulos, we also construct a large class of explicit polynomials for whichthe von Neumann inequality fails. We prove that the originalVaropoulos--Kaijser polynomial is extremal among a, suitably chosen, largeclass of homogeneous polynomials of degree two. We also study the behaviour ofthe constant $C_k(n)$ as $n \\\\to \\\\infty.$',\n",
              " 'A rooted directed tree $\\\\mathscr T=(V, E)$ with can be extended to a directedgraph $\\\\mathscr T_\\\\infty=(V_\\\\infty, E_\\\\infty)$ by adding a vertex $\\\\infty$ to$V$ and declaring each vertex in $V$ as a parent of $\\\\infty.$ One may associatewith the extended directed tree a family of semigroup structures $\\\\sqcup_{b}$with extreme ends being induced by the join operation $\\\\sqcup$ and the meetoperation $\\\\sqcap$. Each semigroup structure among these leads to a family ofdensely defined linear operators $W^{b}_{\\\\lambda_u}$ acting on $\\\\ell^2(V),$which we refer to as weighted join operators at a given base point $b \\\\inV_{\\\\infty}$ with prescribed vertex $u \\\\in V$. The extreme ends of this familyare weighted join operators $W^{\\\\mathsf{root}}_{\\\\lambda_u}$ and weighted meetoperators $W^{\\\\infty}_{\\\\lambda_u}$. In this paper, we systematically studythese operators. We also present a more involved counter-part of weighted joinoperators on rootless directed trees. In both cases, the class of weighted joinoperators overlaps with the well-studied classes of complex Jordan operatorsand $n$-symmetric operators. An important half of this paper is devoted to thestudy of rank one extensions $W_{f, g}$ of weighted join operators, where $f\\\\in \\\\ell^2(V)$ and $g : V \\\\to \\\\mathbb C$ is unspecified. Unlike weighted joinoperators, these operators are not necessarily closed. We provide a couple ofcompatibility conditions involving the weight system $\\\\lambda_u$ and $g$ toensure closedness of $W_{f, g}$. We discuss the role of the Gelfand-triplet inthe realization of the Hilbert space adjoint of $W_{f, g}$. Further, wedescribe various spectral parts of $W_{f, g}$ in terms of the weight system andthe tree data. We also provide sufficient conditions for $W_{f, g}$ to be asectorial operator. In case $\\\\mathscr T$ is leafless, we characterize rank oneextensions $W_{f, g}$, which admit compact resolvent.',\n",
              " 'The bulk samples of CuO and Fe-doped CuO were synthesized by ceramicsmethods. Structural and compositional analyses were performed by using X-raydiffraction, SEM, and EDAX. Through this manuscript, we are going to report theeffect of trivalent iron doping (Fe$^{3+}$) in copper (II) oxide(Cu$_{0.95}$Fe$_{0.05}$O) bulk samples on magnetic and dielectric behavior. Theparamagnetic phase has been established in CuO as a result of Fe$^{3+}$ doping.The strong correlation between magnetic and dielectric properties indicatedspin-polaron interaction at the transition temperature. Bulk CuO and alsoCu$_{0.95}$Fe$_{0.05}$O exhibit the multiferroic phase in a narrow temperaturerange (190 K to 230 K). Two transitions happened from aparamagnetic-paraelectric phase to incommensurate or asymmetricalantiferromagnetic (AF) and ferroelectric state near highest Neel temperature(TN1) ~230 K and another second phase transition, the order of AF phasetransformed to commensurate AF phase and ferroelectricity disappeared at aroundthe Neel temperature (TN2) ~210 K in all samples. This Cu$_{0.95}$Fe$_{0.05}$Owould show its potential in the spintronic application for a high dielectricconstant with low loss and high magnetic susceptibility.',\n",
              " 'We report Electron Paramagnetic Resonance measurements on single crystallineand powder samples of Nd_0.5Ca_0.5MnO_3 across the charge-ordering transitionat T_co=240 K down to the antiferromagnetic ordering transition at T_N = 140 K.The changes in the linewidth, g-factor and intensity as functions oftemperature are studied to understand the nature of spin-dynamics in thesystem. We explain the observed large decrease in the linewidth from T_N toT_co in terms of motional narrowing caused by the hopping of the Jahn-Tellerpolarons yielding an activation energy of E_a = 0.13 eV. Similar analysis ofdata on Pr_0.6Ca_0.4MnO_3 published earlier gives E_a=0.2 eV. Below T_co, theg-value increases continuously suggesting a gradual strengthening of theorbital ordering. We give a qualitative explanation of the temperaturedependence and of the maximum in the asymmetry ratio A/B observed at T_co insingle crystal spectra which also supports the themodel of motional narrowing.',\n",
              " 'Maintaining communications during major hurricanes is critically importantfor public safety operations by first responders. This requires accurateknowledge of the propagation channel during hurricane conditions. In this work,we have carried out ultra-wideband (UWB) channel measurements during hurricaneconditions ranging from Category-1 to Category-4, generated at the Wall of Wind(WoW) facility of Florida International University (FIU). Time Domain P410radios are used for channel measurements. From the empirical data analysis intime domain, we developed a UWB statistical broadband channel model forhurricanes. In particular, we characterize the effects of rain and wind speedon large scale and small scale UWB propagation parameters.',\n",
              " '3GPP LTE-Advanced has started a new study item to investigate HeterogeneousNetwork (HetNet) deployments as a cost effective way to deal with theunrelenting traffic demand. HetNets consist of a mix of macrocells, remoteradio heads, and low-power nodes such as picocells, femtocells, and relays.Leveraging network topology, increasing the proximity between the accessnetwork and the end-users, has the potential to provide the next significantperformance leap in wireless networks, improving spatial spectrum reuse andenhancing indoor coverage. Nevertheless, deployment of a large number of smallcells overlaying the macrocells is not without new technical challenges. Inthis article, we present the concept of heterogeneous networks and alsodescribe the major technical challenges associated with such networkarchitecture. We focus in particular on the standardization activities withinthe 3GPP related to enhanced inter-cell interference coordination.',\n",
              " 'In an uncoordinated network, the link performance between the devices mightdegrade significantly due to the interference from other links in the networksharing the same spectrum. As a solution, in this study, the concept ofpartially overlapping tones (POT) is introduced. The interference energyobserved at the victim receiver is mitigated by partially overlapping theindividual subcarriers via an intentional carrier frequency offset between thelinks. Also, it is shown that while orthogonal transformations at the receivercannot mitigate the other-user interference without losing spectral efficiency,non-orthogonal transformations are able to mitigate the other-user interferencewithout any spectral efficiency loss at the expense of self-interference. Usingspatial Poisson point process, a tractable bit error rate analysis is providedto demonstrate potential benefits emerging from POT.',\n",
              " 'The handover process is one of the most critical functions in a cellularnetwork, and is in charge of maintaining seamless connectivity of userequipments (UEs) across multiple cells. It is usually based on signalmeasurements from the neighboring base stations (BSs), and it is adverselyaffected by the time and frequency selectivity of the radio propagationchannel. In this paper, we introduce a new model for analyzing handoverperformance in heterogeneous networks (HetNets) as a function of vehicular uservelocity, cell size, and mobility management parameters. In order toinvestigate the impact of shadowing and fading on handover performance, weextract relevant statistics obtained from a 3rd Generation Partnership Project(3GPP)-compliant HetNet simulator, and subsequently, we integrate thesestatistics into our analytical model to analyze handover failure probabilityunder fluctuating channel conditions. Computer simulations validate theanalytical findings, which show that fading can significantly degrade thehandover performance in HetNets with vehicular users.',\n",
              " 'Effective emergency and natural disaster management depend on the efficientmission-critical voice and data communication between first responders andvictims. Land Mobile Radio System (LMRS) is a legacy narrowband technology usedfor critical voice communications with limited use for data applications.Recently Long Term Evolution (LTE) emerged as a broadband communicationtechnology that has a potential to transform the capabilities of public safetytechnologies by providing broadband, ubiquitous, and mission-critical voice anddata support. For example, in the United States, FirstNet is building anationwide coast-to-coast public safety network based of LTE broadbandtechnology. This paper presents a comparative survey of legacy and theLTE-based public safety networks, and discusses the LMRS-LTE convergence aswell as mission-critical push-to-talk over LTE. A simulation study of LMRS andLTE band class 14 technologies is provided using the NS-3 open source tool. Anexperimental study of APCO-25 and LTE band class 14 is also conducted usingsoftware-defined radio, to enhance the understanding of the public safetysystems. Finally, emerging technologies that may have strong potential for usein public safety networks are reviewed.',\n",
              " 'The cyber-physical nature of the smart grid has rendered it vulnerable to amultitude of attacks that can occur at its communication, networking, andphysical entry points. Such cyber-physical attacks can have detrimental effectson the operation of the grid as exemplified by the recent attack which caused ablackout of the Ukranian power grid. Thus, to properly secure the smart grid,it is of utmost importance to: a) understand its underlying vulnerabilities andassociated threats, b) quantify their effects, and c) devise appropriatesecurity solutions. In this paper, the key threats targeting the smart grid arefirst exposed while assessing their effects on the operation and stability ofthe grid. Then, the challenges involved in understanding these attacks anddevising defense strategies against them are identified. Potential solutionapproaches that can help mitigate these threats are then discussed. Last, anumber of mathematical tools that can help in analyzing and implementingsecurity solutions are introduced. As such, this paper will provide the firstcomprehensive overview on smart grid security.',\n",
              " \"Passive Radio-Frequency IDentification (RFID) systems carry criticalimportance for Internet of Things (IoT) applications due to their energyharvesting capabilities. RFID based position estimation, in particular, isexpected to facilitate a wide array of location based services for IoTapplications with low-power requirements. In this paper, considering monostaticand bistatic configurations and 3D antenna radiation pattern, we investigatethe accuracy of received signal strength based wireless localization usingpassive ultra high frequency (UHF) RFID systems. The Cramer-Rao Lower Bound(CRLB) for the localization accuracy is derived, and is compared with theaccuracy of maximum likelihood estimators for various RFID antennaconfigurations. Numerical results show that due to RFID tag/antennasensitivity, and the directional antenna pattern, the localization accuracy candegrade at blind locations that remain outside of the RFID reader antennas'main beam patterns. In such cases optimizing elevation angle of antennas areshown to improve localization coverage, while using bistatic configurationimproves localization accuracy significantly.\",\n",
              " 'Zone-level occupancy counting is a critical technology for smart buildingsand can be used for several applications such as building energy management,surveillance, and public safety. Existing occupancy counting techniquestypically require installation of large number of occupancy monitoring sensorsinside a building as well as an established network. In this study, in order toachieve occupancy counting, we consider the use of WiFi probe requests that arecontinuously transmitted from WiFi enabled smart devices. To this end, WiFiPineapple equipment are used for passively capturing ambient probe requestsfrom WiFi devices such as smart phones and tablets, where no connectivity to aWiFi network is required. This information is then used to localize userswithin coarsely defined occupancy zones, and subsequently to obtain occupancycount within each zone at different time scales. An interacting multi-modelKalman filter technique is developed to improve occupancy counting accuracy.Our numerical results using WiFi data collected at a university building showthat utilization of WiFi probe requests can be considered a viable solutiontool for zone-level occupancy counting in smart buildings.',\n",
              " 'Establishing a reliable communication infrastructure at an emergency site isa crucial task for mission-critical and real-time public safety communications(PSC). To this end, use of the unmanned aerial vehicles (UAVs) has recentlyreceived extensive interest for PSC to establish reliable connectivity in aheterogeneous network (HetNet) environment. These UAVs can be deployed asunmanned aerial base stations (UABSs) as a part of HetNet infrastructure. Inthis article, we explore the role of agile UABSs in LTE-Advanced HetNets byapplying 3GPP Release 11 further-enhanced inter-cell interference coordination(FeICIC) and cell range expansion (CRE) techniques. Through simulations, wecompare the system-wide 5th percentile spectral efficiency (SE) when UABSs aredeployed in a hexagonal grid and when their locations are optimized using agenetic algorithm, while also jointly optimizing the CRE and the FeICICparameters. Our simulation results show that at optimized UABS locations, the3GPP Release 11 FeICIC with reduced power subframes can provide considerablybetter 5th percentile SE than the 3GPP Release~10 with almost blank subframes.',\n",
              " 'Pilot contamination is known to be one of the main bottlenecks for massivemulti-input multi-output (MIMO) networks. For moderately large antenna arrays(of importance to recent/emerging deployments) and correlated MIMO, pilotcontamination may not be the dominant limiting factor in certain scenarios. Tothe best of our knowledge, a rigorous characterization of the achievable ratesand their explicit dependence on the angular spread (AS) is not available inthe existing literature for moderately large antenna array regime. In thispaper, considering eigen-beamforming (EBF) precoding, we derive an exactanalytical expression for achievable rates in multi-cell MIMO systems underpilot contamination, and characterize the relation between the AS, array size,and respective user rates. Our analytical and simulation results reveal thatthe achievable rates for both the EBF and the regularized zero-forcing (RZF)precoders follow a non-monotonic behavior for increasing AS when the antennaarray size is moderate. We argue that knowledge of this non-monotonic behaviorcan be exploited to develop effective user-cell pairing techniques.',\n",
              " 'Communication at mmWave bands carries critical importance for 5G wirelessnetworks. In this paper, we study the characterization of mmWave air-to-ground(AG) channels for unmanned aerial vehicle (UAV) communications. In particular,we use ray tracing simulations using Remcom Wireless InSite software to studythe behavior of AG mmWave bands at two different frequencies: 28~GHz and60~GHz. Received signal strength (RSS) and root mean square delay spread(RMS-DS) of multipath components (MPCs) are analyzed for different UAV heightsconsidering four different environments: urban, suburban, rural, and over sea.It is observed that the RSS mostly follows the two ray propagation model alongthe UAV flight path for higher altitudes. This two ray propagation model isaffected by the presence of high rise scatterers in urban scenario. Moreover,we present details of a universal serial radio peripheral (USRP) based channelsounder that can be used for AG channel measurements for mmWave (60 GHz) UAVcommunications.',\n",
              " 'In the modern era of radio frequency (RF) spectrum crunch, visible lightcommunication (VLC) is a recent and promising alternative technology thatoperates at the visible light spectrum. Thanks to its unlicensed and largebandwidth, VLC can deliver high throughput, better energy efficiency, and lowcost data communications. In this article, a hybrid RF/VLC architecture isconsidered that can simultaneously provide light- ing and communicationcoverage across a room. Considered architecture involves a novel multi-elementhemispherical bulb design, which can transmit multiple data streams over lightemitting diode (LED) modules. Simulations considering various VLC transmitterconfigurations and topologies show that good link quality and high spatialreuse can be maintained in typical indoor communication scenarios.',\n",
              " 'Ensuring ubiquitous mission-critical public safety communications (PSC) toall the first responders in the public safety network (PSN) is crucial at anemergency site. Recently, the use of unmanned aerial vehicles (UAVs) hasreceived extensive interest for PSC to fill the coverage holes and establishreliable connectivity. The UAVs can be deployed as unmanned aerial basestations (UABSs) as part of a heterogeneous network (HetNet) PSCinfrastructure. In this article, we address the inter-cell interferencelimiting factor in LTE-Advanced HetNets by applying 3GPP Release-11further-enhanced inter-cell interference coordination (FeICIC) and cell rangeexpansion (CRE) to enhance the system-wide spectral efficiency (SE). Throughsimulation with different path-loss models, we compare the system-wide 5thpercentile SE when UABSs are deployed in a hexagonal grid and when theirlocations are optimized using a genetic algorithm, while also jointlyoptimizing the CRE and the FeICIC parameters. Our results show that atoptimized UABS locations, the 3GPP Release-11 FeICIC with reduced powersubframes can provide considerably better 5th percentile SE than the 3GPPRelease-10 with almost blank subframes',\n",
              " 'The fifth generation (5G) wireless network technology is to be standardizedby 2020, where main goals are to improve capacity, reliability, and energyefficiency, while reducing latency and massively increasing connection density.An integral part of 5G is the capability to transmit touch perception typereal-time communication empowered by applicable robotics and haptics equipmentat the network edge. In this regard, we need drastic changes in networkarchitecture including core and radio access network (RAN) for achievingend-to-end latency on the order of 1 ms. In this paper, we present a detailedsurvey on the emerging technologies to achieve low latency communicationsconsidering three different solution domains: RAN, core network, and caching.We also present a general overview of 5G cellular networks composed of softwaredefined network (SDN), network function virtualization (NFV), caching, andmobile edge computing (MEC) capable of meeting latency and other 5Grequirements.',\n",
              " 'In this paper, we investigate the propagation coupling loss (captures allsources of attenuation between serving cell and mobile station (MS)) andgeometry metric (GM) (downlink average signal-to-interference plus noise ratio)performance of mmWave cellular networks for outdoor and indoor MSs, consideringurban micro (UMi) environments. Based on these studies, we identify effectivemmWave frequency bands for cellular communication. We consider 3GPP compliantsystem-level simulations with two power allocation schemes: 1) transmit powerscaled with communication bandwidth, and 2) constant total transmit power.Simulation results show that with scaled transmit power allocation, GMperformance degradation is small: 20% of MSs experience GM less than 0 dB atall mmWave frequencies considered, for outdoor MSs. With constant Tx powerallocation, 20% of MSs experience GM less than 0 dB for frequencies up to 30GHz. Furthermore, 35% (48%) of outdoor MSs experience GM performance less than0 dB at 60 GHz (100 GHz). On the other hand, for indoor MSs, even with scaledTx power allocation, favorable GM performance is observed only at lowfrequencies, i.e., 2 GHz.',\n",
              " 'Conventional correlation-based frame synchronization techniques can suffersignificant performance degradation over multi-path frequency-selectivechannels. As a remedy, in this paper we consider joint frame synchronizationand channel estimation. This, however, increases the length of the resultingcombined channel and its estimation becomes more challenging. On the otherhand, since the combined channel is a sparse vector, sparse channel estimationmethods can be applied. We propose a joint frame synchronization and channelestimation method using the orthogonal matching pursuit (OMP) algorithm whichexploits the sparsity of the combined channel vector. Subsequently, the channelestimate is used to design the equalizer. Our simulation results andexperimental outcomes using software defined radios show that the proposedapproach improves the overall system performance in terms of the mean squareerror (MSE) between the transmitted and the equalized symbols compared to theconventional method.',\n",
              " 'Efficient symbol detection algorithms carry critical importance for achievingthe spatial multiplexing gains promised by multi-input multi-output (MIMO)systems. In this paper, we consider a maximum a posteriori probability (MAP)based symbol detection algorithm, called M-BLAST, over uncoded quasi-staticMIMO channels. Relying on the successive interference cancellation (SIC)receiver, M-BLAST algorithm offers a superior error performance over itspredecessor V-BLAST with a signal-to-noise ratio (SNR) gain of as large as 2 dBunder various settings of recent interest. Performance analysis of the M-BLASTalgorithm is very complicated since the proposed detection order depends on thedecision errors dynamically, which makes an already complex analysis of theconventional ordered SIC receivers even more difficult. To this end, a rigorousanalytical framework is proposed to analyze the outage behavior of the M-BLASTalgorithm over binary complex alphabets and two transmitting antennas, whichhas a potential to be generalized to multiple transmitting antennas andmultidimensional constellation sets. The numerical results show a very goodmatch between the analytical and simulation data under various SNR values andmodulation alphabets.',\n",
              " 'Visible Light Communications (VLC) has been studied thoroughly in recentyears as an alternative or complementary technology to radio frequencycommunications. The reliability of VLC channels highly depends on theavailability and alignment of line of sight links. In this work, we study theeffect of random receiver orientation for mobile users over VLC downlinkchannels, which affects the existence of line of sight links and the receiverfield of view. Based on the statistics of vertical receiver orientation anduser mobility, we develop a unified analytical framework to characterize thestatistical distribution of VLC downlink channels, which is then utilized toobtain the outage probability and the bit error rate. Our analysis isgeneralized for arbitrary distributions of receiver orientation/location for asingle transmitter, and extended to multiple transmitter case for certainscenarios. Extensive Monte Carlo simulations show a perfect match between theanalytical and the simulation data in terms of both the statistical channeldistribution and the resulting bit error rate. Our results also characterizethe channel attenuation due to random receiver orientation/location for variousscenarios of interest.',\n",
              " 'In recent years, there has been a dramatic increase in the use of unmannedaerial vehicles (UAVs), particularly for small UAVs, due to their affordableprices, ease of availability, and ease of operability. Existing and futureapplications of UAVs include remote surveillance and monitoring, reliefoperations, package delivery, and communication backhaul infrastructure.Additionally, UAVs are envisioned as an important component of 5G wirelesstechnology and beyond. The unique application scenarios for UAVs necessitateaccurate air-to-ground (AG) propagation channel models for designing andevaluating UAV communication links for control/non-payload as well as payloaddata transmissions. These AG propagation models have not been investigated indetail when compared to terrestrial propagation models. In this paper, acomprehensive survey is provided on available AG channel measurement campaigns,large and small scale fading channel models, their limitations, and futureresearch directions for UAV communication scenarios.',\n",
              " 'Localization of a radio frequency (RF) transmitter with intermittenttransmissions is considered via a group of unmanned aerial vehicles (UAVs)equipped with omnidirectional received signal strength (RSS) sensors. Thisgroup embarks on an autonomous patrol to localize and track the target with aspecified accuracy, as quickly as possible. The challenge can be decomposedinto two stages: 1) estimation of the target position given previousmeasurements (localization), and 2) planning the future trajectory of thetracking UAVs to get lower expected localization error given current estimation(path planning). For each stage we compare two algorithms in terms ofperformance and computational load. For the localization stage, we compare adetection based extended Kalman filter (EKF) and a recursive Bayesianestimator. For the path planning stage, we compare steepest descent posteriorCramer-Rao lower bound (CRLB) path planning and a bio-inspired heuristic pathplanning. Our results show that the steepest descent path planning outperformsthe bio-inspired path planning by an order of magnitude, and recursive Bayesianestimator narrowly outperforms detection based EKF.',\n",
              " 'We consider a downlink multiuser visible light communications (VLC) networkwhere users randomly change their location and vertical orientation. Thenon-orthogonal multiple access (NOMA) strategy is adopted to serve multipleusers simultaneously, and, hence, to improve spectral efficiency. We proposetwo novel user scheduling schemes for NOMA, which are referred to as individualand group-based. In order to further reduce the computational complexity andlink overhead, novel limited-feedback schemes (on channel quality) are alsoproposed, which basically involve mean vertical angle (instead of itsinstantaneous value). Moreover, a two-bit feedback scheme is proposed forgroup-based user scheduling, which relies on not only distance but alsovertical angle (in contrast to conventional one-bit feedback with distanceonly). The outage probability and sum-rate expressions are derivedanalytically, which show a very good match with the simulation data. Numericalresults verify that the practical feedback scheme with the mean vertical angleachieves a near-optimal sum-rate performance, and the two-bit feedbacksignificantly outperforms the one-bit feedback.',\n",
              " 'The overheads associated with feedback-based channel acquisition can greatlycompromise the achievable rates of FDD based massive MIMO systems. Indeed,downlink (DL) training and uplink (UL) feedback overheads scale linearly withthe number of base station (BS) antennas, in sharp contrast to TDD-basedmassive MIMO, where a single UL pilot trains the whole BS array. In this work,we propose a graph-theoretic approach to reducing DL training and UL feedbackoverheads in FDD massive MIMO systems. In particular, we consider a single-cellscenario involving a single BS with a massive antenna array serving tosingle-antenna mobile stations (MSs) in the DL. We assume the BS employstwo-stage beamforming in the DL, comprising DFT pre-beamforming followed byMU-MIMO precoding. The proposed graph-theoretic approach exploits knowledge ofthe angular spectra of the BS-MS channels to construct DL training protocolswith reduced overheads. Simulation results reveal that the proposedtraining-resources allocation method can provide approximately 35% sum-rateperformance gain compared to conventional orthogonal training. Our analysisalso sheds light into the impact of overhead reduction on channel estimationquality, and, in turn, achievable rates.',\n",
              " '5G millimeter wave (mmWave) technology is envisioned to be an integral partof next-generation vehicle-to-everything (V2X) networks and autonomous vehiclesdue to its broad bandwidth, wide field of view sensing, and preciselocalization capabilities. The reliability of mmWave links may be compromiseddue to difficulties in beam alignment for mobile channels and due to blockingeffects between a mmWave transmitter and a receiver. To address suchchallenges, out-of-band information from sub-6 GHz channels can be utilized forpredicting the temporal and angular channel characteristics in mmWave bands,which necessitates a good understanding of how propagation characteristics arecoupled across different bands. In this paper, we use ray tracing simulationsto characterize the angular and temporal correlation across a wide range ofpropagation frequencies for V2X channels ranging from 900 MHz up to 73 GHz, fora vehicle maintaining line-of-sight (LOS) and non-LOS (NLOS) beams with atransmitter in an urban environment. Our results shed light on increasingsparsity behavior of propagation channels with increasing frequency andhighlight the strong temporal/angular correlation among 5.9 GHz and 28 GHzbands especially for LOS channels.',\n",
              " 'Magnetic high entropy alloys (HEAs) are a new category of high-performancemagnetic materials, with multi-component concentrated compositions and complexmulti-phase structures. Although there have been numerous reports of theirinteresting magnetic properties, there is very limited understanding about theinterplay between their hierarchical multi-phase structures and their localmagnetic structures. By employing high spatial resolution correlative magnetic,structural and chemical studies, we reveal the influence of a hierarchicallydecomposed B2 + A2 structure in an AlCo0.5Cr0.5FeNi HEA on the formation ofmagnetic vortex states within individual A2 (disordered BCC) precipitates,which are distributed in an ordered B2 matrix that is weakly ferromagnetic.Non-magnetic or weakly ferromagnetic B2 precipitates in large magnetic domainsof the A2 phase, and strongly magnetic Fe-Co-rich interphase A2 regions, arealso observed. These results provide important insight into the origin ofcoercivity in this HEA, which can be attributed to a complex magnetizationprocess that includes the successive reversal of magnetic vortices.',\n",
              " 'This study presents a distributed gradient-based approach to solve systemoptimal dynamic traffic assignment (SODTA) formulated based on the celltransmission model. The algorithm distributes SODTA into local sub-problems,who find optimal values for their decision variables within an intersection.Each sub-problem communicates with its immediate neighbors to reach a consensuson the values of common decision variables. A sub-problem receives proposedvalues for common decision variables from all adjacent sub-problems andincorporates them into its own offered values by weighted averaging andenforcing a gradient step to minimize its objective function. Then, the updatedvalues are projected onto the feasible region of the sub-problems. Thealgorithm finds high quality solutions in all tested scenarios with a finitenumber of iterations. The algorithm is tested on a case study network underdifferent demand levels and finds solutions with at most a 5% optimality gap.',\n",
              " 'This study presents a vehicle-level distributed coordination strategy tocontrol a mixed traffic stream of connected automated vehicles (CAVs) andconnected human-driven vehicles (CHVs) through signalized intersections. We useCAVs as mobile traffic controllers during a newly introduced white phase,during which CAVs will negotiate the right-of-way to lead a group of CHVs whileCHVs must follow their immediate front vehicle. The white phase will not beactivated under low CAV penetration rates, where vehicles must wait for greensignals. We have formulated this problem as a distributed mixed-integernon-linear program and developed a methodology to form an agreement among allvehicles on their trajectories and signal timing parameters. The agreement ontrajectories is reached through an iterative process, where CAVs update theirtrajectory based on shared trajectory of other vehicles to avoid collisions andshare their trajectory with other vehicles. Additionally, the agreement onsignal timing parameters is formed through a voting process where the mostvoted feasible signal timing parameters are selected. The numerical experimentsindicate that the proposed methodology can efficiently control vehiclemovements at signalized intersections under various CAV market shares. Theintroduced white phase reduces the total delay by 3.2% to 94.06% compared tocooperative trajectory and signal optimization under different CAV marketshares in our tests. In addition, our numerical results show that the proposedtechnique yields reductions in total delay, ranging from 40.2% - 98.9%,compared to those of a fully-actuated signal control obtained from astate-of-practice traffic signal optimization software.',\n",
              " 'This paper analyzes the potential effects of connected and automated vehicleson saturation headway and capacity at signalized intersections. A signalizedintersection is created in Vissim as a testbed, where four vehicle types aremodeled and tested: (I) human-driven vehicles (HVs), (II) connected vehicles(CVs), (III) automated vehicles (AVs), and (IV) connected automated vehicles(CAVs). Various scenarios are defined based on different market penetrationrates of these four vehicle types. AVs are assumed to move more cautiouslycompared to human drivers. CVs and CAVs are supposed to receive informationabout the future state of traffic lights and adjust their speeds to avoidstopping at the intersection. As a result, their movements are expected to besmoother with a lower number of stops. The effects of these vehicle types inmixed traffic are investigated in terms of saturation headway, capacity, traveltime, delay, and queue length in different lane groups of an intersection. APython script code developed by Vissim is used to provide the communicationbetween the signal controller and CVs and CAVs to adjust their speedsaccordingly. The results show that increasing CV and CAV market penetrationrate reduces saturation headway and consequently increases capacity atsignalized intersections. On the other hand, increasing the AV marketpenetration rate deteriorates traffic operations. Results also indicate thatthe highest increase (80%) and decrease (20%) in lane group capacity areobserved, respectively, in a traffic stream of 100% CAVs and 100% AVs.',\n",
              " 'The susceptibility of millimeter-wave (mmWave) signals to physical blockageand abrupt signal strength variations presents a challenge to reliable 5Gcommunication. This work proposes and examines the feasibility of utilizinglower-frequency signals as early-warning indicators of mobile mmWave signalblockage or recovery. A physics-based channel simulation tool incorporatingFresnel diffraction and image sources is employed to demonstrate that sub-6 GHzsignals \"lead\" mmWave signals in reaching a specific signal-strength thresholdby several to tens of milliseconds at mobile speeds, suggesting early-warningsystems are viable. This predictive approach stems from frequency-dependentproperties of diffraction and does not assume a specific topology or mobile andobstacle speeds. Realistic simulations that include transitions from line ofsight (LoS) to non-line of sight (NLoS) and reflection scenarios are employedto verify the proposed prediction capabilities. Moreover, prediction of thestrongest multipath component and its angle of arrival (AoA) using sub-6 GHzobservations is investigated.',\n",
              " 'We present a novel explanation of the well known $M_{\\\\bullet}-\\\\sigma$relation. In a triaxial potential binaries with chaotic orbits within a spherethat encompass $\\\\sim100$ times the mass of the super-massive black-hole (SMBH)have a finite probability to be tidally disrupted by the SMBH. As a result onecomponent loses energy and might itself break apart tidally and accreted ontothe SMBH. More significantly, the other component, which gains energy, returnsto the bulge and equilibrates its excess energy with the environment therebychanging the kinetic temperature, hence the velocity dispersion. We develop amathematical model and find that its results are in agreement with the observedrelation.',\n",
              " 'In many theoretical scenarios it is expected that intermediate-mass blackholes (IMBHs, with masses M ~ 100-10000 solar masses) reside at the centers ofsome globular clusters. However, observational evidence for their existence islimited. Several previous numerical investigations have focused on the impactof an IMBH on the cluster dynamics or brightness profile. Here we insteadpresent results from a large set of direct N-body simulations including singleand binary stars. These show that there is a potentially more detectable IMBHsignature, namely on the variation of the average stellar mass between thecenter and the half-light radius. We find that the existence of an IMBHquenches mass segregation and causes the average mass to exhibit only modestradial variation in collisionally relaxed star clusters. This differs from whenthere is no IMBH. To measure this observationally requires high resolutionimaging at the level of that already available from the Hubble Space Telescope(HST) for the cores of a large sample of galactic globular clusters. With amodest additional investment of HST time to acquire fields around thehalf-light radius, it will be possible to identify the best candidate clustersto harbor an IMBH. This test can be applied only to globulars with a half-lightrelaxation time less than or equal to 1 Gyr, which is required to guaranteeefficient energy equipartition due to two-body relaxation.',\n",
              " 'The theory of quandle (co)homology and cocycle knot invariants is rapidlybeing developed. We begin with a summary of these recent advances. One suchadvance is the notion of a dynamical cocycle.  We show how dynamical cocycles can be used to color knotted surfaces that areobtained from classical knots by twist-spinning. We also demonstrate relationsbetween cocycle invariants and Alexander matrices.',\n",
              " \"The recent outbreak of the COVID-19 led to the death of millions of peopleworldwide. To stave off the spread of the virus, the authorities in the USemployed different strategies, including the mask mandate order issued by thestates' governors. In the current work, we defined a parameter called theaverage death ratio as the monthly average of the number of daily deaths to themonthly average number of daily cases. We utilized survey data to quantifypeople's abidance by the mask mandate order. Additionally, we implicitlyaddressed the extent to which people abide by the mask mandate order that maydepend on some parameters like population, income, and education level. Usingdifferent machine learning classification algorithms, we investigated how thedecrease or increase in death ratio for the counties in the US West Coastcorrelates with the input parameters. The results showed that for most countiesthere, the mask mandate order decreased the death ratio reflecting theeffectiveness of this preventive measure on the West Coast. Additionally, thechanges in the death ratio demonstrated a noticeable correlation with thesocio-economic condition of each county. Moreover, the results showed apromising classification accuracy score as high as around 90%.\",\n",
              " 'We report photometric follow-up observations of thirteen exoplanets (HATS-1b, HATS2 b, HATS-3 b, HAT-P-18 b, HAT-P-27 b, HAT-P-30 b, HAT-P-55 b, KELT-4Ab, WASP-25 b, WASP-42 b, WASP-57 b, WASP-61 b and WASP-123 b), as part of theOriginal Research By Young Twinkle Students (ORBYTS) programme. All theseplanets are potentially viable targets for atmospheric characterisation and ourdata, which were taken using the LCOGT network of ground-based telescopes, willbe combined with observations from other users of ExoClock to ensure that thetransit times of these planets continue to be well-known, far into the future.',\n",
              " 'ROI extraction is an active but challenging task in remote sensing because ofthe complicated landform, the complex boundaries and the requirement ofannotations. Weakly supervised learning (WSL) aims at learning a mapping frominput image to pixel-wise prediction under image-wise labels, which candramatically decrease the labor cost. However, due to the imprecision oflabels, the accuracy and time consumption of WSL methods are relativelyunsatisfactory. In this paper, we propose a two-step ROI extraction based oncontractive learning. Firstly, we present to integrate multiscale Grad-CAM toobtain pseudo pixelwise annotations with well boundaries. Then, to reduce thecompact of misjudgments in pseudo annotations, we construct a contrastivelearning strategy to encourage the features inside ROI as close as possible andseparate background features from foreground features. Comprehensiveexperiments demonstrate the superiority of our proposal. Code is available athttps://github.com/HE-Lingfeng/ROI-Extraction',\n",
              " 'Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims tomatch pedestrian images of the same identity from different modalities withoutannotations. Existing works mainly focus on alleviating the modality gap byaligning instance-level features of the unlabeled samples. However, therelationships between cross-modality clusters are not well explored. To thisend, we propose a novel bilateral cluster matching-based learning framework toreduce the modality gap by matching cross-modality clusters. Specifically, wedesign a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)algorithm through optimizing the maximum matching problem in a bipartite graph.Then, the matched pairwise clusters utilize shared visible and infraredpseudo-labels during the model training. Under such a supervisory signal, aModality-Specific and Modality-Agnostic (MSMA) contrastive learning frameworkis proposed to align features jointly at a cluster-level. Meanwhile, thecross-modality Consistency Constraint (CC) is proposed to explicitly reduce thelarge modality discrepancy. Extensive experiments on the public SYSU-MM01 andRegDB datasets demonstrate the effectiveness of the proposed method, surpassingstate-of-the-art approaches by a large margin of 8.76% mAP on average.',\n",
              " 'Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)aims at learning modality-invariant features from unlabeled cross-modalitydataset, which is crucial for practical applications in video surveillancesystems. The key to essentially address the USL-VI-ReID task is to solve thecross-modality data association problem for further heterogeneous jointlearning. To address this issue, we propose a Dual Optimal Transport LabelAssignment (DOTLA) framework to simultaneously assign the generated labels fromone modality to its counterpart modality. The proposed DOTLA mechanismformulates a mutual reinforcement and efficient solution to cross-modality dataassociation, which could effectively reduce the side-effects of someinsufficient and noisy label associations. Besides, we further propose across-modality neighbor consistency guided label refinement and regularizationmodule, to eliminate the negative effects brought by the inaccurate supervisedsignals, under the assumption that the prediction or label distribution of eachexample should be similar to its nearest neighbors. Extensive experimentalresults on the public SYSU-MM01 and RegDB datasets demonstrate theeffectiveness of the proposed method, surpassing existing state-of-the-artapproach by a large margin of 7.76% mAP on average, which even surpasses somesupervised VI-ReID methods.',\n",
              " 'Techniques for training artificial neural networks (ANNs) and convolutionalneural networks (CNNs) using simulated dynamical electron diffraction patternsare described. The premise is based on the following facts. First, given asuitable crystal structure model and scattering potential, electron diffractionpatterns can be simulated accurately using dynamical diffraction theory.Secondly, using simulated diffraction patterns as input, ANNs can be trainedfor the determination of crystal structural properties, such as crystalorientation and local strain. Further, by applying the trained ANNs tofour-dimensional diffraction datasets (4D-DD) collected using the scanningelectron nanodiffraction (SEND) or 4D scanning transmission electron microscopy(4D-STEM) techniques, the crystal structural properties can be mapped at highspatial resolution. Here, we demonstrate the ANN-enabled possibilities for theanalysis of crystal orientation and strain at high precision and benchmark theperformance of ANNs and CNNs by comparing with previous methods. A factor ofthirty improvement in angular resolution at 0.009 degrees (0.16 mrad) fororientation mapping, sensitivity at 0.04% or less for strain mapping, andimprovements in computational performance are demonstrated.',\n",
              " 'To efficiently capture the energy of the nuclear bond, advanced nuclearreactor concepts seek solid fuels that must withstand unprecedented temperatureand radiation extremes. In these advanced fuels, thermal energy transport underirradiation is directly related to reactor performance as well as reactorsafety. The science of thermal transport in nuclear fuel is a grand challengedue to both computational and experimental complexities. Here, we provide acomprehensive review of thermal transport research on two actinide oxides: onecurrently in use in commercial nuclear reactors, uranium dioxide (UO2), and oneadvanced fuel candidate material, thorium dioxide (ThO2). In both materials,heat is carried by lattice waves or phonons. Crystalline defects caused byfission events effectively scatter phonons and lead to a degradation in fuelperformance over time. Bolstered by new computational and experimental tools,researchers are now developing the foundational work necessary to accuratelymodel and ultimately control thermal transport in advanced nuclear fuel. Webegin by reviewing research aimed at understanding thermal transport in perfectsingle crystals. The absence of defects enables studies that focus on thefundamental aspects of phonon transport. Next, we review research that targetsdefect generation and evolution. Here, the focus is on ion irradiation studiesused as surrogates for damage caused by fission products. We end this reviewwith a discussion of modeling and experimental efforts directed at predictingand validating mesoscale thermal transport in the presence of irradiationdefects. While efforts into these research areas have been robust, challengingwork remains in developing holistic tools to capture and predict thermal energytransport across widely varying environmental conditions.',\n",
              " 'In visual exploration and analysis of data, determining how to select andtransform the data for visualization is a challenge for data-unfamiliar orinexperienced users. Our main hypothesis is that for many data sets and commonanalysis tasks, there are relatively few \"data slices\" that result in effectivevisualizations. By focusing human users on appropriate and suitably transformedparts of the underlying data sets, these data slices can help the users carrytheir task to correct completion.  To verify this hypothesis, we develop a framework that permits us to captureexemplary data slices for a user task, and to explore and parsevisual-exploration sequences into a format that makes them distinct and easy tocompare. We develop a recommendation system, DataSlicer, that matches a\"currently viewed\" data slice with the most promising \"next effective\" dataslices for the given exploration task. We report the results of controlledexperiments with an implementation of the DataSlicer system, using four commonanalytical task types. The experiments demonstrate statistically significantimprovements in accuracy and exploration speed versus users without access toour system.',\n",
              " 'Stereo matching has recently witnessed remarkable progress using Deep NeuralNetworks (DNNs). But, how robust are they? Although it has been well-known thatDNNs often suffer from adversarial vulnerability with a catastrophic drop inperformance, the situation is even worse in stereo matching. This paper firstshows that a type of weak white-box attacks can overwhelm state-of-the-artmethods. The attack is learned by a proposed stereo-constrained projectedgradient descent (PGD) method in stereo matching. This observation raisesserious concerns for the deployment of DNN-based stereo matching. Parallel tothe adversarial vulnerability, DNN-based stereo matching is typically trainedunder the so-called simulation to reality pipeline, and thus domaingeneralizability is an important problem. This paper proposes to rethink thelearnable DNN-based feature backbone towards adversarially-robust and domaingeneralizable stereo matching by completely removing it for matching. Inexperiments, the proposed method is tested in the SceneFlow dataset and theKITTI2015 benchmark, with promising results. We compute the matching costvolume using the classic multi-scale census transform (i.e., local binarypattern) of the raw input stereo images, followed by a stacked Hourglass headsub-network solving the matching problem. It significantly improves theadversarial robustness, while retaining accuracy performance comparable tostate-of-the-art methods. It also shows better generalizability from simulation(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.',\n",
              " 'For a given class ${\\\\cal F}$ of uniform frames of fixed redundancy we definea Grassmannian frame as one that minimizes the maximal correlation $|< f_k,f_l>|$ among all frames $\\\\{f_k\\\\}_{k \\\\in {\\\\cal I}} \\\\in {\\\\cal F}$. We first analyzefinite-dimensional Grassmannian frames. Using links to packings in Grassmannianspaces and antipodal spherical codes we derive bounds on the minimal achievablecorrelation for Grassmannian frames. These bounds yield a simple conditionunder which Grassmannian frames coincide with uniform tight frames. We exploitconnections to graph theory, equiangular line sets, and coding theory in orderto derive explicit constructions of Grassmannian frames. Our findings extendrecent results on uniform tight frames. We then introduce infinite-dimensionalGrassmannian frames and analyze their connection to uniform tight frames forframes which are generated by group-like unitary systems. We derive an exampleof a Grassmannian Gabor frame by using connections to sphere packing theory.Finally we discuss the application of Grassmannian frames to wirelesscommunication and to multiple description coding.',\n",
              " \"Blended courses have become the norm in post-secondary education.Universities use large-scale learning management systems to manage classcontent. Instructors deliver readings, lectures, and office hours online;students use intelligent tutors, web forums, and online submission systems; andclasses communicate via web forums. These online tools allow students to formnew social networks or bring social relationships online. They also allow us tocollect data on students' social relationships. In this paper we report on ourresearch on community formation in blended courses based on online foruminteractions. We found that it was possible to group students into communitiesusing standard community detection algorithms via their posts and replystructure and that the students' grades are significantly correlated with theirclosest peers.\",\n",
              " \"Online tools provide unique access to research students' study habits andproblem-solving behavior. In MOOCs, this online data can be used to informinstructors and to provide automatic guidance to students. However, thesetechniques may not apply in blended courses with face to face and onlinecomponents. We report on a study of integrated user-system interaction logsfrom 3 computer science courses using four online systems: LMS, forum, versioncontrol, and homework system. Our results show that students rarely work acrossplatforms in a single session, and that final class performance can bepredicted from students' system use.\",\n",
              " \"Students' interactions with online tools can provide us with insights intotheir study and work habits. Prior research has shown that these habits, evenas simple as the number of actions or the time spent on online platforms candistinguish between the higher performing students and low-performers. Thesehabits are also often used to predict students' performance in classes. One keyfeature of these actions that is often overlooked is how and when the studentstransition between different online platforms. In this work, we study sequencesof student transitions between online tools in blended courses and identifywhich habits make the most difference between the higher and lower performinggroups. While our results showed that most of the time students focus on asingle tool, we were able to find patterns in their transitions todifferentiate high and low performing groups. These findings can helpinstructors to provide procedural guidance to the students, as well as toidentify harmful habits and make timely interventions.\",\n",
              " 'Computer science educators seek to understand the types of mistakes thatstudents make when learning a new (programming) language so that they can helpstudents avoid those mistakes in the future. While educators know what mistakesstudents regularly make in languages such as C and Python, students strugglewith SQL and regularly make mistakes when working with it. We present ananalysis of mistakes that students made when first working with SQL, classifythe types of errors introduced, and provide suggestions on how to avoid themgoing forward. In addition, we present an automated tool, SQLRepair, that iscapable of repairing errors introduced by undergraduate programmers whenwriting SQL queries. Our results show that students find repairs produced byour tool comparable in understandability to queries written by themselves or byother students, suggesting that SQL repair tools may be useful in aneducational context. We also provide to the community a benchmark of SQLqueries written by the students in our study that we used for evaluation ofSQLRepair.',\n",
              " \"The computing education community endeavors to consistently move forward,improving the educational experience of our students. As new innovations incomputing education practice are learned and shared, however, these papers maynot exhibit the desired qualities that move simple experience reports to trueScholarship of Teaching and Learning (SoTL). We report on our six years ofexperience in running professional development for computing educators inempirical research methods for social and behavioral studies in the classroom.Our goal is to have a direct impact on instructors who are in the beginningstages of transitioning their educational innovations from anecdotal toempirical results that can be replicated by instructors at other institutions.To achieve this, we created a year-long mentoring experience, beginning with amulti-day workshop on empirical research methods during the summer, followed byregular mentoring sessions with participants, and culminating in a follow-upsession at the following year's SIGCSE Technical Symposium. From survey resultsand as evidenced by eventual research results and publications fromparticipants, we believe that our method of structuring empirical researchprofessional development was successful and could be a model for similarprograms in other areas.\",\n",
              " 'Classroom dashboards are designed to help instructors effectively orchestrateclassrooms by providing summary statistics, activity tracking, and otherinformation. Existing dashboards are generally specific to an LMS or platformand they generally summarize individual work, not group behaviors. However, CScourses typically involve constellations of tools and mix on- and offlinecollaboration. Thus, cross-platform monitoring of individuals and teams isimportant to develop a full picture of the class. In this work, we describe ourwork on Concert, a data integration platform that collects data about studentactivities from several sources such as Piazza, My Digital Hand, and GitHub anduses it to support classroom monitoring through analysis and visualizations. Wediscuss team visualizations that we have developed to support effective groupmanagement and to help instructors identify teams in need of intervention.',\n",
              " \"The large-scale online management systems (e.g. Moodle), online web forums(e.g. Piazza), and online homework systems (e.g. WebAssign) have been widelyused in the blended courses recently. Instructors can use these systems todeliver class content and materials. Students can communicate with theclassmates, share the course materials, and discuss the course questions viathe online forums. With the increased use of the online systems, a large amountof students' interaction data has been collected. This data can be used toanalyze students' learning behaviors and predict students' learning outcomes.In this work, we collected students' interaction data in three differentblended courses. We represented the data as directed graphs and investigatedthe correlation between the social graph properties and students' final grades.Our results showed that in all these classes, students who asked more answersand received more feedbacks on the forum tend to obtain higher grades. Thesignificance of this work is that we can use the results to encourage studentsto participate more in forums to learn the class materials better; we can alsobuild a predictive model based on the social metrics to show us low performingstudents early in the semester.\",\n",
              " \"Blended courses that mix in-person instruction with online platforms areincreasingly popular in secondary education. These tools record a rich amountof data on students' study habits and social interactions. Prior research hasshown that these metrics are correlated with students' performance in face toface classes. However, predictive models for blended courses are still limitedand have not yet succeeded at early prediction or cross-class predictions evenfor repeated offerings of the same course.  In this work, we use data from two offerings of two different undergraduatecourses to train and evaluate predictive models on student performance basedupon persistent student characteristics including study habits and socialinteractions. We analyze the performance of these models on the same offering,on different offerings of the same course, and across courses to see how wellthey generalize. We also evaluate the models on different segments of thecourses to determine how early reliable predictions can be made. This worktells us in part how much data is required to make robust predictions and howcross-class data may be used, or not, to boost model performance. The resultsof this study will help us better understand how similar the study habits,social activities, and the teamwork styles are across semesters for students ineach performance category. These trained models also provide an avenue toimprove our existing support platforms to better support struggling studentsearly in the semester with the goal of providing timely intervention.\",\n",
              " \"Teamwork, often mediated by version control systems such as Git and ApacheSubversion (SVN), is central to professional programming. As a consequence,many colleges are incorporating both collaboration and online developmentenvironments into their curricula even in introductory courses. In thisresearch, we collected GitHub logs from two programming projects in twoofferings of a CS2 Java programming course for computer science majors.Students worked in pairs for both projects (one optional, the other mandatory)in each year. We used the students' GitHub history to classify the studentteams into three groups, collaborative, cooperative, or solo-submit, based onthe division of labor. We then calculated different metrics for students'teamwork including the total number and the average number of commits indifferent parts of the projects and used these metrics to predict the students'teamwork style. Our findings show that we can identify the students' teamworkstyle automatically from their submission logs. This work helps us to betterunderstand novices' habits while using version control systems. These habitscan identify the harmful working styles among them and might lead to thedevelopment of automatic scaffolds for teamwork and peer support in the future.\",\n",
              " 'Computing Education Research (CER) is critical for supporting the increasingnumber of students who need to learn computing skills. To systematicallyadvance knowledge, publications must be clear enough to support replications,meta-analyses, and theory-building. The goal of this study is to characterizethe reporting of empiricism in CER literature by identifying whetherpublications include information to support replications, meta-analyses, andtheory building. The research questions are: RQ1) What percentage of papers inCER venues have empirical evaluation? RQ2) What are the characteristics of theempirical evaluation? RQ3) Do the papers with empirical evaluation followreporting norms (both for inclusion and for labeling of key information)? Weconducted an SLR of 427 papers published during 2014 and 2015 in five CERvenues: SIGCSE TS, ICER, ITiCSE, TOCE, and CSE. We developed and applied theCER Empiricism Assessment Rubric. Over 80% of papers had some form of empiricalevaluation. Quantitative evaluation methods were the most frequent. Papers mostfrequently reported results on interventions around pedagogical techniques,curriculum, community, or tools. There was a split in papers that had some typeof comparison between an intervention and some other data set or baseline. Manypapers lacked properly reported research objectives, goals, research questions,or hypotheses, description of participants, study design, data collection, andthreats to validity. CER authors are contributing empirical results to theliterature; however, not all norms for reporting are met. We encourage authorsto provide clear, labeled details about their work so readers can use themethodologies and results for replications and meta-analyses. As our communitygrows, our reporting of CER should mature to help establish computing educationtheory to support the next generation of computing learners.',\n",
              " 'Let $V$ be a possibly singular scheme-theoretic complete intersectionsubscheme of $\\\\mathbb{P}^n$ over an algebraically closed field ofcharacteristic zero. Using a recent result of Fullwood (\"On Milnor classes viainvariants of singular subschemes\", Journal of Singularities) we develop analgorithm to compute the Chern-Schwartz-MacPherson class and Eulercharacteristic of $V$. This algorithm complements existing algorithms byproviding performance improvements in the computation of theChern-Schwartz-MacPherson class and Euler characteristic for certain types ofcomplete intersection subschemes of $\\\\mathbb{P}^n$.',\n",
              " 'Topological invariants such as characteristic classes are an important toolto aid in understanding and categorizing the structure and properties ofalgebraic varieties. In this note we consider the problem of computing aparticular characteristic class, the Chern-Schwartz-MacPherson class, of acomplete simplicial toric variety X defined by a fan from the combinatorialdata contained in the fan. Specifically, we give an effective combinatorialalgorithm to compute the Chern-Schwartz-MacPherson class of X, in the Chow ring(or rational Chow ring) of X. This method is formulated by combining, and whennecessary modifying, several known results from the literature and isimplemented in Macaulay2 for test purposes.',\n",
              " 'We determine the Euclidean distance degree of a projective toric variety.This extends the formula of Matsui and Takeuchi for the degree of the$A$-discriminant in terms of Euler obstructions. Our primary goal is thedevelopment of reliable algorithmic tools for computing the points on a realtoric variety that are closest to a given data point.',\n",
              " 'We study questions of existence and uniqueness of quadrature domains usingcomputational tools from real algebraic geometry. These problems aretransformed into questions about the number of solutions to an associated realsemi-algebraic system, which is analyzed using the method of real comprehensivetriangular decomposition.',\n",
              " 'Let $V$ be a closed subscheme of a projective space $\\\\mathbb{P}^n$. We givean algorithm to compute the Chern-Schwartz-MacPherson class, Eulercharacteristic and Segre class of $ V$. The algorithm can be implemented usingeither symbolic or numerical methods. The algorithm is based on a new methodfor calculating the projective degrees of a rational map defined by ahomogeneous ideal. Using this result and known formulas for theChern-Schwartz-MacPherson class of a projective hypersurface and the Segreclass of a projective variety in terms of the projective degrees of certainrational maps we give algorithms to compute the Chern-Schwartz-MacPherson classand Segre class of a projective variety. Since the Euler characteristic of $V$is the degree of the zero dimensional component of theChern-Schwartz-MacPherson class of $V$ our algorithm also computes the Eulercharacteristic $\\\\chi(V)$. Relationships between the algorithm developed hereand other existing algorithms are discussed. The algorithm is tested on severalexamples and performs favourably compared to current algorithms for computingChern-Schwartz-MacPherson classes, Segre classes and Euler characteristics.',\n",
              " 'We derive a general formula for the Euler characteristic of a fibration ofprojective hypersurfaces in terms of invariants of an arbitrary base variety.When the general fiber is an elliptic curve, such formulas have appeared in thephysics literature in the context of calculating D-brane charge for M-/F-theoryand type-IIB compactifications of string vacua. While there are various methodsfor computing Euler characteristics of algebraic varieties, we prove abase-independent pushforward formula which reduces the computation of the Eulercharacteristic of relative hypersurfaces to simple algebraic manipulations ofrational expressions determined by its divisor class in a projective bundle. Weillustrate our methods by applying them to an explicit family of relativehypersurfaces whose fibers are of arbitrary dimension and degree.',\n",
              " 'We study the maximum likelihood degree (ML degree) of toric varieties, knownas discrete exponential models in statistics. By introducing scalingcoefficients to the monomial parameterization of the toric variety, one canchange the ML degree. We show that the ML degree is equal to the degree of thetoric variety for generic scalings, while it drops if and only if the scalingvector is in the locus of the principal $A$-determinant. We also illustrate howto compute the ML estimate of a toric variety numerically via homotopycontinuation from a scaled toric variety with low ML degree. Throughout, weinclude examples motivated by algebraic geometry and statistics. We compute theML degree of rational normal scrolls and a large class of Veronese-typevarieties. In addition, we investigate the ML degree of scaled Segre varieties,hierarchical loglinear models, and graphical models.',\n",
              " 'Let $X \\\\subset Y$ be closed (possibly singular) subschemes of a smoothprojective toric variety $T$. We show how to compute the Segre class $s(X,Y)$as a class in the Chow group of $T$. Building on this, we give effectivemethods to compute intersection products in projective varieties, to determinealgebraic multiplicity without working in local rings, and to test pairwisecontainment of subvarieties of $T$. Our methods may be implemented withoutusing Groebner bases; in particular any algorithm to compute the number ofsolutions of a zero-dimensional polynomial system may be used.',\n",
              " 'Bistability and multistationarity are properties of reaction networks linkedto switch-like responses and connected to cell memory and cell decision making.Determining whether and when a network exhibits bistability is a hard and openmathematical problem. One successful strategy consists of analyzing smallnetworks and deducing that some of the properties are preserved upon passage tothe full network. Motivated by this we study chemical reaction networks withfew chemical complexes. Under mass-action kinetics the steady states of thesenetworks are described by fewnomial systems, that is polynomial systems havingfew distinct monomials. Such systems of polynomials are often studied in realalgebraic geometry by the use of Gale dual systems. Using this Gale duality wegive precise conditions in terms of the reaction rate constants for the numberand stability of the steady states of families of reaction networks with onenon-flow reaction.',\n",
              " 'We study families of chemical reaction networks whose positive steady statesare toric, and therefore can be parameterized by monomials. Families areconstructed algorithmically from a core network; we show that if a familymember is multistationary, then so are all subsequent networks in the family.Further, we address the questions of model selection and experimental designfor families by investigating the algebraic dependencies of the chemicalconcentrations using matroids. Given a family with toric steady states and aconstant number of conservation relations, we construct a matroid that encodesimportant information regarding the steady state behaviour of the entirefamily. Among other things, this gives necessary conditions for thedistinguishability of families of reaction networks with respect to a data setof measured chemical concentrations. We illustrate our results using multi-sitephosphorylation networks.',\n",
              " \"Alt's problem, formulated in 1923, is to count the number of four-barlinkages whose coupler curve interpolates nine general points in the plane.This problem can be phrased as counting the number of solutions to a system ofpolynomial equations which was first solved numerically using homotopycontinuation by Wampler, Morgan, and Sommese in 1992. Since there is still nota proof that all solutions were obtained, we consider upper bounds for Alt'sproblem by counting the number of solutions outside of the base locus to asystem arising as the general linear combination of polynomials. In particular,we derive effective symbolic and numeric methods for studying such systemsusing probabilistic saturations that can be employed using both finite fieldsand floating-point computations. We give bounds on the size of finite fieldrequired to achieve a desired level of certainty. These methods can also beapplied to many other problems where similar systems arise such as computingthe volumes of Newton-Okounkov bodies and computing intersection theoreticinvariants including Euler characteristics, Chern classes, and Segre classes.\",\n",
              " 'Suppose that $X_A\\\\subset \\\\mathbb{P}^{n-1}$ is a toric variety of codimensiontwo defined by an $(n-2)\\\\times n$ integer matrix $A$, and let $B$ be a Galedual of $A$. In this paper we compute the Euclidean distance degree and polardegrees of $X_A$ (along with other associated invariants) combinatoriallyworking from the matrix $B$. Our approach allows for the consideration ofexamples that would be impractical using algebraic or geometric methods. Italso yields considerably simpler computational formulas for these invariants,allowing much larger examples to be computed much more quickly than theanalogous combinatorial methods using the matrix $A$ in the codimension twocase.',\n",
              " 'We describe a new algorithm for computing Whitney stratifications of complexprojective varieties. The main ingredients are (a) an algebraic criterion, dueto L\\\\^e and Teissier, which reformulates Whitney regularity in terms ofconormal spaces and maps, and (b) a new interpretation of this conormalcriterion via primary decomposition, which can be practically implemented on acomputer. We show that this algorithm improves upon the existing state of theart by several orders of magnitude, even for relatively small input varieties.En route, we introduce related algorithms for efficiently stratifying affinevarieties, flags on a given variety, and algebraic maps.',\n",
              " 'The connection between Feynman integrals and GKZ $A$-hypergeometric systemshas been a topic of recent interest with advances in mathematical techniquesand computational tools opening new possibilities; in this paper we continue toexplore this connection. To each such hypergeometric system there is anassociated toric ideal, we prove that the latter has the Cohen-Macaulayproperty for two large families of Feynman integrals. This implies, forexample, that both the number of independent solutions and dynamicalsingularities are independent of space-time dimension and generalizedpropagator powers. Furthermore, in particular, it means that the process offinding a series representation of these integrals is fully algorithmic.',\n",
              " 'We present a probabilistic algorithm to test if a homogeneous polynomialideal $I$ defining a scheme $X$ in $\\\\mathbb{P}^n$ is radical using Segreclasses and other geometric notions from intersection theory. Its worst casecomplexity depends on the geometry of $X$. If the scheme $X$ has reducedisolated primary components and no embedded components supported the singularlocus of $X_{\\\\rm red}=V(\\\\sqrt{I})$, then the worst case complexity is doublyexponential in $n$; in all the other cases the complexity is singlyexponential. The realm of the ideals for which our radical testing procedurerequires only single exponential time includes examples which are oftenconsidered pathological, such as the ones drawn from the famous Mayr-Meyer setof ideals which exhibit doubly exponential complexity for the ideal membershipproblem.',\n",
              " 'Let $X_{\\\\Sigma}$ be a smooth complete toric variety defined by a fan $\\\\Sigma$and let $V=V(I)$ be a subscheme of $X_{\\\\Sigma}$ defined by an ideal $I$homogeneous with respect to the grading on the total coordinate ring of$X_{\\\\Sigma}$. We show a new expression for the Segre class $s(V,X_{\\\\Sigma})$ interms of the projective degrees of a rational map specified by the generatorsof $I$ when each generator corresponds to a numerically effective (nef)divisor. Restricting to the case where $X_{\\\\Sigma}$ is a smooth projectivetoric variety and dehomogenizing the total homogeneous coordinate ring of$X_{\\\\Sigma}$ via a dehomogenizing ideal we also give an expression for theprojective degrees of this rational map in terms of the dimension of anexplicit quotient ring. Under an additional technical assumption we constructwhat we call a general dehomogenizing ideal and apply this construction to giveeffective algorithms to compute the Segre class $s(V,X_{\\\\Sigma})$, theChern-Schwartz-MacPherson class $c_{SM}(V)$ and the topological Eulercharacteristic $\\\\chi(V)$ of $V$. These algorithms can, in particular, be usedfor subschemes of any product of projective spaces $\\\\mathbb{P}^{n_1} \\\\times\\\\cdots \\\\times \\\\mathbb{P}^{n_j}$ or for subschemes of many other projectivetoric varieties. Running time bounds for several of the algorithms are givenand the algorithms are tested on a variety of examples. In all applicable casesour algorithms to compute these characteristic classes are found to offersignificantly increased performance over other known algorithms.',\n",
              " 'In this paper we investigate the complexity of model selection and modeltesting for dynamical systems with toric steady states. Such systems frequentlyarise in the study of chemical reaction networks. We do this by formulatingthese tasks as a constrained optimization problem in Euclidean space. Thisoptimization problem is known as a Euclidean distance problem; the complexityof solving this problem is measured by an invariant called the Euclideandistance (ED) degree. We determine closed-form expressions for the ED degree ofthe steady states of several families of chemical reaction networks with toricsteady states and arbitrarily many reactions. To illustrate the utility of thiswork we show how the ED degree can be used as a tool for estimating thecomputational cost of solving the model testing and model selection problems.',\n",
              " 'We describe a framework for estimating Hilbert-Samuel multiplicities $e_XY$for pairs of projective varieties $X \\\\subset Y$ from finite point samplesrather than defining equations. The first step involves proving that thismultiplicity remains invariant under certain hyperplane sections which reduce$X$ to a point $p$ and $Y$ to a curve $C$. Next, we establish that $e_pC$equals the Euler characteristic (and hence, the cardinality) of the complexlink of $p$ in $C$. Finally, we provide explicit bounds on the number ofuniform point samples needed (in an annular neighborhood of $p$ in $C$) todetermine this Euler characteristic with high confidence.',\n",
              " \"We provide evidence through two loops, that rational letters ofpolylogarithmic Feynman integrals are captured by the Landau equations, whenthe latter are recast as a polynomial of the kinematic variables of theintegral, known as the principal $A$-determinant. Focusing on one loop, wefurther show that all square-root letters may also be obtained, byre-factorizing the principal $A$-determinant with the help of Jacobiidentities. We verify our findings by explicitly constructing canonicaldifferential equations for the one-loop integrals in both odd and evendimensions of loop momenta, also finding agreement with earlier results in theliterature for the latter case. We provide a computer implementation of ourresults for the principal $A$-determinant, symbol alphabets and canonicaldifferential equations in an accompanying Mathematica file. Finally, we studythe question of when a one-loop integral satisfies the Cohen-Macaulay propertyand show that for almost all choices of kinematics the Cohen-Macaulay propertyholds. Throughout, in our approach to Feynman integrals, we make extensive useof the Gel'fand, Graev, Kapranov and Zelevinski\\\\u{\\\\i} on what are now commonlycalled GKZ-hypergeometric systems whose singularities are described by theprincipal $A$-determinant.\",\n",
              " 'We give an explicit computation of the Banach envelope for the Paley-Wienertype spaces $E^p, 0<p<1$.  This answers a question by Joel Shapiro.',\n",
              " 'Adaptive optics workbenches are fully functional optical systems that can beused to illustrate and teach a variety of concepts and cognitive processes.Four systems have been funded, designed and constructed by various institutionsand people as part of education programs associated with the Center forAdaptive Optics, the Professional Development Program and the Institute forScience and Engineer Educators. Activities can range from first-yearundergraduate explorations to professional level training. These workbencheshave been used in many venues including the Center for Adaptive Optics AOSummer School, the Maui Community College hosted Akamai Maui Short Course,classrooms, training of new staff in laboratories and other venues. Theactivity content has focused on various elements of systems thinking,characterization, feedback and system control, basic optics and opticalalignment as well as advanced topics such as phase conjugation, wave-frontsensing and correction concepts and system design. The workbenches haveslightly different designs and performance capabilities. We describe hereoutlines for several activities utilizing these different designs and someexamples of common student learner outcomes and experiences.',\n",
              " \"In this paper we present results from our exploratory mid-IR study ofCentaurus A circumnuclear environment using high-angular resolution imaging atthe Magellan 6.5m telescope with the MIRAC/BLINC camera. We detected emissionfrom a compact region surrounding the nuclear source, and obtained photometryat 8.8 microns and in the N band. Our analysis suggests that the nuclear regionis resolved with a size of approximately 3 pc. The mid-IR emission from thisregion is likely associated with cool dust with an estimated temperature of\\\\~160 K, surrounding the central ``hidden'' AGN. We discuss the characteristicsof this emission in relation to other mid-IR observations and the implicationson models of dust formation in AGNs.\",\n",
              " 'Although T Tauri is one of the most studied young objects in astronomy, thenature of its circumstellar environment remains elusive due, in part, to thesmall angular separation of its three components (North-South and South a-b areseparated by 0.68\" and 0.12\" respectively). Taking advantage of incrediblystable, high Strehl, PSFs obtained with Mid-IR adaptive optics at the 6.5 meterMMT, we are able to resolve the system on and off the 10um silicate dustfeature (8.7um, 10.55um, and 11.86um; 10% bandwidth), and broad N. At thesewavelengths, South a-b are separated by only ~0.3 lambda/D. This paperdescribes a robust Markov Chain Monte Carlo technique to separate all threecomponents astrometrically and photometrically, for the first time, in themid-IR. Our results show that the silicate feature previously observed in theunresolved T Tau South binary is dominated by T Tau Sa\\'s absorption, while Sbdoes not appear to have a significant feature. This suggests that a largecircumbinary disk around Sa-Sb is not likely the primary source of cool dust inour line-of-sight, and that T Tau Sa is enshrouded by a nearly edge-oncircumstellar disk. Surprisingly, T Tau Sb does not appear to have a similarlyoriented disk.',\n",
              " 'Adaptive optics will almost completely remove the effects of atmosphericturbulence at 10 microns on the Extremely Large Telescope (ELT) generation oftelescopes. In this paper, we observationally confirm that the next mostimportant limitation to image quality is atmospheric dispersion, rather thantelescope diffraction. By using the 6.5 meter MMT with its unique mid-IRadaptive optics system, we measure atmospheric dispersion in the N-band withthe newly commissioned spectroscopic mode on MIRAC4-BLINC. Our results indicatethat atmospheric dispersion is generally linear in the N-band, although thereis some residual curvature. We compare our measurements to theory, and makepredictions for ELT Strehls and image FHWM with and without an atmosphericdispersion corrector (ADC). We find that for many mid-IR applications, an ADCwill be necessary on ELTs.',\n",
              " 'We study the number of real critical points of a cyclotomic polynomial$\\\\Phi_{n}(x)$, that is, the real roots of $\\\\Phi_{n}^{\\\\prime}(x)$. As usual, onecan, without losing generality, restrict $n$ to be the product of distinct oddprimes, say $p_{1}<\\\\cdots<p_{k}$. We show that if the primes are \"sufficientlyseparated\" then there are exactly $2^{k}-1$ real roots of$\\\\Phi_{n}^{\\\\prime}(x)$ and each of them is simple.',\n",
              " \"In 1990 Lazard proposed an improved projection operation for cylindricalalgebraic decomposition (CAD). For the proof he introduced a certain notion ofvaluation of a multivariate Puiseux series at a point. However a gap in one ofthe key supporting results for the improved projection was subsequentlynoticed. In this report we study a more limited but rigorous concept ofLazard's valuation: namely, we study Lazard's valuation of a multivariatepolynomial at a point. We prove some basic properties of the limited Lazardvaluation and identify some relationships between valuation-invariance andorder-invariance.\",\n",
              " 'The resultant plays a crucial role in (computational) algebra and algebraicgeometry. One of the most important and well known properties of the resultantis that it is equal to the determinant of the Sylvester matrix. In 2008,Odagiri proved that a similar property holds over the tropical semiring if onereplaces subtraction with addition. The tropical semiring belongs to a largefamily of algebraic structures called commutative idempotent semiring. In thispaper, we prove that the same property (with subtraction replaced withaddition) holds over an \\\\emph{arbitrary\\\\/} commutative idempotent semiring.',\n",
              " 'It is well known that for two univariate polynomials over complex numberfield the number of their common roots is equal to the order of theirresultant. In this paper, we show that this fundamental relationship stillholds for the tropical polynomials under suitable adaptation of the notion oforder, if the roots are simple and non-zero.',\n",
              " 'We provide an algorithm for computing semi-Fourier sequences for expressionsconstructed from arithmetic operations, exponentiations and integrations. Thesemi-Fourier sequence is a relaxed version of Fourier sequence for polynomials(expressions made of additions and multiplications).',\n",
              " 'In this paper, we list several interesting structures of cyclotomicpolynomials: specifically relations among blocks obtained by suitable partitionof cyclotomic polynomials.  We present explicit and self-contained proof for all of them, using a uniformterminology and technique.',\n",
              " 'A semi-algebraic set is a subset of the real space defined by polynomialequations and inequalities having real coefficients and is a union of finitelymany maximally connected components. We consider the problem of decidingwhether two given points in a semi-algebraic set are connected; that is,whether the two points lie in the same connected component. In particular, weconsider the semi-algebraic set defined by f <> 0 where f is a given polynomialwith integer coefficients. The motivation comes from the observation that manyimportant or non-trivial problems in science and engineering can be oftenreduced to that of connectivity. Due to its importance, there has been intenseresearch effort on the problem. We will describe a symbolic-numeric methodbased on gradient ascent. The method will be described in two papers. The firstpaper (the present one) will describe the symbolic part and the forthcomingsecond paper will describe the numeric part. In the present paper, we giveproofs of correctness and termination for the symbolic part and illustrate theefficacy of the method using several non-trivial examples.',\n",
              " \"Conversational Intelligence requires that a person engage on informational,personal and relational levels. Advances in Natural Language Understanding havehelped recent chatbots succeed at dialog on the informational level. However,current techniques still lag for conversing with humans on a personal level andfully relating to them. The University of Michigan's submission to the AlexaPrize Grand Challenge 3, Audrey, is an open-domain conversational chat-bot thataims to engage customers on these levels through interest driven conversationsguided by customers' personalities and emotions. Audrey is built fromsocially-aware models such as Emotion Detection and a Personal UnderstandingModule to grasp a deeper understanding of users' interests and desires. Ourarchitecture interacts with customers using a hybrid approach balanced betweenknowledge-driven response generators and context-driven neural responsegenerators to cater to all three levels of conversations. During thesemi-finals period, we achieved an average cumulative rating of 3.25 on a 1-5Likert scale.\",\n",
              " \"In 1853 Sylvester stated and proved an elegant formula that expresses thepolynomial subresultants in terms of the roots of the input polynomials.Sylvester's formula was also recently proved by Lascoux and Pragacz by usingmulti-Schur functions and divided differences. In this paper, we provide anelementary proof that uses only basic properties of matrix multiplication andVandermonde determinants.\",\n",
              " 'We consider the problem of counting (stable) equilibriums of an importantfamily of algebraic differential equations modeling multistable biologicalregulatory systems. The problem can be solved, in principle, using realquantifier elimination algorithms, in particular real root classificationalgorithms. However, it is well known that they can handle only very smallcases due to the enormous computing time requirements. In this paper, wepresent a special algorithm which is much more efficient than the generalmethods. Its efficiency comes from the exploitation of certain interestingstructures of the family of differential equations.',\n",
              " 'Consider the problem: given a real number $x$ and an error bound $\\\\epsilon$,find an interval such that it contains the $\\\\sqrt[n]{x}$ and its width is lessthan $\\\\epsilon$. One way to solve the problem is to start with an initialinterval and to repeatedly update it by applying an interval refinement map onit until it becomes narrow enough. In this paper, we prove that the well knownSecant-Newton map is optimal among a certain family of natural generalizations.',\n",
              " 'The maximum gap $g(f)$ of a polynomial $f$ is the maximum of the differences(gaps) between two consecutive exponents that appear in $f$. Let $\\\\Phi_{n}$ and$\\\\Psi_{n}$ denote the $n$-th cyclotomic and $n$-th inverse cyclotomicpolynomial, respectively. In this paper, we give several lower bounds for$g(\\\\Phi_{n})$ and $g(\\\\Psi_{n})$, where $n$ is the product of odd primes. Weobserve that they are very often exact. We also give an exact expression for$g(\\\\Psi_{n})$ under a certain condition. Finally we conjecture an exactexpression for $g(\\\\Phi_{n})$ under a certain condition.',\n",
              " 'In this paper, we give an explicit expression for a certain family of ternarycyclotomic polynomials: specifically $\\\\Phi_{p_{1}p_{2}p_{3}}$, where$p_{1}<p_{2}<p_{3}$ are odd primes such that $p_{2} \\\\equiv1 \\\\mod p_{1}$ and$p_{3} \\\\equiv1 \\\\mod {p_{1}p_{2}}$. As an application of the explicitexpressions, we give an exact formula for the number of nonzero terms in thepolynomials in the family, which in turn immediately shows that the density(number of non-zeros terms / degree) is roughly inversely proportional to$p_{2}$, when $p_{1}$ is sufficiently large.',\n",
              " 'Cyclotomic polynomials play fundamental roles in number theory,combinatorics, algebra and their applications. Hence their properties have beenextensively investigated. In this paper, we study the maximum gap $g$ (maximumof the differences between any two consecutive exponents). In 2012, it wasshown that $g\\\\left( \\\\Phi_{p_{1}p_{2}}\\\\right) =p_{1} -1$ for primes$p_{2}>p_{1}$. In 2017, based on numerous calculations, the followinggeneralization was conjectured: $g\\\\left( \\\\Phi_{mp}\\\\right) =\\\\varphi(m)$ forsquare free odd $m$ and prime $p>m$. The main contribution of this paper is aproof of this conjecture.',\n",
              " \"We consider the problem of finding a condition for a univariate polynomialhaving a given multiplicity structure when the number of distinct roots isgiven. It is well known that such conditions can be written as conjunctions ofseveral polynomial equations and one inequation in the coefficients, by usingrepeated parametric gcd's. In this paper, we give a novel condition which isnot based on repeated gcd's. Furthermore, it is shown that the number ofpolynomials in the condition is optimal and the degree of polynomials issmaller than that in the previous condition based on repeated gcd's.\",\n",
              " 'We present our Julia software package ParameterEstimation.jl, which estimatesparameter values in parametric ODE models, using measured data. Our approach,unlike most other approaches, is not based on optimization. Instead, it isbased on differential algebra, interpolation of the data using rationalfunctions, and multivariable polynomial system solving. We compare the accuracyand time/memory performance of our software with other estimation softwarepackages, SciMLSensitivity, AMIGO2, and IQM.',\n",
              " 'We propose Iterative Homography Network, namely IHN, a new deep homographyestimation architecture. Different from previous works that achieve iterativerefinement by network cascading or untrainable IC-LK iterator, the iterator ofIHN has tied weights and is completely trainable. IHN achieves state-of-the-artaccuracy on several datasets including challenging scenes. We propose 2versions of IHN: (1) IHN for static scenes, (2) IHN-mov for dynamic scenes withmoving objects. Both versions can be arranged in 1-scale for efficiency or2-scale for accuracy. We show that the basic 1-scale IHN already outperformsmost of the existing methods. On a variety of datasets, the 2-scale IHNoutperforms all competitors by a large gap. We introduce IHN-mov by producingan inlier mask to further improve the estimation accuracy of moving-objectsscenes. We experimentally show that the iterative framework of IHN can achieve95% error reduction while considerably saving network parameters. Whenprocessing sequential image pairs, IHN can achieve 32.7 fps, which is about 8xthe speed of IC-LK iterator. Source code is available athttps://github.com/imdumpl78/IHN.',\n",
              " \"Autonomous swarms of robots can bring robustness, scalability andadaptability to safety-critical tasks such as search and rescue but theirapplication is still very limited. Using semi-autonomous swarms with humancontrol can bring robot swarms to real-world applications. Human operators candefine goals for the swarm, monitor their performance and interfere with, oroverrule, the decisions and behaviour. We present the ``Human And RobotInteractive Swarm'' simulator (HARIS) that allows multi-user interaction with arobot swarm and facilitates qualitative and quantitative user studies throughsimulation of robot swarms completing tasks, from package delivery to searchand rescue, with varying levels of human control. In this demonstration, weshowcase the simulator by using it to study the performance gain offered bymaintaining a ``human-in-the-loop'' over a fully autonomous system as anexample. This is illustrated in the context of search and rescue, with anautonomous allocation of resources to those in need.\",\n",
              " 'The kappa_SQ software package is designed to assist researchers working onrandomized row sampling. The package contains a collection of Matlab functionsalong with a GUI that ties them all together and provides a platform for theuser to perform experiments. In particular, kappa_SQ is designed to doexperiments related to the two-norm condition number of a sampled matrix,$\\\\kappa(SQ)$, where $S$ is a row sampling matrix and $Q$ is a tall and skinnymatrix with orthonormal columns. Via a simple GUI, kappa_SQ can generate testmatrices, perform various types of row sampling, measure $\\\\kappa(SQ)$,calculate bounds and produce high quality plots of the results. All of theimportant codes are written in separate Matlab function files in a standardformat which makes it easy for a user to either use the codes by themselves orincorporate their own codes into the kappa_SQ package.',\n",
              " 'A fundamental task in numerical computation is the solution of large linearsystems. The conjugate gradient method is an iterative method which offersrapid convergence to the solution, particularly when an effectivepreconditioner is employed. However, for more challenging systems a substantialerror can be present even after many iterations have been performed. Theestimates obtained in this case are of little value unless further informationcan be provided about the numerical error. In this paper we propose a novelstatistical model for this numerical error set in a Bayesian framework. Ourapproach is a strict generalisation of the conjugate gradient method, which isrecovered as the posterior mean for a particular choice of prior. The estimatesobtained are analysed with Krylov subspace methods and a contraction result forthe posterior is presented. The method is then analysed in a simulation studyas well as being applied to a challenging problem in medical imaging.',\n",
              " 'This paper is concerned with approximating the dominant left singular vectorspace of a real matrix $A$ of arbitrary dimension, from block Krylov spacesgenerated by the matrix $AA^T$ and the block vector $AX$. Two classes ofresults are presented. First are bounds on the distance, in the two andFrobenius norms, between the Krylov space and the target space. The distance isexpressed in terms of principal angles. Second are quality of approximationbounds, relative to the best approximation in the Frobenius norm. For startingguesses $X$ of full column-rank, the bounds depend on the tangent of theprincipal angles between $X$ and the dominant right singular vector space of$A$. The results presented here form the structural foundation for the analysisof randomized Krylov space methods. The innovative feature is a combination oftraditional Lanczos convergence analysis with optimal approximations via leastsquares problems.',\n",
              " 'A common challenge faced in quantum physics is finding the extremaleigenvalues and eigenvectors of a Hamiltonian matrix in a vector space so largethat linear algebra operations on general vectors are not possible. There arenumerous efficient methods developed for this task, but they generally failwhen some control parameter in the Hamiltonian matrix exceeds some thresholdvalue. In this work we present a new technique called eigenvector continuationthat can extend the reach of these methods. The key insight is that while aneigenvector resides in a linear space with enormous dimensions, the eigenvectortrajectory generated by smooth changes of the Hamiltonian matrix is wellapproximated by a very low-dimensional manifold. We prove this statement usinganalytic function theory and propose an algorithm to solve for the extremaleigenvectors. We benchmark the method using several examples from quantummany-body theory.',\n",
              " 'The Monge-Kantorovich mass transfer problem is equivalently formulated as aconvex optimization problem for a potential function. In the light of thisformulation an interative algorithm is developed for determining the solution.It is a gradient flow algorithm and each iterate solves a linear ellipticequation. Well-posedness and convergence of the proposed method are analyzedand numerical findings are presented.',\n",
              " 'We develop an efficient algorithm for Synthetic Aperture Sonar imaging basedon the one-way wave equations. The algorithm utilizes the operator-splittingmethod to integrate the one-way wave equations. The well-posedness of theone-way wave equations and the proposed algorithm is shown. A computationalresult against real field data is reported and the resulting image is enhancedby the BV-like regularization.',\n",
              " 'This paper develops and analyzes a generic method for reconstructingsolutions to the abstract Cauchy problem in a general Hilbert space, from noisymeasured data. The method is based on the relationship between a partialdifferential equation and its adjoint equation with control. We demonsrate thecapability of the method through analysis and numerical experiments.',\n",
              " 'We propose an implicit iterative algorithm for an exact penalty methodarising from inequality constrained optimization problems. A rapidly convergentfixed point method is developed for a regularized penalty functional. Theapplicability and feasibility of the proposed method is demonstrated usinglarge scale inequality constrained problems.',\n",
              " 'In this paper, we propose a direct probing method for the inverse problembased on the Eikonal equation. For the Eikonal equation with a point source,the viscosity solution represents the least travel time of wave fields from thesource to the point at the high-frequency limit. The corresponding inverseproblem is to determine the inhomogeneous wave-speed distribution from thefirst-arrival time data at the measurement surfaces corresponding todistributed point sources, which is called transmission travel-time tomography.At the low-frequency regime, the reconstruction approximates thefrequency-depend wave-speed distribution. We analyze the Eikonal inverseproblem and show that it is highly ill-posed. Then we developed a directprobing method that incorporates the solution analysis of the Eikonal equationand several aspects of the velocity models. When the wave-speed distributionhas a small variation from the homogeneous medium, we reconstruct theinhomogeneous media using the filtered back projection method. For thehigh-contrast media, we assume a background medium and develop theadjoint-based back projection method to identify the variations of the mediumfrom the assumed background.',\n",
              " \"We propose a multi-moment method for one-dimensional hyperbolic equationswith smooth coefficient and piecewise constant coefficient. The method isentirely based on the backward characteristic method and uses the solution andits derivative as unknowns and cubic Hermite interpolation for eachcomputational cell. The exact update formula for solution and its derivative isderived and used for an efficient time integration. At points of discontinuityof wave speed we define a piecewise cubic Hermite interpolation based onimmersed interface method. The method is extended to the one-dimensionalMaxwell's equations with variable material properties.\",\n",
              " 'We present a novel approach to nonlinear constrained Tikhonov regularizationfrom the viewpoint of optimization theory. A second-order sufficient optimalitycondition is suggested as a nonlinearity condition to handle the nonlinearityof the forward operator. The approach is exploited to derive convergence ratesresults for a priori as well as a posteriori choice rules, e.g., discrepancyprinciple and balancing principle, for selecting the regularization parameter.The idea is further illustrated on a general class of parameter identificationproblems, for which (new) source and nonlinearity conditions are derived andthe structural property of the nonlinearity term is revealed. A number ofexamples including identifying distributed parameters in elliptic differentialequations are presented.',\n",
              " \"We develop a numerical scheme for solving time-domain Maxwell's equation. Themethod is motivated by CIP method which uses function values and itsderivatives as unknown variables. The proposed scheme is developed by using thePoisson formula for the wave equation. It is fully explicit space and timeintegration method with higher order accuracy and CFL number being one. Thebi-cubic interpolation is used for the solution profile to attain theresolution. It preserves sharp profiles very accurately without any smearingand distortion due to the exact time integration and high resolutionapproximation. The stability and numerical accuracy are investigated.\",\n",
              " 'We study multi-parameter regularization (multiple penalties) for solvinglinear inverse problems to promote simultaneously distinct features of thesought-for objects. We revisit a balancing principle for choosingregularization parameters from the viewpoint of augmented Tikhonovregularization, and derive a new parameter choice strategy called the\\\\textit{balanced discrepancy principle}. A priori and a posteriori errorestimates are provided to theoretically justify the principles, and numericalalgorithms for efficiently implementing the principles are also provided.Numerical results on denoising are presented to illustrate the feasibility ofthe balanced discrepancy principle.',\n",
              " 'In this work, we are concerned with the diffusive optical tomography (DOT)problem in the case when only one or two pairs of Cauchy data is available. Wepropose a simple and efficient direct sampling method (DSM) to locateinhomogeneities inside a homogeneous background and solve the DOT problem inboth full and limited aperture cases. This new method is easy to implement andless expensive computationally. Numerical experiments demonstrate itseffectiveness and robustness against noise in the data. This provides a newpromising numerical strategy for the DOT problem.',\n",
              " 'Optimal control problems involving hybrid binary-continuous control costs arechallenging due to their lack of convexity and weak lower semicontinuity.Replacing such costs with their convex relaxation leads to a primal-dualoptimality system that allows an explicit pointwise characterization and whoseMoreau-Yosida regularization is amenable to a semismooth Newton method infunction space. This approach is especially suited for computing switchingcontrols for partial differential equations. In this case, the optimality gapbetween the original functional and its relaxation can be estimated and shownto be zero for controls with switching structure. Numerical examples illustratethe effectiveness of this approach.',\n",
              " 'We propose some numerical schemes for forward-backward stochasticdifferential equations (FBSDEs) based on a new fundamental concept oftransposition solutions. These schemes exploit time-splitting methods for thevariation of constants formula of the associated partial differential equationsand a discrete representation of the transition semigroups. The convergence ofthe schemes is established for FBSDEs with uniformly Lipschitz drivers, locallyLipschitz and maximal monotone drivers. Numerical experiments are presented forseveral nonlinear financial derivative pricing problems to demonstrate theadaptivity and effectiveness of the new schemes. The ideas here can be appliedto construct high-order schemes for FBSDEs with general Markov forwardprocesses.',\n",
              " 'In this work, we develop efficient solvers for linear inverse problems basedon randomized singular value decomposition (RSVD). This is achieved bycombining RSVD with classical regularization methods, e.g., truncated singularvalue decomposition, Tikhonov regularization, and general Tikhonovregularization with a smoothness penalty. One distinct feature of the proposedapproach is that it explicitly preserves the structure of the regularizedsolution in the sense that it always lies in the range of a certain adjointoperator. We provide error estimates between the approximation and the exactsolution under canonical source condition, and interpret the approach in thelens of convex duality. Extensive numerical experiments are provided toillustrate the efficiency and accuracy of the approach.',\n",
              " \"This paper formulates a generalized classification algorithm with anapplication to classifying (or `decoding') neural activity in the brain.Medical doctors and researchers have long been interested in how brain activitycorrelates to body movement. Experiments have been conducted on patients whomare unable to move, in order to gain insight as to how thinking about movementsmight generate discernable neural activity. Researchers are tasked withdetermining which neurons are responsible for different imagined movements andhow the firing behavior changes, given neural firing data. For instance,imagined movements may include wrist flexion, elbow extension, or closing thehand. This is just one of many applications to data classification. Though thisarticle deals with an application in neuroscience, the generalized algorithmproposed in this article has applications in scientific areas ranging fromneuroscience to acoustic and medical imaging.\",\n",
              " 'We study multi-parameter Tikhonov regularization, i.e., with multiplepenalties. Such models are useful when the sought-for solution exhibits severaldistinct features simultaneously. Two choice rules, i.e., discrepancy principleand balancing principle, are studied for choosing an appropriate(vector-valued) regularization parameter, and some theoretical results arepresented. In particular, the consistency of the discrepancy principle as wellas convergence rate are established, and an a posteriori error estimate for thebalancing principle is established. Also two fixed point algorithms areproposed for computing the regularization parameter by the latter rule.Numerical results for several nonsmooth multi-parameter models are presented,which show clearly their superior performance over their single-parametercounterparts.',\n",
              " 'We present a novel numerical method to the time-harmonic inverse mediumscattering problem of recovering the refractive index from near-field scattereddata. The approach consists of two stages, one pruning step of detecting thescatterer support, and one resolution enhancing step with mixed regularization.The first step is strictly direct and of sampling type, and faithfully detectsthe scatterer support. The second step is an innovative application ofnonsmooth mixed regularization, and it accurately resolves the scatterer sizesas well as intensities. The model is efficiently solved by a semi-smoothNewton-type method. Numerical results for two- and three-dimensional examplesindicate that the approach is accurate, computationally efficient, and robustwith respect to data noise.',\n",
              " 'In this paper, we study the inverse electromagnetic medium scattering problemof estimating the support and shape of medium scatterers from scatteredelectric or magnetic near-field data. We shall develop a novel direct samplingmethod based on an analysis of electromagnetic scattering and the behavior ofthe fundamental solution. The method is applicable even with one incident fieldand needs only to compute inner products of the measured scattered field withthe fundamental solutions located at sampling points. Hence it is strictlydirect, computationally very efficient, and highly tolerant to the presence ofnoise in the data. Two- and three-dimensional numerical experiments indicatethat it can provide reliable support estimates of one single and multiplescatterers in case of both exact and highly noisy data.',\n",
              " 'We propose an inexact Uzawa algorithm with two variable relaxation parametersfor solving the generalized saddle-point system. The saddle-point problems canbe found in a wide class of applications, such as the augmented Lagrangianformulation of the constrained minimization, the mixed finite element method,the mortar domain decomposition method and the discretization of elliptic andparabolic interface problems. The two variable parameters can be updated ateach iteration, requiring no a priori estimates on the spectrum of twopreconditioned subsystems involved. The convergence and convergence rate of thealgorithm are analysed. Both symmetric and nonsymmetric saddle-point systemsare discussed, and numerical experiments are presented to demonstrate therobustness and effectiveness of the algorithm.',\n",
              " 'In this work we perform some mathematical analysis on non-negative matrixfactorizations (NMF) and apply NMF to some imaging and inverse problems. Wewill propose a sparse low-rank approximation of big positive data and images interms of tensor products of positive vectors, and investigate its effectivenessin terms of the number of tensor products to be used in the approximation. Anew concept of multi-level analysis (MLA) framework is also suggested toextract major components in the matrix representing structures of differentresolutions, but still preserving the positivity of the basis and sparsity ofthe approximation. We will also propose a semi-smooth Newton method based onprimal-dual active sets for the non-negative factorization. Numerical resultsare given to demonstrate the effectiveness of the proposed method to capturefeatures in images and structures of inverse problems under no a-prioriassumption on the data structure, as well as to provide a sparse low-rankrepresentation of the data.',\n",
              " 'The Poisson model is frequently employed to describe count data, but in aBayesian context it leads to an analytically intractable posterior probabilitydistribution. In this work, we analyze a variational Gaussian approximation tothe posterior distribution arising from the Poisson model with a Gaussianprior. This is achieved by seeking an optimal Gaussian distribution minimizingthe Kullback-Leibler divergence from the posterior distribution to theapproximation, or equivalently maximizing the lower bound for the modelevidence. We derive an explicit expression for the lower bound, and show theexistence and uniqueness of the optimal Gaussian approximation. The lower boundfunctional can be viewed as a variant of classical Tikhonov regularization thatpenalizes also the covariance. Then we develop an efficient alternatingdirection maximization algorithm for solving the optimization problem, andanalyze its convergence. We discuss strategies for reducing the computationalcomplexity via low rank structure of the forward operator and the sparsity ofthe covariance. Further, as an application of the lower bound, we discusshierarchical Bayesian modeling for selecting the hyperparameter in the priordistribution, and propose a monotonically convergent algorithm for determiningthe hyperparameter. We present extensive numerical experiments to illustratethe Gaussian approximation and the algorithms.',\n",
              " 'We present a two-stage least-squares method to inverse medium problems ofreconstructing multiple unknown coefficients simultaneously from noisy data. Adirect sampling method is applied to detect the location of the inhomogeneityin the first stage, while a total least-squares method with mixedregularization is used to recover the medium profile in the second stage. Thetotal least-squares method is designed to minimize the residual of the modelequation and the data fitting, along with an appropriate regularization, in anattempt to significantly improve the accuracy of the approximation obtainedfrom the first stage. We shall also present an analysis on the well-posednessand convergence of this algorithm. Numerical experiments are carried out toverify the accuracies and robustness of this novel two-stage least-squaresalgorithm, with great tolerance of noise.',\n",
              " 'In this work, we propose a class of numerical schemes for solving semilinearHamilton-Jacobi-Bellman-Isaacs (HJBI) boundary value problems which arisenaturally from exit time problems of diffusion processes with controlled drift.We exploit policy iteration to reduce the semilinear problem into a sequence oflinear Dirichlet problems, which are subsequently approximated by a multilayerfeedforward neural network ansatz. We establish that the numerical solutionsconverge globally in the $H^2$-norm, and further demonstrate that thisconvergence is superlinear, by interpreting the algorithm as an inexact Newtoniteration for the HJBI equation. Moreover, we construct the optimal feedbackcontrols from the numerical value functions and deduce convergence. Thenumerical schemes and convergence results are then extended to HJBI boundaryvalue problems corresponding to controlled diffusion processes with obliqueboundary reflection. Numerical experiments on the stochastic Zermelo navigationproblem are presented to illustrate the theoretical results and to demonstratethe effectiveness of the method.',\n",
              " \"We conducted a search for very short-period transiting objects in thepublicly available Kepler dataset. Our preliminary survey has revealed fourplanetary candidates, all with orbital periods less than twelve hours. We haveanalyzed the data for these candidates using photometric models that includetransit light curves, ellipsoidal variations, and secondary eclipses toconstrain the candidates' radii, masses, and effective temperatures. Even withmasses of only a few Earth masses, the candidates' short periods mean they mayinduce stellar radial velocity signals (a few m/s) detectable by currentlyoperating facilities. The origins of such short-period planets are unclear, butwe discuss the possibility that they may be the remnants of disrupted hotJupiters. Whatever their origins, if confirmed as planets, these candidateswould be among the shortest-period planets ever discovered. Such planets wouldbe particularly amenable to discovery by the planned TESS mission.\",\n",
              " \"Extra-solar planets close to their host stars have likely undergonesignificant tidal evolution since the time of their formation. Tides probablydominated their orbital evolution once the dust and gas had cleared away, andas the orbits evolved there was substantial tidal heating within the planets.The tidal heating history of each planet may have contributed significantly tothe thermal budget that governed the planet's physical properties, includingits radius, which in many cases may be measured by observing transit events.Typically, tidal heating increases as a planet moves inward toward its star andthen decreases as its orbit circularizes. Here we compute the plausible heatinghistories for several planets with measured radii, using the same tidalparameters for the star and planet that had been shown to reconcile theeccentricity distribution of close-in planets with other extra-solar planets.Several planets are discussed, including for example HD 209458 b, which mayhave undergone substantial tidal heating during the past billion years, perhapsenough to explain its large measured radius. Our models also show that GJ 876 dmay have experienced tremendous heating and is probably not a solid, rockyplanet. Theoretical models should include the role of tidal heating, which islarge, but time-varying.\",\n",
              " \"The tidal heating of hypothetical rocky (or terrestrial) extra-solar planetsspans a wide range of values depending on stellar masses and initial orbits.Tidal heating may be sufficiently large (in many cases, in excess of radiogenicheating) and long-lived to drive plate tectonics, similar to the Earth's, whichmay enhance the planet's habitability. In other cases, excessive tidal heatingmay result in Io-like planets with violent volcanism, probably rendering themunsuitable for life. On water-rich planets, tidal heating may generatesub-surface oceans analogous to Europa's with similar prospects forhabitability. Tidal heating may enhance the outgassing of volatiles,contributing to the formation and replenishment of a planet's atmosphere. Toaddress these issues, we model the tidal heating and evolution of hypotheticalextra-solar terrestrial planets. The results presented here constrain theorbital and physical properties required for planets to be habitable.\",\n",
              " \"Transiting planets are generally close enough to their host stars that tidesmay govern their orbital and thermal evolution of these planets. We presentcalculations of the tidal evolution of recently discovered transiting planetsand discuss their implications. The tidal heating that accompanies this orbitalevolution can be so great that it controls the planet's physical properties andmay explain the large radii observed in several cases, including, for example,TrES-4. Also because a planet's transit probability depends on its orbit, itevolves due to tides. Current values depend sensitively on the physicalproperties of the star and planet, as well as on the system's age. As a result,tidal effects may introduce observational biases in transit surveys, which mayalready be evident in current observations. Transiting planets tend to beyounger than non-transiting planets, an indication that tidal evolution mayhave destroyed many close-in planets. Also the distribution of the masses oftransiting planets may constrain the orbital inclinations of non-transitingplanets.\",\n",
              " \"The distribution of the orbits of close-in exoplanets shows evidence foron-going removal and destruction by tides. Tides raised on a planet's host starcause the planet's orbit to decay, even after the orbital eccentricity hasdropped to zero. Comparison of the observed orbital distribution andpredictions of tidal theory show good qualitative agreement, suggesting tidaldestruction of close-in exoplanets is common. The process can explain theobserved cut-off in small a-values, the clustering of orbital periods nearthree days, and the relative youth of transiting planets. Contrary to previousconsiderations, a mechanism to stop the inward migration of close-in planets attheir current orbits is not necessarily required. Planets nearing tidaldestruction may be found with extremely small a, possibly already stripped ofany gaseous envelope. The recently discovered CoRoT-Exo-7 b may be an exampleof such a planet and will probably be destroyed by tides within the next fewGyrs. Also, where one or more planets have already been accreted, a star mayexhibit an unusual composition and/or spin rate.\",\n",
              " \"A re-purposed Kepler mission could continue the search for nearly Earth-sizedplanets in very short-period (< 1 day) orbits. Recent surveys of the Keplerdata already available have revealed at least a dozen such planetarycandidates, and a more complete and focused survey is likely to reveal more.Given the planets' short orbital periods, building the requisitesignal-to-noise to detect the candidates by stacking multiple transits requiresa much shorter observational baseline than for longer-period planets, and thetransits are likely more robust against the much larger instrumental variationsanticipated for the modified Kepler pointing capabilities. Searching for theseunusual planets will also leverage the Kepler mission's already considerableexpertise in planetary transit detection and analysis. These candidates mayrepresent an entirely new class of planet. They may also provide unprecedentedinsights into planet formation and evolution and sensitive probes forplanet-star interactions and the stellar wind. Whatever their origins andnatures, such planets would be particularly amenable to discovery by theplanned TESS mission, and a preliminary survey by Kepler could pave the way forsuch TESS discoveries.\",\n",
              " \"Conditions on Saturn's moon Titan suggest dust devils, which are convective,dust-laden plumes, may be active. Although the exact nature of dust on Titan isunclear, previous observations confirm an active aeolian cycle, and dust devilsmay play an important role in Titan's aeolian cycle, possibly contributing toregional transport of dust and even production of sand grains. The Dragonflymission to Titan will document dust devil and convective vortex activity andthereby provide a new window into these features, and our analysis shows thatassociated winds are likely to be modest and pose no hazard to the mission.\",\n",
              " \"The influence of dust devils on the martian atmosphere depends on theircapacity to loft dust, which depends on their wind profiles and footprint onthe martian surface, i.e., on their radii, $R$. Previous work suggests the windprofile depends on a devil's thermodynamic efficiency, which scales with itsheight, $h$. However, the precise mechanisms that set a dust devil's radiushave remained unclear. Combining previous work with simple assumptions aboutangular momentum conservation in dust devils predicts that $R \\\\propto h^{1/2}$,and a model fit to observed radii and heights from a survey of martian dustdevils using the Mars Express High Resolution Stereo Camera agrees reasonablywell with this prediction. Other observational tests involving additional,statistically robust dust devil surveys and field measurements may furtherelucidate these relationships.\",\n",
              " 'Electronic structure calculations on small systems such as H$_2$, H$_2$O,LiH, and BeH$_2$ with chemical accuracy are still a challenge for the currentgeneration of the noisy intermediate-scale quantum (NISQ) devices. One of thereasons is that due to the device limitations, only minimal basis sets arecommonly applied in quantum chemical calculations, which allow one to keep thenumber of qubits employed in the calculations at minimum. However, the use ofminimal basis sets leads to very large errors in the computed molecularenergies as well as potential energy surface shapes. One way to increase theaccuracy of electronic structure calculations is through the development ofsmall basis sets better suited for quantum computing. In this work, we showthat the use of adaptive basis sets, in which exponents and contractioncoefficients depend on molecular structure, provide an easy way to dramaticallyimprove the accuracy of quantum chemical calculations without the need toincrease the basis set size and thus the number of qubits utilized in quantumcircuits. As a proof of principle, we optimize an adaptive minimal basis setfor quantum computing calculations on an H$_2$ molecule, in which exponents andcontraction coefficients depend on the H-H distance, and apply it to thegeneration of H$_2$ potential energy surface on IBM-Q quantum devices. Theadaptive minimal basis set reaches the accuracy of the double-zeta basis sets,thus allowing one to perform double-zeta quality calculations on quantumdevices without the need to utilize twice as many qubits in simulations. Thisapproach can be extended to other molecular systems and larger basis sets in astraightforward manner.',\n",
              " 'This work presents an implementation of a social architecture model forauthoring Non-Player Character (NPC) in open world games inspired in academicresearch on agentbased modeling. Believable NPC authoring is burdensome interms of rich dialogue and responsive behaviors.  We briefly present the characteristics and advantages of using a social agentarchitecture for this task and describe an implementation of a social agentarchitecture CiF-CK released as a mod Social NPCs for The Elder Scrolls V:Skyrim',\n",
              " 'Rupture is a nonlinear instability resulting in a finite-time singularity asa fluid layer approaches zero thickness at a point. We study the dynamics ofrupture in a generalized mathematical model of thin films of viscous fluidswith evaporative effects. The governing lubrication model is a fourth-ordernonlinear parabolic partial differential equation with a non-conservative lossterm due to evaporation. Several different types of finite-time singularitiesare observed due to balances between evaporation and surface tension orintermolecular forces. Non-self-similar behavior and two classes ofself-similar rupture solutions are analyzed and validated against highresolution PDE simulations.',\n",
              " 'Existence of non-negative weak solutions is shown for a full curvaturethin-film model of a liquid thin film flowing down a vertical fibre. The proofis based on the application of a priori estimates derived for energy-entropyfunctionals. Long-time behaviour of these weak solutions is analysed and, undersome additional constraints for the model parameters and initial values,convergence towards a travelling wave solution is obtained. Numerical studiesof energy minimizers and travelling waves are presented to illustrateanalytical results.',\n",
              " 'We study the effects of nozzle geometry on the dynamics of thin fluid filmsflowing down a vertical cylindrical fibre. Recent experiments show that varyingthe nozzle diameter can lead to different flow regimes and dropletcharacteristics in the film. Using a weighted residual modeling approach, wedevelop a system of coupled equations that account for inertia, surface tensioneffects, gravity, and a film stabilization mechanism to describe bothnear-nozzle fluid structures and downstream bead dynamics. We report goodagreement between the predicted droplet properties and the experimental data.',\n",
              " 'Volatile viscous fluids on partially-wetting solid substrates can exhibitinteresting interfacial instabilities and pattern formation. We study thedynamics of vapor condensation and fluid evaporation governed by a one-sidedmodel in a low Reynolds number lubrication approximation incorporating surfacetension, intermolecular effects and evaporative fluxes. Parameter ranges forevaporation- dominated and condensation-dominated regimes and a critical caseare identified. Interfacial instabilities driven by the competition between thedisjoining pressure and evaporative effects are studied via linear stabilityanalysis. Transient pattern formation in nearly-flat evolving films in thecritical case is investigated. In the weak evaporation limit unstable modes offinite amplitude non-uniform steady states lead to rich droplet dynamics,including flattening, symmetry breaking, and droplet merging. Numericalsimulations show long time behaviors leading to evaporation or condensation aresensitive to transitions between film-wise and drop-wise dynamics.',\n",
              " 'We study the steady states and dynamics of a thin film-type equation withnon-conserved mass in one dimension. The evolution equation is a nonlinearfourth-order degenerate parabolic PDE motivated by a model of volatile viscousfluid films allowing for condensation or evaporation. We show that by changingthe sign of the non-conserved flux and breaking from a gradient flow structure,the problem can exhibit novel behaviors including having two distinct classesof coexisting steady state solutions. Detailed analysis of the bifurcationstructure for these steady states and their stability reveal severalpossibilities for the dynamics. For some parameter regimes, solutions can leadto finite-time rupture singularities. Interestingly, we also show that a finiteamplitude limit cycle can occur as a singular perturbation in thenearly-conserved limit.',\n",
              " 'Motivated by a model proposed by Peng et al. [Advances in Coll. and Interf.Sci. 206 (2014)] for break-up of tear films on human eyes, we study thedynamics of a generalized thin film model. The governing equations form afourth-order coupled system of nonlinear parabolic PDE for the film thicknessand salt concentration subject to non-conservative effects representingevaporation. We analytically prove the global existence of solutions to thismodel with mobility exponents in several different ranges and the results arethen validated against PDE simulations. We also numerically capture otherinteresting dynamics of the model, including finite-time rupture-shockphenomenon due to the instabilities caused by locally elevated evaporationrates, convergence to equilibrium and infinite-time thinning.',\n",
              " 'We study a continuum model for solid films that arises from the modeling ofone-dimensional step flows on a vicinal surface in theattachment-detachment-limited regime. The resulting nonlinear partialdifferential equation, $u_t = -u^2(u^3+\\\\alpha u)_{hhhh}$, gives the evolutionfor the surface slope $u$ as a function of the local height $h$ in a monotonestep train. Subject to periodic boundary conditions and positive initialconditions, we prove the existence, uniqueness and positivity of global strongsolutions to this PDE using two Lyapunov energy functions. The long timebehavior of $u$ converging to a constant that only depends on the initial datais also investigated both analytically and numerically.',\n",
              " 'This paper analytically investigates the solutions to a control-volume modelfor liquid films flowing down a vertical fibre. The dynamic evolution of thefree surface is governed by a coupled degenerate nonlinear PDE system for thefluid film radius and the axial velocity. We prove the existence of weaksolutions to the coupled system based on the application of a priori estimatesderived for energy-entropy functionals. Existence of travelling wave solutionsto the system is also established. Numerical studies are presented toillustrate the derived analytical results for both the dynamic PDE solutionsand the travelling wave structures.',\n",
              " 'A lubrication model can be used to describe the dynamics of a weakly volatileviscous fluid layer on a hydrophobic substrate. Thin layers of the fluid areunstable to perturbations and break up into slowly evolving interactingdroplets. A reduced-order dynamical system is derived from the lubricationmodel based on the nearest-neighbor droplet interactions in the weakcondensation limit. Dynamics for periodic arrays of identical drops andpairwise droplet interactions are investigated which provide insights to thecoarsening dynamics of a large droplet system. Weak condensation is shown to bea singular perturbation, fundamentally changing the long-time coarseningdynamics for the droplets and the overall mass of the fluid in two additionalregimes of long-time dynamics.',\n",
              " 'This paper develops a method of biologically guided deep learning forpost-radiation FDG-PET image outcome prediction based on pre-radiation imagesand radiotherapy dose information. Based on the classic reaction-diffusionmechanism, a novel biological model was proposed using a partial differentialequation that incorporates spatial radiation dose distribution as apatient-specific treatment information variable. A 7-layerencoder-decoder-based convolutional neural network (CNN) was designed andtrained to learn the proposed biological model. As such, the model couldgenerate post-radiation FDG-PET image outcome predictions with possibletime-series transition from pre-radiotherapy image states to post-radiotherapystates. The proposed method was developed using 64 oropharyngeal patients withpaired FDG-PET studies before and after 20Gy delivery (2Gy/daily fraction) byIMRT. In a two-branch deep learning execution, the proposed CNN learns specificterms in the biological model from paired FDG-PET images and spatial dosedistribution as in one branch, and the biological model generates post-20GyFDG-PET image prediction in the other branch. The proposed method successfullygenerated post-20Gy FDG-PET image outcome prediction with breakdownillustrations of biological model components. Time-series FDG-PET imagepredictions were generated to demonstrate the feasibility of disease responserendering. The developed biologically guided deep learning method achievedpost-20Gy FDG-PET image outcome predictions in good agreement with ground-truthresults. With break-down biological modeling components, the outcome imagepredictions could be used in adaptive radiotherapy decision-making to optimizepersonalized plans for the best outcome in the future.',\n",
              " 'We revisit the tears of wine problem for thin films in water-ethanol mixturesand present a new model for the climbing dynamics. The new formulation includesa Marangoni stress balanced by both the normal and tangential components ofgravity as well as surface tension which lead to distinctly different behavior.The prior literature did not address the wine tears but rather the behavior ofthe film at earlier stages and the behavior of the meniscus. In the lubricationlimit we obtain an equation that is already well-known for rising films in thepresence of thermal gradients. Such models can exhibit non-classical shocksthat are undercompressive. We present basic theory that allows one to identifythe signature of an undercompressive (UC) wave. We observe both compressive andundercompressive waves in new experiments and we argue that, in the case of apre-coated glass, the famous \"wine tears\" emerge from a reverseundercompressive shock originating at the meniscus.',\n",
              " \"We develop efficient and accurate numerical methods to solve a class ofshallow shell problems of the von Karman type. The governing equations form afourth-order coupled system of nonlinear biharnomic equations for thetransverse deflection and Airy's stress function. A second-order finitedifference discretization with three iterative methods (Picard, Newton andTrust-Region Dogleg) are proposed for the numerical solution of the nonlinearPDE system. Three simple boundary conditions and two application-motivatedmixed boundary conditions are considered. Along with the nonlinearity of thesystem, boundary singularities that appear when mixed boundary conditions arespecified are the main numerical challenges. Two approaches that use either atransition function or local corrections are developed to deal with theseboundary singularities. All the proposed numerical methods are validated usingcarefully designed numerical tests, where expected orders of accuracy and ratesof convergence are observed. A rough run-time performance comparison is alsoconducted to illustrate the efficiency of our methods. As an application of themethods, a snap-through thermal buckling problem is considered. The criticalthermal loads of shell buckling with various boundary conditions arenumerically calculated, and snap-through bifurcation curves are also obtainedusing our numerical methods together with a pseudo-arclength continuationmethod. Our results are consistent with previous studies.\",\n",
              " 'Efficient and accurate numerical algorithms are developed to solve ageneralized Kirchhoff-Love plate model subject to three common physicalboundary conditions: (i) clamped; (ii) simply supported; and (iii) free. Wesolve the model equation by discretizing the spatial derivatives usingsecond-order finite-difference schemes, and then advancing the semi-discreteproblem in time with either an explicit predictor-corrector or an implicitNewmark-Beta time-stepping algorithm. Stability analysis is conducted for theschemes and the results are used to determine stable time steps in practice.  A series of carefully chosen test problems are solved to demonstrate theproperties and applications of our numerical approaches. The numerical resultsconfirm the stability and 2nd-order accuracy of the algorithms, and are alsocomparable with experiments for similar thin plates. As an application, weillustrate a strategy to identify the natural frequencies of a plate using ournumerical methods in conjunction with a fast Fourier transformation (FFT) powerspectrum analysis of the computed data. Then we take advantage of one of thecomputed natural frequencies to simulate the interesting physical phenomenaknown as resonance and beat for a generalized Kirchhoff-Love plate.',\n",
              " 'We propose heavy ball neural ordinary differential equations (HBNODEs),leveraging the continuous limit of the classical momentum accelerated gradientdescent, to improve neural ODEs (NODEs) training and inference. HBNODEs havetwo properties that imply practical advantages over NODEs: (i) The adjointstate of an HBNODE also satisfies an HBNODE, accelerating both forward andbackward ODE solvers, thus significantly reducing the number of functionevaluations (NFEs) and improving the utility of the trained models. (ii) Thespectrum of HBNODEs is well structured, enabling effective learning oflong-term dependencies from complex sequential data. We verify the advantagesof HBNODEs over NODEs on benchmark tasks, including image classification,learning complex dynamics, and sequential modeling. Our method requiresremarkably fewer forward and backward NFEs, is more accurate, and learnslong-term dependencies more effectively than the other ODE-based neural networkmodels. Code is available at \\\\url{https://github.com/hedixia/HeavyBallNODE}.',\n",
              " 'In this work, we propose and develop efficient and accurate numerical methodsfor solving the Kirchhoff-Love plate model in domains with complex geometries.The algorithms proposed here employ curvilinear finite-difference methods forspatial discretization of the governing PDEs on general composite overlappinggrids. The coupling of different components of the composite overlapping gridis through numerical interpolations. However, interpolations introduceperturbation to the finite-difference discretization, which causes numericalinstability for time-stepping schemes used to advance the resultedsemi-discrete system. To address the instability, we propose to add afourth-order hyper-dissipation to the spatially discretized system to stabilizeits time integration; this additional dissipation term captures the essentialupwinding effect of the original upwind scheme. The investigation of strategiesfor incorporating the upwind dissipation term into several time-steppingschemes (both explicit and implicit) leads to the development of four novelalgorithms. For each algorithm, formulas for determining a stable time step anda sufficient dissipation coefficient on curvilinear grids are derived byperforming a local Fourier analysis. Quadratic eigenvalue problems for asimplified model plate in 1D domain are considered to reveal the weakinstability due to the presence of interpolating equations in the spatialdiscretization. This model problem is further investigated for thestabilization effects of the proposed algorithms. Carefully designed numericalexperiments are carried out to validate the accuracy and stability of theproposed algorithms, followed by two benchmark problems to demonstrate thecapability and efficiency of our approach for solving realistic applications.Results that concern the performance of the proposed algorithms are alsopresented.',\n",
              " 'Purpose: To develop a neural ordinary differential equation (ODE) model forvisualizing deep neural network (DNN) behavior during multi-parametric MRI(mp-MRI) based glioma segmentation as a method to enhance deep learningexplainability. Methods: By hypothesizing that deep feature extraction can bemodeled as a spatiotemporally continuous process, we designed a novel deeplearning model, neural ODE, in which deep feature extraction was governed by anODE without explicit expression. The dynamics of 1) MR images afterinteractions with DNN and 2) segmentation formation can be visualized aftersolving ODE. An accumulative contribution curve (ACC) was designed toquantitatively evaluate the utilization of each MRI by DNN towards the finalsegmentation results. The proposed neural ODE model was demonstrated using 369glioma patients with a 4-modality mp-MRI protocol: T1, contrast-enhanced T1(T1-Ce), T2, and FLAIR. Three neural ODE models were trained to segmentenhancing tumor (ET), tumor core (TC), and whole tumor (WT). The key MRmodalities with significant utilization by DNN were identified based on ACCanalysis. Segmentation results by DNN using only the key MR modalities werecompared to the ones using all 4 MR modalities. Results: All neural ODE modelssuccessfully illustrated image dynamics as expected. ACC analysis identifiedT1-Ce as the only key modality in ET and TC segmentations, while both FLAIR andT2 were key modalities in WT segmentation. Compared to the U-Net results usingall 4 MR modalities, Dice coefficient of ET (0.784->0.775), TC (0.760->0.758),and WT (0.841->0.837) using the key modalities only had minimal differenceswithout significance. Conclusion: The neural ODE model offers a new tool foroptimizing the deep learning model inputs with enhanced explainability. Thepresented methodology can be generalized to other medical image-related deeplearning applications.',\n",
              " 'Understanding how youth make sense of machine learning and how learning aboutmachine learning can be supported in and out of school is more relevant thanever before as young people interact with machine learning powered applicationseveryday; while connecting with friends, listening to music, playing games, orattending school. In this symposium, we present different perspectives onunderstanding how learners make sense of machine learning in their everydaylives, how sensemaking of machine learning can be supported in and out ofschool through the construction of applications, and how youth criticallyevaluate machine learning powered systems. We discuss how sensemaking ofmachine learning applications involves the development and integration ofconceptual, creative, and critical understandings that are increasinglyimportant to prepare youth to participate in the world.',\n",
              " \"Involving integrated development environments (IDEs) in introductory-level(CS1) programming courses is critical. However, it is difficult for instructorsto find a suitable IDE that is beginner friendly and supports strongfunctionality. In this paper, we report the experience of using Visual StudioCode (VS Code) in a CS1 programming course. We describe our motivation forchoosing VS Code and how we introduce it to students. We create comprehensiveguidance with hierarchical indexing to help students with diverse programmingbackgrounds. We perform an experimental evaluation of students' programmingexperience of using VS Code and validate the VS Code together with guidance asa promising solution for CS1 programming courses.\",\n",
              " 'Java is the \"go-to\" programming language choice for developing scalableenterprise cloud applications. In such systems, even a few percent CPU timesavings can offer a significant competitive advantage and cost saving. Althoughperformance tools abound in Java, those that focus on the data locality in thememory hierarchy are rare.  In this paper, we present DJXPerf, a lightweight, object-centric memoryprofiler for Java, which associates memory-hierarchy performance metrics (e.g.,cache/TLB misses) with Java objects. DJXPerf uses statistical sampling ofhardware performance monitoring counters to attribute metrics to not onlysource code locations but also Java objects. DJXPerf presents Java objectallocation contexts combined with their usage contexts and presents themordered by the poor locality behaviors. DJXPerf\\'s performance measurement,object attribution, and presentation techniques guide optimizing objectallocation, layout, and access patterns. DJXPerf incurs only ~8% runtimeoverhead and ~5% memory overhead on average, requiring no modifications tohardware, OS, Java virtual machine, or application source code, which makes itattractive to use in production. Guided by DJXPerf, we study and optimize anumber of Java and Scala programs, including well-known benchmarks andreal-world applications, and demonstrate significant speedups.',\n",
              " 'Memory bloat is an important source of inefficiency in complex productionsoftware, especially in software written in managed languages such as Java.Prior approaches to this problem have focused on identifying objects thatoutlive their life span. Few studies have, however, looked into whether and towhat extent myriad objects of the same type are identical. A quantitativeassessment of identical objects with code-level attribution can assistdevelopers in refactoring code to eliminate object bloat, and favor reuse ofexisting object(s). The result is reduced memory pressure, reduced allocationand garbage collection, enhanced data locality, and reduced re-computation, allof which result in superior performance.  We develop OJXPerf, a lightweight sampling-based profiler, whichprobabilistically identifies identical objects. OJXPerf employs hardwareperformance monitoring units (PMU) in conjunction with hardware debug registersto sample and compare field values of different objects of the same typeallocated at the same calling context but potentially accessed at differentprogram points. The result is a lightweight measurement, a combination ofobject allocation contexts and usage contexts ordered by duplication frequency.This class of duplicated objects is relatively easier to optimize. OJXPerfincurs 9% runtime and 6% memory overheads on average. We empirically show thebenefit of OJXPerf by using its profiles to instruct us to optimize a number ofJava programs, including well-known benchmarks and real-world applications. Theresults show a noticeable reduction in memory usage (up to 11%) and asignificant speedup (up to 25%).',\n",
              " \"We use the correlation functions of vertex operators to give a proof ofCauchy's formula.\",\n",
              " 'We give explicit constructions of quantum symplectic affine algebras at level1 using vertex operators.',\n",
              " 'We construct level one representations of the quantum affine algebra$U_q(G_2^{(1)})$ by vertex operators from bosonic fields.',\n",
              " 'The irreducible character values of the spin wreath products of the symmetricgroup and an arbitrary finite group are completely determined.',\n",
              " 'In this paper, vertex representations of the 2-toroidal Lie superalgebras oftype $D(m, n)$ are constructed using both bosonic fields and vertex operatorsbased on their loop algebraic presentation.',\n",
              " 'We study the fully entangled fraction of a quantum state. An upper bound isobtained for arbitrary bipartite system. This upper bound only depends on theFrobenius norm of the state.',\n",
              " 'The multiparameter quantum Pfaffian of the $(p, \\\\lambda)$-quantum group isintroduced and studied together with the quantum determinant, and an identityrelating the two invariants is given. Generalization to the multiparameterhyper-Pfaffian and relationship with the quantum minors are also considered.',\n",
              " 'We introduce an affinization of the quantum Kac-Moody algebra associated to asymmetric generalized Cartan matrix. Based on the affinization, we construct arepresentation of the quantum Kac-Moody algebra by vertex operators frombosonic fields. We also obtain a combinatorial indentity about Hall-Littlewoodpolynomials.',\n",
              " 'Generalizing our earlier work, we introduce the homogeneous quantum$Z$-algebras for all quantum affine algebras $\\\\alg$ of type one. With the newalgebras we unite previously scattered realizations of quantum affine algebrasin various cases. As a result we find a realization of $U_q(F_4^{(1)})$.',\n",
              " 'We prove Auslander-Gorenstein and $\\\\GKdim$-Macaulay properties for certaininvariant subrings of some quantum algebras, the Weyl algebras, and theuniversal enveloping algebras of finite dimensional Lie algebras.',\n",
              " 'Recently the second named author discovered a combinatorial identity in thecontext of vertex representations of quantum Kac-Moody algebras. We give adirect and elementary proof of this identity. Our method is to show a relatedidentity of distributions.',\n",
              " 'We provide a direct proof of the Drinfeld realization for the quantum affinealgebras.',\n",
              " 'We construct explicitly the quantum symplectic affine algebra$U_q(\\\\widehat{sp}_{2n})$ using bosonic fields. The Fock space decomposes intoirreducible modules of level -1/2, quantizing the Feingold-Frenkel constructionfor q=1.',\n",
              " 'Using our recent bosonic realization of $U_q(\\\\widehat{sp}_{2n})$, weconstruct explicitly the vertex operators for the level -1/2 modules of$U_q(\\\\widehat{sp}_{2n})$ using bosonic fields. Our method contains a detailedanalysis of all the q-intertwining relations.',\n",
              " 'We use fermionic operators to construct toroidal Lie algebras of classicaltypes, including in particular that of symplectic affine algebras, which isfirst realized by fermions.',\n",
              " 'We propose a quantum analogue of a Tits-Kantor-Koecher algebra with a Jordantorus as an coordinated algebra by looking at the vertex operator constructionover a Fock space.',\n",
              " 'We systematically study wreath product Schur functions and give acombinatorial construction using colored partitions and tableaux. The Pierirule and the Littlewood-Richardson rule are studied. We also discuss theconnection with representations of generalized symmetric groups.',\n",
              " \"Generalizing Feingold-Frenkel's construction we use Weyl bosonic fields toconstruct toroidal Lie algebras of types $A_n, B_n$, $C_n$ and $D_n$ of level$-1, -2, -1/2$ and -2 respectively. In particular, our construction also givesnew bosonic construction for the orthogonal Lie algebras in the cases of affineLie algebras.\",\n",
              " 'We introduce two-parameter quantum toroidal algebras of simply laced typesand provide their group theoretic realization using finite subgroups of$SL_2(\\\\mathbb C)$ via McKay correspondence. In particular our constructioncontains a realization of the vertex representation of the two-parameterquantum affine algebras of $ADE$ types.',\n",
              " 'We construct all fundamental modules for the two parameter quantum affinealgebra of type $A$ using a combinatorial model of Young diagrams. Inparticular we also give a fermionic realization of the two-parameter quantumaffine algebra.',\n",
              " 'On the vertex operator algebra associated with rank one lattice we derive ageneral formula for products of vertex operators in terms of generalizedhomogeneous symmetric functions. As an application we realize Jack symmetricfunctions of rectangular shapes as well as marked rectangular shapes.',\n",
              " 'The action of the Bernstein operators on Schur functions was given in termsof codes in [CG] and extended to the analog in Schur Q-functions in [HJS]. Wedefine a new combinatorial model of extended codes and show that both of theseresults follow from a natural combinatorial relation induced on codes. The newalgebraic structure provides a natural setting for Schur functions indexed bycompositions.',\n",
              " 'We give the principal realization of the twisted Yangians of orthogonal andsymplectic types. The new bases are interpreted in terms of discrete Fouriertransform over the cyclic group Z_N.',\n",
              " 'A generalization of Newton\\'s identity on symmetric functions is given. Usingthe generalized Newton identity we give a unified method to show the existenceof Hall-Littlewood, Jack and Macdonald polynomials. We also give a simple proofof the Jing-J\\\\\"ozefiak formula for two-row Macdonald functions.',\n",
              " 'A notion of Drinfeld polynomials is introduced for modules of two-parameterquantum affine algebras. Finite dimensional representations are thencharacterized by sets of $l$-tuples of pairs of Drinfeld polynomials withcertain conditions.',\n",
              " 'Counting homomorphisms between cyclic groups is a common exercise in a firstcourse in abstract algebra. A similar problem, accessible at the same level, isto count the number of group homomorphisms from a dihedral group of order $2m$into a dihedral group of order $2n$. While the solution requires onlyelementary group theory, the result does not appear in the literature or in theusual texts. As the solution may be of interest, particularly to those teachingundergraduate abstract algebra, it is provided in this note.',\n",
              " \"The artistic style of a painting is a subtle aesthetic judgment used by arthistorians for grouping and classifying artwork. The recently introduced`neural-style' algorithm substantially succeeds in merging the perceivedartistic style of one image or set of images with the perceived content ofanother. In light of this and other recent developments in image analysis viaconvolutional neural networks, we investigate the effectiveness of a`neural-style' representation for classifying the artistic style of paintings.\",\n",
              " 'Learning informative representations of data is one of the primary goals ofdeep learning, but there is still little understanding as to whatrepresentations a neural network actually learns. To better understand this,subspace match was recently proposed as a method for assessing the similarityof the representations learned by neural networks. It has been shown that twonetworks with the same architecture trained from different initializationslearn representations that at hidden layers show low similarity when assessedwith subspace match, even when the output layers show high similarity and thenetworks largely exhibit similar performance on classification tasks. In thisnote, we present a simple example motivated by standard results in commutativealgebra to illustrate how this can happen, and show that although the subspacematch at a hidden layer may be 0, the representations learned may be isomorphicas vector spaces. This leads us to conclude that a subspace match comparison oflearned representations may well be uninformative, and it points to the needfor better methods of understanding learned representations.',\n",
              " 'We introduce the dense captioning task, which requires a computer visionsystem to both localize and describe salient regions in images in naturallanguage. The dense captioning task generalizes object detection when thedescriptions consist of a single word, and Image Captioning when one predictedregion covers the full image. To address the localization and description taskjointly we propose a Fully Convolutional Localization Network (FCLN)architecture that processes an image with a single, efficient forward pass,requires no external regions proposals, and can be trained end-to-end with asingle round of optimization. The architecture is composed of a ConvolutionalNetwork, a novel dense localization layer, and Recurrent Neural Networklanguage model that generates the label sequences. We evaluate our network onthe Visual Genome dataset, which comprises 94,000 images and 4,100,000region-grounded captions. We observe both speed and accuracy improvements overbaselines based on current state of the art approaches in both generation andretrieval settings.',\n",
              " 'We consider image transformation problems, where an input image istransformed into an output image. Recent methods for such problems typicallytrain feed-forward convolutional neural networks using a \\\\emph{per-pixel} lossbetween the output and ground-truth images. Parallel work has shown thathigh-quality images can be generated by defining and optimizing\\\\emph{perceptual} loss functions based on high-level features extracted frompretrained networks. We combine the benefits of both approaches, and proposethe use of perceptual loss functions for training feed-forward networks forimage transformation tasks. We show results on image style transfer, where afeed-forward network is trained to solve the optimization problem proposed byGatys et al in real-time. Compared to the optimization-based method, ournetwork gives similar qualitative results but is three orders of magnitudefaster. We also experiment with single-image super-resolution, where replacinga per-pixel loss with a perceptual loss gives visually pleasing results.',\n",
              " \"To truly understand the visual world our models should be able not only torecognize images but also generate them. To this end, there has been excitingrecent progress on generating images from natural language descriptions. Thesemethods give stunning results on limited domains such as descriptions of birdsor flowers, but struggle to faithfully reproduce complex sentences with manyobjects and relationships. To overcome this limitation we propose a method forgenerating images from scene graphs, enabling explicitly reasoning aboutobjects and their relationships. Our model uses graph convolution to processinput graphs, computes a scene layout by predicting bounding boxes andsegmentation masks for objects, and converts the layout to an image with acascaded refinement network. The network is trained adversarially against apair of discriminators to ensure realistic outputs. We validate our approach onVisual Genome and COCO-Stuff, where qualitative results, ablations, and userstudies demonstrate our method's ability to generate complex images withmultiple objects.\",\n",
              " 'When building artificial intelligence systems that can reason and answerquestions about visual data, we need diagnostic tests to analyze our progressand discover shortcomings. Existing benchmarks for visual question answeringcan help, but have strong biases that models can exploit to correctly answerquestions without reasoning. They also conflate multiple sources of error,making it hard to pinpoint model weaknesses. We present a diagnostic datasetthat tests a range of visual reasoning abilities. It contains minimal biasesand has detailed annotations describing the kind of reasoning each questionrequires. We use this dataset to analyze a variety of modern visual reasoningsystems, providing novel insights into their abilities and limitations.',\n",
              " 'Recent progress in style transfer on images has focused on improving thequality of stylized images and speed of methods. However, real-time methods arehighly unstable resulting in visible flickering when applied to videos. In thiswork we characterize the instability of these methods by examining the solutionset of the style transfer objective. We show that the trace of the Gram matrixrepresenting style is inversely related to the stability of the method. Then,we present a recurrent convolutional network for real-time video style transferwhich incorporates a temporal consistency loss and overcomes the instability ofprior methods. Our networks can be applied at any resolution, do not re- quireoptical flow at test time, and produce high quality, temporally consistentstylized videos in real-time.',\n",
              " 'Existing methods for visual reasoning attempt to directly map inputs tooutputs using black-box architectures without explicitly modeling theunderlying reasoning processes. As a result, these black-box models often learnto exploit biases in the data rather than learning to perform visual reasoning.Inspired by module networks, this paper proposes a model for visual reasoningthat consists of a program generator that constructs an explicit representationof the reasoning process to be performed, and an execution engine that executesthe resulting program to produce an answer. Both the program generator and theexecution engine are implemented by neural networks, and are trained using acombination of backpropagation and REINFORCE. Using the CLEVR benchmark forvisual reasoning, we show that our model significantly outperforms strongbaselines and generalizes better in a variety of settings.',\n",
              " 'BatchNorm is a critical building block in modern convolutional neuralnetworks. Its unique property of operating on \"batches\" instead of individualsamples introduces significantly different behaviors from most other operationsin deep learning. As a result, it leads to many hidden caveats that cannegatively impact model\\'s performance in subtle ways. This paper thoroughlyreviews such problems in visual recognition tasks, and shows that a key toaddress them is to rethink different choices in the concept of \"batch\" inBatchNorm. By presenting these caveats and their mitigations, we hope thisreview can help researchers use BatchNorm more effectively.',\n",
              " 'Recent advancements in differentiable rendering and 3D reasoning have drivenexciting results in novel view synthesis from a single image. Despite realisticresults, methods are limited to relatively small view change. In order tosynthesize immersive scenes, models must also be able to extrapolate. Wepresent an approach that fuses 3D reasoning with autoregressive modeling tooutpaint large view changes in a 3D-consistent manner, enabling scenesynthesis. We demonstrate considerable improvement in single image large-angleview synthesis results compared to a variety of methods and possible variantsacross simulated and real datasets. In addition, we show increased 3Dconsistency compared to alternative accumulation methods. Project website:https://crockwell.github.io/pixelsynth/',\n",
              " \"Visual and linguistic concepts naturally organize themselves in a hierarchy,where a textual concept ``dog'' entails all images that contain dogs. Despitebeing intuitive, current large-scale vision and language models such as CLIP donot explicitly capture such hierarchy. We propose MERU, a contrastive modelthat yields hyperbolic representations of images and text. Hyperbolic spaceshave suitable geometric properties to embed tree-like data, so MERU can bettercapture the underlying hierarchy in image-text data. Our results show that MERUlearns a highly interpretable representation space while being competitive withCLIP's performance on multi-modal tasks like image classification andimage-text retrieval.\",\n",
              " 'An approach to evolutionary ensemble learning for classification is proposedin which boosting is used to construct a stack of programs. Each application ofboosting identifies a single champion and a residual dataset, i.e. the trainingrecords that thus far were not correctly classified. The next program is onlytrained against the residual, with the process iterating until some maximumensemble size or no further residual remains. Training against a residualdataset actively reduces the cost of training. Deploying the ensemble as astack also means that only one classifier might be necessary to make aprediction, so improving interpretability. Benchmarking studies are conductedto illustrate competitiveness with the prediction accuracy of currentstate-of-the-art evolutionary ensemble learning algorithms, while providingsolutions that are orders of magnitude simpler. Further benchmarking with ahigh cardinality dataset indicates that the proposed method is also moreaccurate and efficient than XGBoost.',\n",
              " 'We show that the rigid C*-tensor categories of finite dimensional type 1unitary representations of the quantum groups $U_{q}(\\\\mathfrak{g}_{2})$corresponding to the exceptional Lie group $G_2$ for positive $q\\\\ne 1$ haveproperty (T).',\n",
              " 'We establish rank-finiteness for the class of $G$-crossed braided fusioncategories, generalizing the recent result for modular categories and includingthe important case of braided fusion categories. This necessitates a study ofslightly degenerate braided fusion categories and their centers, which areinteresting for their own sake.',\n",
              " 'We provide a parameterization of all fusion subcategories of theequivariantization by a group action on a fusion category. As applications, weclassify the Hopf subalgebras of a family of semisimple Hopf algebras ofKac-Paljutkin type and recover Naidu-Nikshych-Witherspoon classification of thefusion subcategories of the representation category of a twisted quantum doubleof a finite group.',\n",
              " 'A $G$-graded extension of a fusion category $\\\\mathcal{C}$ yields acategorical action of $G$ on the center $Z(\\\\mathcal C)$. If the extensionadmits a spherical structure, we provide a method for recovering its fusionrules in terms of the action. We then apply this to find closed formulas forthe fusion rules of extensions of some group theoretical categories and ofcyclic permutation crossed extensions of modular categories.',\n",
              " 'Triangle presentations are combinatorial structures on finite projectivegeometries which characterize groups acting simply transitively on the verticesof a locally finite building of type $\\\\tilde{\\\\text{A}}_{n-1}$ ($n\\\\ge3$). From atype $\\\\tilde{\\\\text{A}}_{n-1}$ triangle presentation on a geometry of order $q$,we construct a fiber functor on the diagrammatic monoidal category$\\\\text{Web}(\\\\text{SL}^{-}_{n})$ over any field $\\\\mathbb{k}$ with characteristic$p\\\\ge n-1$ such that $q \\\\equiv 1$ mod $p$. When $\\\\mathbb{k}$ is algebraicallyclosed and $n$ odd, this gives new fiber functors on the category of tiltingmodules for $\\\\text{SL}_{n}$.',\n",
              " 'A Q-system in a C* 2-category is a unitary version of a separable Frobeniusalgebra object and can be viewed as a unitary version of a higher idempotent.We define a higher unitary idempotent completion for C* 2-categories calledQ-system completion and study its properties. We show that the C* 2-category ofright correspondences of unital C*-algebras is Q-system complete byconstructing an inverse realization $\\\\dag$ 2-functor. We use this result toconstruct induced actions of group theoretical unitary fusion categories oncontinuous trace C*-algebras with connected spectra.',\n",
              " \"Popa introduced the tensor category $\\\\tilde{\\\\chi}(M)$ of approximately inner,centrally trivial bimodules of a $\\\\rm{II}_{1}$ factor $M$, generalizing Connes'$\\\\chi(M)$. We extend Popa's notions to define the $\\\\rm W^*$-tensor category$\\\\operatorname{End}_{\\\\rm loc}(\\\\mathcal{C})$ of local endofunctors on a $\\\\rmW^*$-category $\\\\mathcal{C}$. We construct a unitary braiding on$\\\\operatorname{End}_{\\\\rm loc}(\\\\mathcal{C})$, giving a new construction of abraided tensor category associated to an arbitrary $\\\\rm W^*$-category. For the$\\\\rm W^*$-category of finite modules over a $\\\\rm{II}_{1}$ factor, this yields aunitary braiding on Popa's $\\\\tilde{\\\\chi}(M)$, which extends Jones' $\\\\kappa$invariant for $\\\\chi(M)$.  Given a finite depth inclusion $M_{0}\\\\subseteq M_{1}$ of non-Gamma$\\\\rm{II}_1$ factors, we show that the braided unitary tensor category$\\\\tilde{\\\\chi}(M_{\\\\infty})$ is equivalent to the Drinfeld center of the standardinvariant, where $M_{\\\\infty}$ is the inductive limit of the associated Jonestower. This implies that for any pair of finite depth non-Gamma subfactors$N_{0}\\\\subseteq N_{1}$ and $M_{0}\\\\subseteq M_{1}$, if the standard invariantsare not Morita equivalent, then the inductive limit factors $N_{\\\\infty}$ and$M_{\\\\infty}$ are not stably isomorphic.\",\n",
              " 'We provide a description of the annular representation category of the freeproduct of two rigid C*-tensor categories.',\n",
              " 'Bisch and Jones suggested the skein theoretic classification of planaralgebras and investigated the ones generated by 2-boxes with the second author.In this paper, we consider 3-box generators and classify subfactor planaralgebras generated by a non-trivial 3-box satisfying a relation proposed byThurston. The subfactor planar algebras in the classification are either $E^6$or the ones from representations of quantum $SU(N)$. We introduce a new methodto determine positivity of planar algebras and new techniques to reduce thecomplexity of computations.',\n",
              " 'We give a characterization of extremal irreducible discrete subfactors$(N\\\\subseteq M, E)$ where $N$ is type ${\\\\rm II}_1$ in terms of connectedW*-algebra objects in rigid C*-tensor categories. We prove an equivalence ofcategories where the morphisms for discrete inclusions are normal $N-N$bilinear ucp maps which preserve the state $\\\\tau \\\\circ E$, and the morphismsfor W*-algebra objects are categorical ucp morphisms.  As an application, we get a well-behaved notion of the standard invariant ofan extremal irreducible discrete subfactor, together with a subfactorreconstruction theorem. Thus our equivalence provides many new examples ofdiscrete inclusions $(N\\\\subseteq M, E)$, in particular, examples where $M$ istype ${\\\\rm III}$ coming from non Kac-type discrete quantum groups andassociated module W*-categories. Finally, we obtain a Galois correspondencebetween intermediate subfactors of an extremal irreducible discrete inclusionand intermediate W*-algebra objects.',\n",
              " 'The orbifold construction $A\\\\mapsto A^G$ for a finite group $G$ isfundamental in rational conformal field theory. The construction of $Rep(A^G)$from $Rep(A)$ on the categorical level, often called gauging, is also prominentin the study of topological phases of matter. Given a non-degenerate braidedfusion category $\\\\mathcal{C}$ with a $G$-action, the key step in thisconstruction is to find a braided $G$-crossed extension compatible with theaction. The extension theory of Etingof-Nikshych-Ostrik gives two obstructionsfor this problem, $o_3\\\\in H^3(G)$ and $o_4\\\\in H^4(G)$ for certain coefficients,the latter depending on a categorical lifting of the action and is notoriouslydifficult to compute. We show that in the case where $G\\\\le S_n$ acts bypermutations on $\\\\mathcal{C}^{\\\\boxtimes n}$, both of these obstructions vanish.This verifies a conjecture of M\\\\\"uger, and constitutes a nontrivial test of theconjecture that all modular tensor categories come from vertex operatoralgebras or conformal nets.',\n",
              " \"For a braided fusion category $\\\\mathcal{V}$, a $\\\\mathcal{V}$-fusion categoryis a fusion category $\\\\mathcal{C}$ equipped with a braided monoidal functor$\\\\mathcal{F}:\\\\mathcal{V} \\\\to Z(\\\\mathcal{C})$. Given a fixed$\\\\mathcal{V}$-fusion category $(\\\\mathcal{C}, \\\\mathcal{F})$ and a fixed$G$-graded extension $\\\\mathcal{C}\\\\subseteq \\\\mathcal{D}$ as an ordinary fusioncategory, we characterize the enrichments $\\\\widetilde{\\\\mathcal{F}}:\\\\mathcal{V}\\\\to Z(\\\\mathcal{D})$ of $\\\\mathcal{D}$ which are compatible with the enrichmentof $\\\\mathcal{C}$. We show that G-crossed extensions of a braided fusioncategory $\\\\mathcal{C}$ are G-extensions of the canonical enrichment of$\\\\mathcal{C}$ over itself. As an application, we parameterize the set of$G$-crossed braidings on a fixed $G$-graded fusion category in terms of certainsubcategories of its center, extending Nikshych's classification of thebraidings on a fusion category.\",\n",
              " 'For a group $G$ and $\\\\omega\\\\in Z^{3}(G, \\\\text{U}(1))$, an $\\\\omega$-anomalousaction on a C*-algebra $B$ is a $\\\\text{U}(1)$-linear monoidal functor between2-groups $\\\\text{2-Gr}(G, \\\\text{U}(1), \\\\omega)\\\\rightarrow\\\\underline{\\\\text{Aut}}(B)$, where the latter denotes the 2-group of$*$-automorphisms of $B$. The class $[\\\\omega]\\\\in H^{3}(G, \\\\text{U}(1))$ iscalled the anomaly of the action. We show for every $n\\\\ge 2$ and every finitegroup $G$, every anomaly can be realized on the stabilization of a commutativeC*-algebra $C(M)\\\\otimes \\\\mathcal{K}$ for some closed connected $n$-manifold$M$. We also show that although there are no anomalous symmetries of RoeC*-algebras of coarse spaces, for every finite group $G$, every anomaly can berealized on the Roe corona $C^{*}(X)/\\\\mathcal{K}$ of some bounded geometrymetric space $X$ with property $A$.',\n",
              " 'Topological domain walls separating 2+1 dimensional topologically orderedphases can be understood in terms of Witt equivalences between the UMTCsdescribing anyons in the bulk topological orders. However, this picture doesnot provide a framework for decomposing stacks of multiple domain walls intosuperselection sectors - i.e., into fundamental domain wall types that cannotbe mixed by any local operators. Such a decomposition can be understood usingan alternate framework in the case that the topological order is anomaly-free,in the sense that it can be realized by a commuting projector lattice model. Byplacing these Witt equivalences in the context of a 3-category of potentiallyanomalous (2+1)D topological orders, we develop a framework for computing thedecomposition of parallel topological domain walls into indecomposablesuperselection sectors, extending the previous understanding to topologicalorders with non-trivial anomaly. We characterize the superselection sectors interms of domain wall particle mobility, which we formalize in terms oftunnelling operators. The mathematical model for the 3-category of topologicalorders is the 3-category of fusion categories enriched over a fixed unitarymodular tensor category.',\n",
              " 'For a net of C*-algebras on a discrete metric space, we introduce a bimoduleversion of the DHR tensor category, and show it is an invariant of quasi-localalgebras under isomorphisms with bounded spread. For abstract spin systems on alattice $L\\\\subseteq \\\\mathbb{R}^{n}$ satisfying a weak version of Haag duality,we construct a braiding on these categories. Applying the general theory toquasi-local algebras $A$ of operators on a lattice invariant under a(categorical) symmetry, we obtain a homomorphism from the group of symmetricquantum cellular automata (QCA) to $\\\\textbf{Aut}_{br}(\\\\textbf{DHR}(A))$,containing symmetric finite depth circuits in the kernel. For a spin chain withfusion categorical symmetry $\\\\mathcal{D}$, we show the DHR category of thequasi-local algebra of symmetric operators is equivalent to the Drinfeld center$\\\\mathcal{Z}(\\\\mathcal{D})$ . We use this to show that for the double spin flipaction $\\\\mathbb{Z}/2\\\\mathbb{Z}\\\\times \\\\mathbb{Z}/2\\\\mathbb{Z}\\\\curvearrowright\\\\mathbb{C}^{2}\\\\otimes \\\\mathbb{C}^{2}$, the group of symmetric QCA modulosymmetric finite depth circuits in 1D contains a copy of $S_{3}$, hence isnon-abelian, in contrast to the case with no symmetry.',\n",
              " 'We define annular algebras for rigid $C^{*}$-tensor categories, providing aunified framework for both Ocneanu\\'s tube algebra and Jones\\' affine annularcategory of a planar algebra. We study the representation theory of annularalgebras, and show that all sufficiently large (full) annular algebras for acategory are isomorphic after tensoring with the algebra of matrix units withcountable index set, hence have equivalent representation theories. Annularalgebras admit a universal $C^{*}$-algebra closure analogous to the universal$C^{*}$-algebra for groups. These algebras have interesting corner algebrasindexed by some set of isomorphism classes of objects, which we callcentralizer algebras. The centralizer algebra corresponding to the identityobject is canonically isomorphic to the fusion algebra of the category, and weshow that the admissible representations of the fusion algebra of Popa and Vaesare precisely the restrictions of arbitrary (non-degenerate)$*$-representations of full annular algebras. This allows approximation andrigidity properties defined for categories by Popa and Vaes to be interpretedin the context of annular representation theory. This perspective also allowsus to define \"higher weight\" approximation properties based on othercentralizer algebras of an annular algebra. Using the analysis of annularrepresentations due to Jones and Reznikoff, we identify all centralizeralgebras for the $TLJ(\\\\delta)$ categories for $\\\\delta\\\\ge 2$.',\n",
              " 'We show that given a rigid C*-tensor category, there is an equivalence ofcategories between normalized irreducible Q-systems, also known as connectedunitary Frobenius algebra objects, and compact connected W*-algebra objects.Although this result could be proved as a corollary of our previous article onrealizations of algebra objects and discrete subfactors, we prove it heredirectly via categorical methods without passing through subfactor theory.',\n",
              " 'In this note, we examine the gauging of the $\\\\mathbb{Z}/2\\\\mathbb{Z}$permutation action on the tensor square of a modular tensor category. When$\\\\mathcal{C}$ has no nontrivial invertible objects, we provide formulas for thefusion rules of both the extensions, expressed in terms of the fusion rules of$\\\\mathcal{C}$, and the subsequent equivariantizations, which additionallyrequires the modular data of $\\\\mathcal{C}$. We discuss several examples relatedto quantum groups at roots of unity.',\n",
              " 'We introduce a K-theoretic invariant for actions of unitary fusion categorieson unital C*-algebras. We show that for inductive limits of finite dimensionalactions of fusion categories on unital AF-algebras, this is a completeinvariant. In particular, this gives a complete invariant for inductive limitactions of finite groups on AF-algebras. We apply our results to obtain aclassification of finite depth, strongly AF-inclusions of unital AF-algebras.',\n",
              " \"In this paper, we extend Ocneanu's theory of connections on graphs to definea 2-category whose 0-cells are tracial Bratteli diagrams, and whose 1-cells aregeneralizations of unitary connections. We show that this 2-category admits anembedding into the 2-category of hyperfinite von Neumann algebras, generalizingfundamental results from subfactor theory to a 2-categorical setting.\",\n",
              " \"In a physical system undergoing a continuous quantum phase transition,spontaneous symmetry breaking occurs when certain symmetries of the Hamiltonianfail to be preserved in the ground state. In the traditional Landau theory, asymmetry group can break down to any subgroup. However, this no longer holdsacross a continuous phase transition driven by anyon condensation in symmetryenriched topological orders (SETOs). For a SETO described by a $G$-crossedbraided extension $\\\\mathcal{C}\\\\subseteq \\\\mathcal{C}^{\\\\times}_{G}$, we show thatphysical considerations require that a connected \\\\'etale algebra $A\\\\in\\\\mathcal{C}$ admit a $G$-equivariant algebra structure for symmetry to bepreserved under condensation of $A$. Given any categorical action$\\\\underline{G}\\\\rightarrow \\\\underline{\\\\sf Aut}_{\\\\otimes}^{\\\\sf br}(\\\\mathcal{C})$such that $g(A)\\\\cong A$ for all $g\\\\in G$, we show there is a short exactsequence whose splittings correspond to $G$-equivariant algebra structures. Thenon-splitting of this sequence forces spontaneous symmetry breaking undercondensation of $A$. Furthermore, we show that if symmetry is preserved, thereis a canonically associated SETO of $\\\\mathcal{C}^{\\\\operatorname{loc}}_{A}$, andgauging this symmetry commutes with anyon condensation.\",\n",
              " \"We give formulae for the multiplicities of eigenvalues of generalizedrotation operators in terms of generalized Frobenius-Schur indicators in asemisimple spherical tensor category $\\\\mathcal{C}$. In particular, this impliesthat the entire collection of rotation eigenvalues for a fusion category can becomputed from the fusion rules and the traces of rotation at finitely manytensor powers. We also establish a rigidity property for FS indicators offusion categories with a given fusion ring via Jones's theory of planaralgebras. If $\\\\mathcal{C}$ is also braided, these formulae yield themultiplicities of eigenvalues for a large class of braids in the associatedbraid group representations. When $\\\\mathcal{C}$ is modular, this allows one todetermine the eigenvalues and multiplicities of braids in terms of just the $S$and $T$ matrices.\",\n",
              " 'In the ONeMg cores of $8.8-9.5~{\\\\rm M}_\\\\odot$ stars, neon and oxygen burningis ignited off-center. Whether the neon-oxygen flame propagates to the centeris critical to determine whether these stars undergo Fe core collapse orelectron capture induced ONeMg core collapse. We present more details of starsthat ignite neon and oxygen burning off-center. The neon flame is establishedin a similar manner to the carbon flame of super-AGB stars, albeit with anarrower flame width. The criteria for establishing a flame are able to be metif the strict Schwarzschild criterion for convective instability is adopted.Mixing across the interface of the convective shell disrupts the conditions forthe propagation of the burning front and instead the shell burns as a series ofinward-moving flashes. While this may not directly affect whether the burningwill reach the center (as in super-AGB stars), the core is allowed to contractbetween each shell flash. Reduction of the electron fraction in the shellreduces the Chandrasekhar mass and the center reaches the threshold density forthe URCA process to activate and steer the remaining evolution of the core.This highlights the importance of a more accurate treatment of mixing in thestellar interior for yet another important question in stellar astrophysics -determining the properties of stellar evolution and supernova progenitors atthe boundary between electron capture supernova and iron core-collapsesupernova.',\n",
              " 'The stellar mass range 8<M/Mo<12 corresponds to the most massive AGB starsand the most numerous massive stars. It is host to a variety of supernovaprogenitors and is therefore very important for galactic chemical evolution andstellar population studies. In this paper, we study the transition fromsuper-AGB star to massive star and find that a propagating neon-oxygen burningshell is common to both the most massive electron capture supernova (EC-SN)progenitors and the lowest mass iron-core collapse supernova (FeCCSN)progenitors. Of the models that ignite neon burning off-center, the 9.5Mo modelwould evolve to an FeCCSN after the neon-burning shell propagates to thecenter, as in previous studies. The neon-burning shell in the 8.8Mo model,however, fails to reach the center as the URCA process and an extended (0.6 Mo)region of low Ye (0.48) in the outer part of the core begin to dominate thelate evolution; the model evolves to an EC-SN. This is the first study tofollow the most massive EC-SN progenitors to collapse, representing anevolutionary path to EC-SN in addition to that from SAGB stars undergoingthermal pulses. We also present models of an 8.75Mo super-AGB star through itsentire thermal pulse phase until electron captures on 20Ne begin at its centerand of a 12Mo star up to the iron core collapse. We discuss key uncertaintiesand how the different pathways to collapse affect the pre-supernova structure.Finally, we compare our results to the observed neutron star mass distribution.',\n",
              " 'Massive stars are key sources of radiative, kinetic, and chemical feedback inthe universe. Grids of massive star models computed by different groups eachusing their own codes, input physics choices and numerical approximations,however, lead to inconsistent results for the same stars. We use three of these1D codes---GENEC, KEPLER and MESA---to compute non-rotating stellar models of$15~\\\\mathrm{M}_\\\\odot$, $20~\\\\mathrm{M}_\\\\odot$, and $25~\\\\mathrm{M}_\\\\odot$ andcompare their nucleosynthesis. We follow the evolution from the main sequenceuntil the end of core helium burning. The GENEC and KEPLER models hold physicsassumptions used in large grids of published models. The MESA code was set upto use convective core overshooting such that the CO core masses are consistentwith those obtained by GENEC. For all models, full nucleosynthesis is computedusing the NuGrid post-processing tool MPPNP.  We find that the surface abundances predicted by the models are in reasonableagreement. In the helium core, the standard deviation of the elementaloverproduction factors for Fe to Mo is less than $30\\\\,\\\\%$---smaller than theimpact of the present nuclear physics uncertainties. For our three initialmasses, the three stellar evolution codes yield consistent results. Differencesin key properties of the models, e.g., helium and CO core masses and the timespent as a red supergiant, are traced back to the treatment of convection and,to a lesser extent, mass loss. The mixing processes in stars remain the keyuncertainty in stellar modelling. Better constrained prescriptions are thusnecessary to improve the predictive power of stellar evolution models.',\n",
              " 'In the classical picture, electron-capture supernovae and theaccretion-induced collapse of oxygen-neon white dwarfs (ONeWDs) undergo anoxygen deflagration phase before gravitational collapse produces a neutron star(NS). These types of core collapse events are postulated to explain severalastronomical phenomena. In this work, the deflagration phase is simulated forthe first time using multidimensional hydrodynamics, with the aim of gainingnew insight into the explosive deaths of $8-10~M_\\\\odot$ stars and ONeWDs thataccrete material from a binary companion star. The main aim is to determinewhether these events are thermonuclear or core-collapse supernova explosions,and hence whether NSs are formed by such phenomena. The deflagration issimulated in ONe cores with three different central ignition densities. Theintermediate density case is perhaps the most realistic, being based on recentnuclear physics calculations and 1D stellar models. The 3D hydrodynamicsimulations presented in this work begin from a centrally confined flamestructure using a level-set-based flame approach and are performed in $256^3$and $512^3$ numerical resolutions. In the simulations with intermediate and lowignition density, the cores do not appear to collapse into NSs. Instead, almosta solar mass of material becomes unbound from the cores, leaving boundremnants. These simulations represent the case in which semiconvective mixingduring the electron-capture phase preceding the deflagration is inefficient.The masses of the bound remnants double when Coulomb corrections are includedin the EoS, however they still do not exceed the effective Chandrasekhar massand, hence, would not collapse into NSs. The simulations with the highestignition density ($\\\\log_{10}\\\\rho_{\\\\rm c}=10.3$), representing the case wheresemiconvective mixing is very efficient, show clear signs that the core willcollapse into a NS.',\n",
              " 'We investigate $^{60}$Fe in massive stars and core-collapse supernovaefocussing on uncertainties that influence its production in 15, 20 and 25$M_\\\\odot$ stars at solar metallicity. We find that the $^{60}$Fe yield is amonotonic increasing function of the uncertain $^{59}$Fe$(n,\\\\gamma)^{60}$Fecross section and that a factor of 10 reduction in the reaction rate results ina factor 8-10 reduction in the $^{60}$Fe yield; while a factor of 10 increasein the rate increases the yield by a factor 4-7. We find that none of the 189simulations we have performed are consistent with a core-collapse supernovatriggering the formation of the Solar System, and that only models using$^{59}$Fe$(n,\\\\gamma)^{60}$Fe cross section that is less than or equal to thatfrom NON-SMOKER can reproduce the observed $^{60}$Fe/$^{26}$Al line flux ratioin the diffuse ISM. We examine the prospects of detecting old core-collapsesupernova remnants (SNRs) in the Milky Way from their $\\\\gamma$-ray emissionfrom the decay of $^{60}$Fe, finding that the next generation of gamma-raymissions could be able to discover up to $\\\\sim100$ such old SNRs as well asmeasure the $^{60}$Fe yields of a handful of known Galactic SNRs. We alsopredict the X-ray spectrum that is produced by atomic transitions in $^{60}$Cofollowing its ionization by internal conversion and give theoretical X-ray linefluxes as a function of remnant age as well as the Doppler and fine-structureline broadening effects. The X-ray emission presents an interesting prospectfor addressing the missing SNR problem with future X-ray missions.',\n",
              " 'The CDF and D\\\\O\\\\ experiments at the Tevatron $p\\\\bar{p}$ collider establishedthat extensive and detailed exploration of the $b$--quark dynamics is possiblein hadron collisions, with results competitive and supplementary to those from$e^+e^-$ colliders. This provides a rich, and highly rewarding program that iscurrently reaching full maturity. I report a few recent world-leading resultson rare decays, CP-violation in $B^0_s$ mixing, $b\\\\to s$ penguin decays, andcharm physics.',\n",
              " 'Charm physics has played all along a central role in particle physics,however the level of attention on it has tremendously increased in the lastyears because of the observation of \"fast\" $D^0-\\\\bar{D}^0$ flavour oscillationsand because of very recent observed hints of CP violation. While in the pastthese would have been unambiguously interpreted as signs of New Physics, therevisitation of theoretical expectations, prompted by the latest experimentalmeasurements, makes the picture not clear. This brief review covers the currentstatus of CP-violating measurements in the $D^0-\\\\bar{D}^0$ system, both on theexperimental and theoretical side.',\n",
              " 'I report some recent results on direct CP violation measurements in hadronicdecays collected by the upgraded Collider Detector (CDF II) at the FermilabTevatron: CP-violating asymmetries in the two-body non-leptonic charmlessdecays of $b$-hadrons, the first reconstruction in hadron collisions of thesuppressed decays $B^- \\\\to D(\\\\to K^+\\\\pi^-)K^-$ and $B^- \\\\to D(\\\\toK^+\\\\pi^-)\\\\pi^-$, and the measurement of TP asymmetries in the $B^{0}_{s} \\\\to\\\\phi \\\\phi$ decays.',\n",
              " 'This paper analyzes a certain action called \"whirling\" that can be defined onany family of functions between two finite sets equipped with a linear (orcyclic) ordering. As a map on injections and surjections, we prove that withinany whirling-orbit, any two elements of the codomain appear as outputs offunctions the same number of times. This result, can be stated in terms of thehomomesy phenomenon, which occurs when a statistic has the same average acrossevery orbit. We further explore whirling on parking functions, order-preservingmaps, and restricted growth words, discussing homomesy results for each case.',\n",
              " 'We deploy algebraic complexity theoretic techniques for constructingsymmetric determinantal representations of for00504925mulas and weakly skewcircuits. Our representations produce matrices of much smaller dimensions thanthose given in the convex geometry literature when applied to polynomialshaving a concise representation (as a sum of monomials, or more generally as anarithmetic formula or a weakly skew circuit). These representations are validin any field of characteristic different from 2. In characteristic 2 we are ledto an almost complete solution to a question of B\\\\\"urgisser on theVNP-completeness of the partial permanent. In particular, we show that thepartial permanent cannot be VNP-complete in a finite field of characteristic 2unless the polynomial hierarchy collapses.',\n",
              " \"Certificates to a linear algebra computation are additional data structuresfor each output, which can be used by a-possibly randomized- verificationalgorithm that proves the correctness of each output. Wiede-mann's algorithmprojects the Krylov sequence obtained by repeatedly multiplying a vector by amatrix to obtain a linearly recurrent sequence. The minimal polynomial of thissequence divides the minimal polynomial of the matrix. For instance, if the$n\\\\times n$ input matrix is sparse with n 1+o(1) non-zero entries, thecomputation of the sequence is quadratic in the dimension of the matrix whilethe computation of the minimal polynomial is n 1+o(1), once that projectedKrylov sequence is obtained. In this paper we give algorithms that computecertificates for the Krylov sequence of sparse or structured $n\\\\times n$matrices over an abstract field, whose Monte Carlo verification complexity canbe made essentially linear. As an application this gives certificates for thedeterminant, the minimal and characteristic polynomials of sparse or structuredmatrices at the same cost.\",\n",
              " 'Computational problem certificates are additional data structures for eachoutput, which can be used by a-possibly randomized-verification algorithm thatproves the correctness of each output. In this paper, we give an algorithm thatcomputes a certificate for the minimal polynomial of sparse or structured nxnmatrices over an abstract field, of sufficiently large cardinality, whose MonteCarlo verification complexity requires a single matrix-vector multiplicationand a linear number of extra field operations. We also propose a novelpreconditioner that ensures irreducibility of the characteristic polynomial ofthe generically preconditioned matrix. This preconditioner takes linear time tobe applied and uses only two random entries. We then combine these twotechniques to give algorithms that compute certificates for the determinant,and thus for the characteristic polynomial, whose Monte Carlo verificationcomplexity is therefore also linear.',\n",
              " 'In this paper, we give novel certificates for triangular equivalence and rankprofiles. These certificates enable somebody to verify the row or column rankprofiles or the whole rank profile matrix faster than recomputing them, with anegligible overall overhead. We first provide quadratic time and spacenon-interactive certificates saving the logarithmic factors of previously knownones. Then we propose interactive certificates for the same problems whoseMonte Carlo verification complexity requires a small constant number ofmatrix-vector multiplications, a linear space, and a linear number of extrafield operations , with a linear number of interactions. As an application wealso give an interactive protocol, certifying the determinant or the signatureof dense matrices, faster for the Prover than the best previously known one.Finally we give linear space and constant round certificates for the row orcolumn rank profiles.',\n",
              " 'Differential (Ore) type polynomials with \"approximate\" polynomialcoefficients are introduced. These provide an effective notion of approximatedifferential operators, with a strong algebraic structure. We introduce theapproximate Greatest Common Right Divisor Problem (GCRD) of differentialpolynomials, as a non-commutative generalization of the well-studiedapproximate GCD problem.  Given two differential polynomials, we present an algorithm to find nearbydifferential polynomials with a non-trivial GCRD, where nearby is defined withrespect to a suitable coefficient norm. Intuitively, given two lineardifferential polynomials as input, the (approximate) GCRD problem correspondsto finding the (approximate) differential polynomial whose solution space isthe intersection of the solution spaces of the two inputs.  The approximate GCRD problem is proven to be locally well-posed. A methodbased on the singular value decomposition of a differential Sylvester matrix isdeveloped to produce an initial approximation of the GCRD. With a sufficientlygood initial approximation, Newton iteration is shown to converge quadraticallyto an optimal solution. Finally, sufficient conditions for existence of asolution to the global problem are presented along with examples demonstratingthat no solution exists when these conditions are not satisfied.',\n",
              " 'Certificates to a linear algebra computation are additional data structuresfor each output, which can be used by a---possibly randomized---verificationalgorithm that proves the correctness of each output. The certificates areessentially optimal if the time (and space) complexity of verification isessentially linear in the input size $N$, meaning $N$ times a factor$N^{o(1)}$, i.e., a factor $N^{\\\\eta(N)}$ with $\\\\lim\\\\_{N\\\\to \\\\infty} \\\\eta(N)$ $=$$0$. We give algorithms that compute essentially optimal certificates for thepositive semidefiniteness, Frobenius form, characteristic and minimalpolynomial of an $n\\\\times n$ dense integer matrix $A$. Our certificates can beverified in Monte-Carlo bit complexity $(n^2 \\\\log\\\\|A\\\\|)^{1+o(1)}$, where$\\\\log\\\\|A\\\\|$ is the bit size of the integer entries, solving an open problem in[Kaltofen, Nehring, Saunders, Proc.\\\\ ISSAC 2011] subject to computationalhardness assumptions. Second, we give algorithms that compute certificates forthe rank of sparse or structured $n\\\\times n$ matrices over an abstract field,whose Monte Carlo verification complexity is $2$ matrix-times-vector products$+$ $n^{1+o(1)}$ arithmetic operations in the field. For example, if the$n\\\\times n$ input matrix is sparse with $n^{1+o(1)}$ non-zero entries, our rankcertificate can be verified in $n^{1+o(1)}$ field operations. This extends alsoto integer matrices with only an extra $\\\\|A\\\\|^{1+o(1)}$ factor. All ourcertificates are based on interactive verification protocols with theinteraction removed by a Fiat-Shamir identification heuristic. The validity ofour verification procedure is subject to standard computational hardnessassumptions from cryptography.',\n",
              " 'To establish an updated understanding of the U.S. textile and apparel (TAP)industrys competitive position within the global textile environment, tradedata from UN-COMTRADE (1996-2016) was used to calculate the Normalized RevealedComparative Advantage (NRCA) index for 169 TAP categories at the four-digitHarmonized Schedule (HS) code level. Univariate time series usingAutoregressive Integrated Moving Average (ARIMA) models forecast short-termfuture performance of Revealed categories with export advantage. Accompanyingoutlier analysis examined permanent level shifts that might convey importantinformation about policy changes, influential drivers and random events.',\n",
              " 'We study the large space and time scale behavior of a totally asymmetric,nearest-neighbor exclusion process in one dimension with random jump ratesattached to the particles. When slow particles are sufficiently rare the systemhas a phase transition. At low densities there are no equilibriumdistributions, and on the hydrodynamic scale the initial profile is transportedrigidly. We elaborate this situation further by finding the correct order ofthe correction from the hydrodynamic limit, together with distributional boundsaveraged over the disorder. We consider two settings, a macroscopicallyconstant low density profile and the outflow from a large jam.',\n",
              " 'The impedance behavior of pre-oxidized iron in liquid lead-bismuth eutectic(LBE) at 200 oC is studied using electrochemical impedance spectroscopy. Thestructures and resistance of oxide grown on iron oxidized in air at differenttemperatures and durations are compared. The results show that the resistanceof the oxide film increases with increasing oxidizing temperature, due to theformation of a thicker scale and fewer defects. At the same temperature (600oC), increasing the oxidation time can also reduce the defect concentration inthe oxide film and improve the impedance of the oxide scale in LBE.',\n",
              " 'Complex heterogeneous dynamic networks like knowledge graphs are powerfulconstructs that can be used in modeling data provenance from computer systems.From a security perspective, these attributed graphs enable causality analysisand tracing for analyzing a myriad of cyberattacks. However, there is a paucityin systematic development of pipelines that transform system executions andprovenance into usable graph representations for machine learning tasks. Thislack of instrumentation severely inhibits scientific advancement in provenancegraph machine learning by hindering reproducibility and limiting theavailability of data that are critical for techniques like graph neuralnetworks. To fulfill this need, we present Flurry, an end-to-end data pipelinewhich simulates cyberattacks, captures provenance data from these attacks atmultiple system and application layers, converts audit logs from these attacksinto data provenance graphs, and incorporates this data with a framework fortraining deep neural models that supports preconfigured or custom-designedmodels for analysis in real-world resilient systems. We showcase this pipelineby processing data from multiple system attacks and performing anomalydetection via graph classification using current benchmark graphrepresentational learning frameworks. Flurry provides a fast, customizable,extensible, and transparent solution for providing this much needed data tocybersecurity professionals.',\n",
              " \"In this paper, we design and deploy a synchronized multi-vantage point webmeasurement study to explore the comparability of web measurements acrossvantage points (VPs). We describe in reproducible detail the system with whichwe performed synchronized crawls on the Alexa top 5K domains from four distinctnetwork VPs: research university, cloud datacenter, residential network, andTor gateway proxy. Apart from the expected poor results from Tor, we observedno shocking disparities across VPs, but we did find significant impact from theresidential VP's reliability and performance disadvantages. We also foundsubtle but distinct indicators that some third-party content consistentlyavoided crawls from our cloud VP. In summary, we infer that cloud VPs do failto observe some content of interest to security and privacy researchers, whoshould consider augmenting cloud VPs with alternate VPs for cross-validation.Our results also imply that the added visibility provided by residential VPsover university VPs is marginal compared to the infrastructure complexity andnetwork fragility they introduce.\",\n",
              " \"Service Workers (SWs) are a powerful feature at the core of Progressive WebApps, namely web applications that can continue to function when the user'sdevice is offline and that have access to device sensors and capabilitiespreviously accessible only by native applications. During the past few years,researchers have found a number of ways in which SWs may be abused to achievedifferent malicious purposes. For instance, SWs may be abused to build aweb-based botnet, launch DDoS attacks, or perform cryptomining; they may behijacked to create persistent cross-site scripting (XSS) attacks; they may beleveraged in the context of side-channel attacks to compromise users' privacy;or they may be abused for phishing or social engineering attacks using web pushnotifications-based malvertising.  In this paper, we reproduce and analyze known attack vectors related to SWsand explore new abuse paths that have not previously been considered. Wesystematize the attacks into different categories, and then analyze whether,how, and estimate when these attacks have been published and mitigated bydifferent browser vendors. Then, we discuss a number of open SW securityproblems that are currently unmitigated, and propose SW behavior monitoringapproaches and new browser policies that we believe should be implemented bybrowsers to further improve SW security. Furthermore, we implement aproof-of-concept version of several policies in the Chromium code base, andalso measure the behavior of SWs used by highly popular web applications withrespect to these new policies. Our measurements show that it should be feasibleto implement and enforce stricter SW security policies without a significantimpact on most legitimate production SWs.\",\n",
              " 'Content blocking is an important part of a performant, user-serving, privacyrespecting web. Most content blockers build trust labels over URLs. Whileuseful, this approach has well understood shortcomings. Attackers may avoiddetection by changing URLs or domains, bundling unwanted code with benign code,or inlining code in pages. The common flaw in existing approaches is that theyevaluate code based on its delivery mechanism, not its behavior. In this workwe address this problem with a system for generating signatures of theprivacy-and-security relevant behavior of executed JavaScript. Our systemconsiders script behavior during each turn on the JavaScript event loop.Focusing on event loop turns allows us to build signatures that are robustagainst code obfuscation, code bundling, URL modification, and other commonevasions, as well as handle unique aspects of web applications. This work makesthe following contributions to improving content blocking: First, implement anovel system to build per-event-loop-turn signatures of JavaScript code byinstrumenting the Blink and V8 runtimes. Second, we apply these signatures tomeasure filter list evasion, by using EasyList and EasyPrivacy as ground truthand finding other code that behaves identically. We build ~2m signatures ofprivacy-and-security behaviors from 11,212 unique scripts blocked by filterlists, and find 3,589 more unique scripts including the same harmful code,affecting 12.48% of websites measured. Third, we taxonomize common filter listevasion techniques. Finally, we present defenses; filter list additions wherepossible, and a proposed, signature based system in other cases. We share theimplementation of our signature-generation system, the dataset from applyingour system to the Alexa 100K, and 586 AdBlock Plus compatible filter list rulesto block instances of currently blocked code being moved to new URLs.',\n",
              " 'While much current web privacy research focuses on browser fingerprinting,the boring fact is that the majority of current third-party web tracking isconducted using traditional, persistent-state identifiers. One possibleexplanation for the privacy community\\'s focus on fingerprinting is that to datebrowsers have faced a lose-lose dilemma when dealing with third-party statefulidentifiers: block state in third-party frames and break a significant numberof webpages, or allow state in third-party frames and enable pervasivetracking. The alternative, middle-ground solutions that have been deployed alltrade privacy for compatibility, rely on manually curated lists, or depend onthe user to manage state and state-access themselves. This work furthersprivacy on the web by presenting a novel system for managing the lifetime ofthird-party storage, \"page-length storage\". We compare page-length storage toexisting approaches for managing third-party state and find that page-lengthstorage has the privacy protections of the most restrictive current option(i.e., blocking third-party storage) but web-compatibility properties mostlysimilar to the least restrictive option (i.e., allowing all third-partystorage). This work further compares page-length storage to an alternativethird-party storage partitioning scheme and finds that page-length storageprovides superior privacy protections with comparable web-compatibility. Weprovide a dataset of the privacy and compatibility behaviors observed whenapplying the compared third-party storage strategies on a crawl of the Tranco1k and the quantitative metrics used to demonstrate that page-length storagematches or surpasses existing approaches. Finally, we provide an open-sourceimplementation of our page-length storage approach, implemented as patchesagainst Chromium.',\n",
              " 'Spatially resolved analysis of uranium isotopes in small volumes ofactinide-bearing materials is critical for a variety of technical disciplines,including earth and planetary sciences, environmental monitoring,bioremediation, and the nuclear fuel cycle. However, achieving sub-nanometerscale spatial resolution for such isotopic analysis is currently a challenge.By using atom probe tomography, a three dimensional nanoscale characterizationtechnique, we demonstrate unprecidented nanoscale mapping of uranium isotopicenrichment with high sensitivity across various microstructural interfaceswithin small volumes (100 nm3) of depleted and low enriched uranium alloyedwith 10 wt % molybdenum with different nominal enrichments of 0.20 and 19.75%235U respectively. The approach presented here can be applied to studynanoscale variations of isotopic abundances in the broad class ofactinide-bearing materials, providing unique insights into their origin andthermo-mechanical processing routes.',\n",
              " 'A deep neural network was developed for the purpose of predicting thermalconductivity with a case study performed on neutron irradiated nuclear fuel.Traditional thermal conductivity modeling approaches rely on existingtheoretical frameworks that describe known, relevant phenomena that govern themicrostructural evolution processes during neutron irradiation (such asrecrystallization, and pore size, distribution and morphology). Currentempirical modeling approaches, however, do not represent all irradiation testdata well. Here, we develop a machine learning approach to thermal conductivitymodeling that does not require a priori knowledge of a specific materialmicrostructure and system of interest. Our approach allows researchers to probedependency of thermal conductivity on a variety of reactor operating andmaterial conditions. The purpose of building such a model is to allow forimproved predictive capabilities linkingstructure-property-processing-performance relationships in the system ofinterest (here, irradiated nuclear fuel), which could lead to improvedexperimental test planning and characterization. The uranium-molybdenum systemis the fuel system studied in this work, and historic irradiation test data isleveraged for model development. Our model achieved a mean absolute percenterror of approximately 4% for the validation data set (when a leave-one-outcross validation approach was applied). Results indicate our model generalizeswell to never before seen data, and thus use of deep learning methods formaterial property predictions from limited, historic irradiation test data is aviable approach.',\n",
              " 'Micrograph quantification is an essential component of several materialsscience studies. Machine learning methods, in particular convolutional neuralnetworks, have previously demonstrated performance in image recognition tasksacross several disciplines (e.g. materials science, medical imaging, facialrecognition). Here, we apply these well-established methods to develop anapproach to microstructure quantification for kinetic modeling of adiscontinuous precipitation reaction in a case study on the uranium-molybdenumsystem. Prediction of material processing history based on image data(classification), calculation of area fraction of phases present in themicrographs (segmentation), and kinetic modeling from segmentation results wereperformed. Results indicate that convolutional neural networks representmicrostructure image data well, and segmentation using the k-means clusteringalgorithm yields results that agree well with manually annotated images.Classification accuracies of original and segmented images are both 94\\\\% for a5-class classification problem. Kinetic modeling results agree well withpreviously reported data using manual thresholding. The image quantificationand kinetic modeling approach developed and presented here aims to reduceresearcher bias introduced into the characterization process, and allows forleveraging information in limited image data sets.',\n",
              " 'We investigate methods of microstructure representation for the purpose ofpredicting processing condition from microstructure image data. A binary alloy(uranium-molybdenum) that is currently under development as a nuclear fuel wasstudied for the purpose of developing an improved machine learning approach toimage recognition, characterization, and building predictive capabilitieslinking microstructure to processing conditions. Here, we test differentmicrostructure representations and evaluate model performance based on the F1score. A F1 score of 95.1% was achieved for distinguishing between micrographscorresponding to ten different thermo-mechanical material processingconditions. We find that our newly developed microstructure representationdescribes image data well, and the traditional approach of utilizing areafractions of different phases is insufficient for distinguishing betweenmultiple classes using a relatively small, imbalanced original data set of 272images. To explore the applicability of generative methods for supplementingsuch limited data sets, generative adversarial networks were trained togenerate artificial microstructure images. Two different generative networkswere trained and tested to assess performance. Challenges and best practicesassociated with applying machine learning to limited microstructure image datasets is also discussed. Our work has implications for quantitativemicrostructure analysis, and development of microstructure-processingrelationships in limited data sets typical of metallurgical process designstudies.',\n",
              " 'The origin of the highest energy Galactic cosmic rays is still notunderstood, nor is the transition to EeV extragalactic particles. Scientificprogress requires enhancements of existing air-shower arrays, such as: IceCubewith its surface detector IceTop, and the low-energy extensions of both theTelescope Array and the Pierre Auger Observatory.',\n",
              " 'The existing Nuclear Resonance Fluorescence (NRF) setup at the HI{\\\\gamma}Sfacility at the Triangle Universities Nuclear Laboratory at Duke University hasbeen extended in order to perform {\\\\gamma}-{\\\\gamma} coincidence experiments.The new setup combines large volume LaBr3:Ce detectors and high resolution HPGedetectors in a very close geometry to offer high efficiency, high energyresolution as well as high count rate capabilities at the same time. Thecombination of a highly efficient {\\\\gamma}-ray spectroscopy setup with themono-energetic high-intensity photon beam of HI{\\\\gamma}S provides a worldwideunique experimental facility to investigate the {\\\\gamma}-decay pattern ofdipole excitations in atomic nuclei. The performance of the new setup has beenassessed by studying the nucleus \\\\sulfur at 8.125 MeV beam energy. The{\\\\gamma}-decay branching ratio from the $1^+$ level at 8125.4 keV to the firstexcited $2^+$ state was determined to 15.7(3)%.',\n",
              " 'In this paper we present a lock-free version of Hopscotch Hashing. HopscotchHashing is an open addressing algorithm originally proposed by Herlihy, Shavit,and Tzafrir, which is known for fast performance and excellent cache locality.The algorithm allows users of the table to skip or jump over irrelevantentries, allowing quick search, insertion, and removal of entries. Unliketraditional linear probing, Hopscotch Hashing is capable of operating under ahigh load factor, as probe counts remain small. Our lock-free version improveson both speed, cache locality, and progress guarantees of the original, being achimera of two concurrent hash tables. We compare our data structure to variousother lock-free and blocking hashing algorithms and show that its performanceis in many cases superior to existing strategies. The proposed lock-freeversion overcomes some of the drawbacks associated with the original blockingversion, leading to a substantial boost in scalability while maintainingattractive features like physical deletion or probe-chain compression.',\n",
              " 'In this paper we examine the issues involved in adding concurrency to theRobin Hood hash table algorithm. We present a non-blocking obstruction-freeK-CAS Robin Hood algorithm which requires only a single word compare-and-swapprimitive, thus making it highly portable. The implementation maintains theattractive properties of the original Robin Hood structure, such as a lowexpected probe length, capability to operate effectively under a high loadfactor and good cache locality, all of which are essential for high performanceon modern computer architectures. We compare our data-structures to variousother lock-free and concurrent algorithms, as well as a simple hardwaretransactional variant, and show that our implementation performs better acrossa number of contexts.',\n",
              " 'Formal transformations somehow resembling the usual derivative aresurprisingly common in computer science, with two notable examples beingderivatives of regular expressions and derivatives of types. A newcomer to thislist is the incremental $\\\\lambda$-calculus, or ILC, a \"theory of changes\" thatdeploys a formal apparatus allowing the automatic generation of efficientupdate functions which perform incremental computation. The ILC is not onlydefined, but given a formal machine-understandable definition---accompanied bymechanically verifiable proofs of various properties, including in particularcorrectness of various sorts. Here, we show how the ILC can be mutated intopropagating tangents, thus serving as a model of Forward Accumulation ModeAutomatic Differentiation. This mutation is done in several steps. These stepscan also be applied to the proofs, resulting in machine-checked proofs of thecorrectness of this model of forward AD.',\n",
              " 'PowNet is a free modelling tool for simulating the Unit Commitment / EconomicDispatch of large-scale power systems. PowNet is specifically conceived forapplications in the water-energy nexus domain, which investigate the impact ofwater availability on electricity supply. To this purpose, PowNet is equippedwith features that guarantee accuracy, reusability, and computationalefficiency over large spatial and temporal domains. Specifically, the model (i)accounts for the techno-economic constraints of both generating units andtransmission networks, (ii) can be easily coupled with models that estimate thestatus of generating units as a function of the climatic conditions, and (iii)explicitly includes import/export nodes, which are often found in cross-bordersystems. PowNet is implemented in Python and runs with the help of any standardoptimization solver (e.g., Gurobi, CPLEX). Its functionality is demonstrated onthe Cambodian power system.',\n",
              " 'A social interaction is a social exchange between two or moreindividuals,where individuals modify and adjust their behaviors in response totheir interaction partners. Our social interactions are one of most fundamentalaspects of our lives and can profoundly affect our mood, both positively andnegatively. With growing interest in virtual reality and avatar-mediatedinteractions,it is desirable to make these interactions natural and human liketo promote positive effect in the interactions and applications such asintelligent tutoring systems, automated interview systems and e-learning. Inthis paper, we propose a method to generate facial behaviors for an agent.These behaviors include facial expressions and head pose and they are generatedconsidering the users affective state. Our models learn semantically meaningfulrepresentations of the face and generate appropriate and temporally smoothfacial behaviors in dyadic interactions.',\n",
              " \"By shutting down frequency carriers, the power consumed by a base station canbe considerably reduced. However, this typically comes with traffic performancedegradation, as the congestion on the remaining active carriers is increased.We leverage a hysteresis carrier shutdown policy that attempts to keep theaverage traffic load on each sector within a certain min/max threshold pair. Wepropose a closed-loop Bayesian method optimizing such thresholds on a sectorbasis and aiming at minimizing the power consumed by the power amplifiers whilemaintaining the probability that KPI's are acceptable above a certain value. Wetested our approach in a live customer 4G network. The power consumption at thebase station was reduced by 11% and the selected KPI's met the predefinedtargets.\",\n",
              " 'Network pruning is a widely-used compression technique that is able tosignificantly scale down overparameterized models with minimal loss ofaccuracy. This paper shows that pruning may create or exacerbate disparateimpacts. The paper sheds light on the factors to cause such disparities,suggesting differences in gradient norms and distance to decision boundaryacross groups to be responsible for this critical issue. It analyzes thesefactors in detail, providing both theoretical and empirical support, andproposes a simple, yet effective, solution that mitigates the disparate impactscaused by pruning.',\n",
              " \"Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses manychallenges. One main reason is that GPS has limited precision in indoorenvironments. The additional fact that MAVs are not able to carry heavy weightor power consuming sensors, such as range finders, makes indoor autonomousnavigation a challenging task. In this paper, we propose a practical system inwhich a quadcopter autonomously navigates indoors and finds a specific target,i.e., a book bag, by using a single camera. A deep learning model,Convolutional Neural Network (ConvNet), is used to learn a controller strategythat mimics an expert pilot's choice of action. We show our system'sperformance through real-time experiments in diverse indoor locations. Tounderstand more about our trained network, we use several visualizationtechniques.\",\n",
              " 'We construct the categories of standard vector bundles over schemes anddefine direct sum and tensor product. These categories are equivalent to theusual categories of vector bundles with additional properties. The tensorproduct is strictly associative, strictly commutative with line bundles, andstrictly functorial on base change.',\n",
              " 'This paper proposes a branched residual network for image classification. Itis known that high-level features of deep neural network are morerepresentative than lower-level features. By sharing the low-level features,the network can allocate more memory to high-level features. The upper layersof our proposed network are branched, so that it mimics the ensemble learning.By mimicking ensemble learning with single network, we have achieved betterperformance on ImageNet classification task.',\n",
              " \"In electron microscopy, charging of non-conductive biological samples byfocused electron beams hinders their high-resolution imaging. Gold or platinumcoatings have been commonly used to prevent such sample charging, but itdisables further quantitative and qualitative chemical analyses by energydispersive spectroscopy (EDS). Here we report that graphene-coating onbiological samples enables non-destructive high-resolution imaging by scanningelectron microscopy (SEM) as well as chemical analysis by EDS, utilizinggraphene's transparency to electron beams, high conductivity, outstandingmechanical strength, and flexibility. We believe that the graphene-coatedimaging and analysis would provide us a new opportunity to explore variousbiological phenomena unseen before due to the limitation in sample preparationand image resolution, which will broaden our understanding on the lifemechanism of various living organisms.\",\n",
              " 'Batch Whitening is a technique that accelerates and stabilizes training bytransforming input features to have a zero mean (Centering) and a unit variance(Scaling), and by removing linear correlation between channels (Decorrelation).In commonly used structures, which are empirically optimized with BatchNormalization, the normalization layer appears between convolution andactivation function. Following Batch Whitening studies have employed the samestructure without further analysis; even Batch Whitening was analyzed on thepremise that the input of a linear layer is whitened. To bridge the gap, wepropose a new Convolutional Unit that is in line with the theory, and ourmethod generally improves the performance of Batch Whitening. Moreover, we showthe inefficacy of the original Convolutional Unit by investigating rank andcorrelation of features. As our method is employable off-the-shelf whiteningmodules, we use Iterative Normalization (IterNorm), the state-of-the-artwhitening module, and obtain significantly improved performance on five imageclassification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, andImageNet. Notably, we verify that our method improves stability and performanceof whitening when using large learning rate, group size, and iteration number.',\n",
              " 'Non-local (NL) block is a popular module that demonstrates the capability tomodel global contexts. However, NL block generally has heavy computation andmemory costs, so it is impractical to apply the block to high-resolutionfeature maps. In this paper, to investigate the efficacy of NL block, weempirically analyze if the magnitude and direction of input feature vectorsproperly affect the attention between vectors. The results show the inefficacyof softmax operation which is generally used to normalize the attention map ofthe NL block. Attention maps normalized with softmax operation highly rely uponmagnitude of key vectors, and performance is degenerated if the magnitudeinformation is removed. By replacing softmax operation with the scaling factor,we demonstrate improved performance on CIFAR-10, CIFAR-100, and Tiny-ImageNet.In Addition, our method shows robustness to embedding channel reduction andembedding weight initialization. Notably, our method makes multi-head attentionemployable without additional computational cost.',\n",
              " 'While variable selection is essential to optimize the learning complexity byprioritizing features, automating the selection process is preferred since itrequires laborious efforts with intensive analysis otherwise. However, it isnot an easy task to enable the automation due to several reasons. First,selection techniques often need a condition to terminate the reduction process,for example, by using a threshold or the number of features to stop, andsearching an adequate stopping condition is highly challenging. Second, it isuncertain that the reduced variable set would work well; our preliminaryexperimental result shows that well-known selection techniques producedifferent sets of variables as a result of reduction (even with the sametermination condition), and it is hard to estimate which of them would work thebest in future testing. In this paper, we demonstrate the potential power ofour approach to the automation of selection process that incorporateswell-known selection methods identifying important variables. Our experimentalresults with two public network traffic data (UNSW-NB15 and IDS2017) show thatour proposed method identifies a small number of core variables, with which itis possible to approximate the performance to the one with the entirevariables.',\n",
              " 'Closed-form solutions for the modified exterior Eshelby tensor, strainconcentration tensor, and effective moduli of particle-reinforced compositesare presented when the interfacial damage is modeled as a linear spring layerof vanishing thickness; the solutions are validated against finite elementanalyses. Based on the closed-form solutions, the applicability of theinterface spring model is tested by calculating those quantities using finiteelement analysis (FEA) augmented with a matrix-inhomogeneity non-overlappingcondition. The results indicate that the interface spring model reasonablycaptures the characteristics of the stress distribution and effective moduli ofcomposites, despite its well-known problem of unphysical overlapping betweenthe matrix and inhomogeneity.',\n",
              " 'We consider a two-player game of war of attrition under complete information.It is well-known that this class of games admits equilibria in pure, as well asmixed strategies, and much of the literature has focused on the latter. We showthat if the players\\' payoffs whilst in \"war\" vary stochastically and their exitpayoffs are heterogeneous, then the game admits Markov Perfect equilibria inpure strategies only. This is true irrespective of the degree of randomness andheterogeneity, thus highlighting the fragility of mixed-strategy equilibria toa natural perturbation of the canonical model. In contrast, when the players\\'flow payoffs are deterministic or their exit payoffs are homogeneous, the gameadmits equilibria in pure and mixed strategies.',\n",
              " \"In the game of investment in the common good, the free rider problem candelay the stakeholders' actions in the form of a mixed strategy equilibrium.However, it has been recently shown that the mixed strategy equilibria of thestochastic war of attrition are destabilized by even the slightest degree ofasymmetry between the players. Such extreme instability is contrary to thewidely accepted notion that a mixed strategy equilibrium is the hallmark of thewar of attrition. Motivated by this quandary, we search for a mixed strategyequilibrium in a stochastic game of investment in the common good. Our resultsshow that, despite asymmetry, a mixed strategy equilibrium exists if the modeltakes into account the repeated investment opportunities. The mixed strategyequilibrium disappears only if the asymmetry is sufficiently high. Since themixed strategy equilibrium is less efficient than pure strategy equilibria, itbehooves policymakers to prevent it by promoting a sufficiently high degree ofasymmetry between the stakeholders through, for example, asymmetric subsidy.\",\n",
              " 'Control barrier function (CBF) has recently started to serve as a basis todevelop approaches for enforcing safety requirements in control systems.However, constructing such function for a general system is a non-trivial task.This paper proposes an iterative, optimization-based framework to obtain a CBFfrom a given user-specified set for a general control affine system. Withoutlosing generality, we parameterize the CBF as a set of linear functions ofstates. By taking samples from the given user-specified set, we reformulate theproblem of learning a CBF into an optimization problem that solves for linearfunction coefficients. The resulting linear functions construct the CBF andyield a safe set which has forward invariance property. In addition, theproposed framework explicitly addresses control input constraints during theconstruction of CBFs. Effectiveness of the proposed method is demonstrated bylearning a CBF for an nonlinear Moore Greitzer jet engine, where the systemtrajectory is prevented from entering unsafe set.',\n",
              " 'We study the \\\\textsc{Max Partial $H$-Coloring} problem: given a graph $G$,find the largest induced subgraph of $G$ that admits a homomorphism into $H$,where $H$ is a fixed pattern graph without loops. Note that when $H$ is acomplete graph on $k$ vertices, the problem reduces to finding the largestinduced $k$-colorable subgraph, which for $k=2$ is equivalent (bycomplementation) to \\\\textsc{Odd Cycle Transversal}.  We prove that for every fixed pattern graph $H$ without loops, \\\\textsc{MaxPartial $H$-Coloring} can be solved:  $\\\\bullet$ in $\\\\{P_5,F\\\\}$-free graphs in polynomial time, whenever $F$ is athreshold graph;  $\\\\bullet$ in $\\\\{P_5,\\\\textrm{bull}\\\\}$-free graphs in polynomial time;  $\\\\bullet$ in $P_5$-free graphs in time $n^{\\\\mathcal{O}(\\\\omega(G))}$;  $\\\\bullet$ in $\\\\{P_6,\\\\textrm{1-subdivided claw}\\\\}$-free graphs in time$n^{\\\\mathcal{O}(\\\\omega(G)^3)}$.  Here, $n$ is the number of vertices of the input graph $G$ and $\\\\omega(G)$ isthe maximum size of a clique in~$G$. Furthermore, combining the mentionedalgorithms for $P_5$-free and for $\\\\{P_6,\\\\textrm{1-subdivided claw}\\\\}$-freegraphs with a simple branching procedure, we obtain subexponential-timealgorithms for \\\\textsc{Max Partial $H$-Coloring} in these classes of graphs.Finally, we show that even a restricted variant of \\\\textsc{Max Partial$H$-Coloring} is $\\\\mathsf{NP}$-hard in the considered subclasses of $P_5$-freegraphs, if we allow loops on $H$.',\n",
              " 'In this paper, we argue that type inferencing incorrectly implementsappropriateness specifications for typed feature structures, promote acombination of type resolution and unfilling as a correct and efficientalternative, and consider the expressive limits of this alternative approach.Throughout, we use feature cooccurence restrictions as illustration andlinguistic motivation.',\n",
              " 'A description is an entity that can be interpreted as true or false of anobject, and using feature structures as descriptions accrues severalcomputational benefits. In this paper, I create an explicit interpretation of atyped feature structure used as a description, define the notion of asatisfiable feature structure, and create a simple and effective algorithm todecide if a feature structure is satisfiable.',\n",
              " 'We construct new ancient compact solutions to the Yamabe flow. Our solutionsare rotationally symmetric and converge, as $t \\\\to -\\\\infty$, to twoself-similar complete non-compact solutions to the Yamabe flow moving inopposite directions. They are type I ancient solutions.',\n",
              " 'We construct new ancient compact solutions to the Yamabe flow. Our solutionsare rotationally symmetric and converge, as $t \\\\to -\\\\infty$, to twoself-similar complete non-compact solutions to the Yamabe flow moving inopposite directions. They are type I ancient solutions.',\n",
              " 'Quantum dots in silicon are promising candidates for implementation ofsolid-state quantum information processing. It is important to understand theeffects of the multiple conduction band valleys of silicon on the properties ofthese devices. Here we introduce a novel, systematic effective mass theory ofvalley-orbit coupling in disordered silicon systems. This theory revealsvalley-orbit hybridization effects that are detrimental for storing quantuminformation in the valley degree of freedom, including non-vanishing dipolematrix elements between valley states and altered intervalley tunneling.',\n",
              " 'This work concerns with the existence and detailed asymptotic analysis ofType II singularities for solutions to complete non-compact conformally flatYamabe flow with cylindrical behavior at infinity. We provide the specificblow-up rate of the maximum curvature and show that the solution converges,after blowing-up around the curvature maximum points, to a rotationallysymmetric steady soliton. It is the first time that the steady soliton is shownto be a finite time singularity model of the Yamabe flow.',\n",
              " \"Experiments in particle physics have hitherto failed to produce anysignificant evidence for the many explicit models of physics beyond theStandard Model (BSM) that had been proposed over the past decades. As a result,physicists have increasingly turned to model-independent strategies as tools insearching for a wide range of possible BSM effects. In this paper, we describethe Standard Model Effective Field Theory (SM-EFT) and analyse it in thecontext of the philosophical discussions about models, theories, and(bottom-up) effective field theories. We find that while the SM-EFT is aquantum field theory, assisting experimentalists in searching for deviationsfrom the SM, in its general form it lacks some of the characteristic featuresof models. Those features only come into play if put in by hand or prompted byempirical evidence for deviations. Employing different philosophical approachesto models, we argue that the case study suggests not to take a view on modelsthat is overly permissive because it blurs the lines between the differentstages of the SM-EFT research strategies and glosses over particle physicists'motivations for undertaking this bottom-up approach in the first place. Lookingat EFTs from the perspective of modelling does not require taking a stance onsome specific brand of realism or taking sides in the debate between reductionand emergence into which EFTs have recently been embedded.\",\n",
              " 'For the observation of Bose-Einstein condensation, excitons in cuprous oxideare regarded as promising candidates due to their large binding energy and longlifetime. High particle densities may be achieved by entrapment in a stressinduced potential. We consider a multi-component gas of interacting para- andorthoexcitons in cuprous oxide confined in a three-dimensional potential trap.Based on the Hartree-Fock-Bogoliubov theory, we calculate density profiles aswell as decay luminescence spectra which exhibit signatures of the separationof the Bose-condensed phases.',\n",
              " 'The energy dissipation and heat flows associated with the particle current ina system with a molecular junction are considered. In this connection, wedetermine the effective temperature of the molecular oscillator that iscompatible with the existence of a steady state. The calculations based on theKadanov-Baym nonequilibrium Green function formalism are carried out supposinga strong coupling of the dot electrons with the molecular vibrations.Accordingly, the representation given by the Lang-Firsov polaron transformationis used and the dependence of results on the electron-phonon interactionstrength is investigated.',\n",
              " 'We consider practical aspects of reconstructing planar curves with prescribedEuclidean or affine curvatures. These curvatures are invariant under thespecial Euclidean group and the equi-affine groups, respectively, and play animportant role in computer vision and shape analysis. We discuss and implementalgorithms for such reconstruction, and give estimates on how closereconstructed curves are relative to the closeness of their curvatures inappropriate metrics. Several illustrative examples are provided.',\n",
              " 'This thesis is devoted to algorithmic aspects of the implementation ofCartan\\'s moving frame method to the problem of the equivalence of submanifoldsunder a Lie group action. We adopt a general definition of a moving frame as anequivariant map from the space of submanifolds to the group itself andintroduce two algorithms, which simplify the construction of such maps. Thefirst algorithm is applicable when the group factors as a product of twosubgroups $G=BA$, allowing us to use moving frames and differential invariantsfor the groups $A$ and $B$ in order to construct a moving frame anddifferential invariants for $G$. This approach produces the relations among theinvariants of $G$ and its subgroups. We use the groups of the projective, theaffine and the Euclidean transformations on the plane to illustrate thealgorithm. We also introduce a recursive algorithm, allowing, provided thegroup action satisfies certain conditions, to construct differential invariantsorder by order, at each step normalizing more and more of the group parameters,at the end obtaining a moving frame for the entire group. The development ofthis algorithm has been motivated by the applications of the moving framemethod to the problems of the equivalence and symmetry of polynomials underlinear changes of variables. In the complex or real case these problems can bereduced and, in theory, completely solved as the problem of the equivalence ofsubmanifolds. Its solution however involves algorithms based on the Gr\\\\\"obnerbasis computations, which due to their complexity, are not always feasible.Nevertheless, some interesting new results were obtained, such as aclassification of ternary cubics and their groups of symmetries, and thenecessary and sufficient conditions for a homogeneous polynomial in threevariables to be equivalent to $x^n+y^n+z^n.$',\n",
              " 'In this paper, we propose a novel lower dimensional representation of a shapesequence. The proposed dimension reduction is invertible and computationallymore efficient in comparison to other related works. Theoretically, thedifferential geometry tools such as moving frame and parallel transportationare successfully adapted into the dimension reduction problem of highdimensional curves. Intuitively, instead of searching for a global flatsubspace for curve embedding, we deployed a sequence of local flat subspacesadaptive to the geometry of both of the curve and the manifold it lies on. Inpractice, the experimental results of the dimension reduction andreconstruction algorithms well illustrate the advantages of the proposedtheoretical innovation.',\n",
              " 'We present a distributed algorithm to compute the first homology of asimplicial complex. Such algorithms are very useful in topological analysis ofsensor networks, such as its coverage properties. We employ spanning trees tocompute a basis for algebraic 1-cycles, and then use harmonics to efficientlyidentify the contractible and homologous cycles. The computational complexityof the algorithm is $O(|P|^\\\\omega)$, where $|P|$ is much smaller than thenumber of edges, and $\\\\omega$ is the complexity order of matrix multiplication.For geometric graphs, we show using simulations that $|P|$ is very close to thefirst Betti number.',\n",
              " 'Successful applications of sparse models in computer vision and machinelearning imply that in many real-world applications, high dimensional data isdistributed in a union of low dimensional subspaces. Nevertheless, theunderlying structure may be affected by sparse errors and/or outliers. In thispaper, we propose a bi-sparse model as a framework to analyze this problem andprovide a novel algorithm to recover the union of subspaces in presence ofsparse corruptions. We further show the effectiveness of our method byexperiments on both synthetic data and real-world vision data.',\n",
              " 'In this paper, we present a new approach for analyzing gene expression datathat builds on topological characteristics of time series. Our goal is toidentify cell cycle regulated genes in micro array dataset. We construct apoint cloud out of time series using delay coordinate embeddings. Persistenthomology is utilized to analyse the topology of the point cloud for detectionof periodicity. This novel technique is accurate and robust to noise, missingdata points and varying sampling intervals. Our experiments using YeastSaccharomyces cerevisiae dataset substantiate the capabilities of the proposedmethod.',\n",
              " 'We propose a geometric model-free causality measurebased on multivariatedelay embedding that can efficiently detect linear and nonlinear causalinteractions between time series with no prior information. We then exploit theproposed causal interaction measure in real MEG data analysis. The results areused to construct effective connectivity maps of brain activity to decodedifferent categories of visual stimuli. Moreover, we discovered that theMEG-based effective connectivity maps as a response to structured imagesexhibit more geometric patterns, as disclosed by analyzing the evolution oftoplogical structures of the underlying networks using persistent homology.Extensive simulation and experimental result have been carried out tosubstantiate the capabilities of the proposed approach.',\n",
              " 'We introduce the idea of Data Readiness Level (DRL) to measure the relativerichness of data to answer specific questions often encountered by datascientists. We first approach the problem in its full generality explaining itsdesired mathematical properties and applications and then we propose and studytwo DRL metrics. Specifically, we define DRL as a function of at least fourproperties of data: Noisiness, Believability, Relevance, and Coherence. Theinformation-theoretic based metrics, Cosine Similarity and Document Disparity,are proposed as indicators of Relevance and Coherence for a piece of data. Theproposed metrics are validated through a text-based experiment using Twitterdata.',\n",
              " 'We propose a computationally efficient and high-performance classificationalgorithm by incorporating class structural information in analysis dictionarylearning. To achieve more consistent classification, we associate a classcharacteristic structure of independent subspaces and impose it on theclassification error constrained analysis dictionary learning. Experimentsdemonstrate that our method achieves a comparable or better performance thanthe state-of-the-art algorithms in a variety of visual classification tasks. Inaddition, our method greatly reduces the training and testing computationalcomplexity.',\n",
              " 'Discriminative Dictionary Learning (DL) methods have been widely advocatedfor image classification problems. To further sharpen their discriminativecapabilities, most state-of-the-art DL methods have additional constraintsincluded in the learning stages. These various constraints, however, lead toadditional computational complexity. We hence propose an efficientDiscriminative Convolutional Analysis Dictionary Learning (DCADL) method, as alower cost Discriminative DL framework, to both characterize the imagestructures and refine the interclass structure representations. The proposedDCADL jointly learns a convolutional analysis dictionary and a universalclassifier, while greatly reducing the time complexity in both training andtesting phases, and achieving a competitive accuracy, thus demonstrating greatperformance in many experiments with standard databases.',\n",
              " 'Parametric approaches to Learning, such as deep learning (DL), are highlypopular in nonlinear regression, in spite of their extremely difficult trainingwith their increasing complexity (e.g. number of layers in DL). In this paper,we present an alternative semi-parametric framework which foregoes theordinarily required feedback, by introducing the novel idea of geometricregularization. We show that certain deep learning techniques such as residualnetwork (ResNet) architecture are closely related to our approach. Hence, ourtechnique can be used to analyze these types of deep learning. Moreover, wepresent preliminary results which confirm that our approach can be easilytrained to obtain complex structures.',\n",
              " 'This paper presents a technique that combines the occurrence of certainevents, as observed by different sensors, in order to detect and classifyobjects. This technique explores the extent of dependence between featuresbeing observed by the sensors, and generates more informed probabilitydistributions over the events. Provided some additional information about thefeatures of the object, this fusion technique can outperform other existingdecision level fusion approaches that may not take into account therelationship between different features.',\n",
              " 'In this work, we seek to exploit the deep structure of multi-modal data torobustly exploit the group subspace distribution of the information using theConvolutional Neural Network (CNN) formalism. Upon unfolding the set ofsubspaces constituting each data modality, and learning their correspondingencoders, an optimized integration of the generated inherent information iscarried out to yield a characterization of various classes. Referred to as deepMultimodal Robust Group Subspace Clustering (DRoGSuRe), this approach iscompared against the independently developed state-of-the-art approach namedDeep Multimodal Subspace Clustering (DMSC). Experiments on different multimodaldatasets show that our approach is competitive and more robust in the presenceof noise.',\n",
              " 'In this paper, we address the problem of unsupervised Domain Adaptation. Theneed for such an adaptation arises when the distribution of the target datadiffers from that which is used to develop the model and the ground truthinformation of the target data is unknown. We propose an algorithm that usesoptimal transport theory with a verifiably efficient and implementable solutionto learn the best latent feature representation. This is achieved by minimizingthe cost of transporting the samples from the target domain to the distributionof the source domain.',\n",
              " 'For a given point set $S$ in a plane, we develop a distributed algorithm tocompute the $\\\\alpha-$shape of $S$. $\\\\alpha-$shapes are well known geometricobjects which generalize the idea of a convex hull, and provide a gooddefinition for the shape of $S$. We assume that the distances between pairs ofpoints which are closer than a certain distance $r>0$ are provided, and we showconstructively that this information is sufficient to compute the alpha shapesfor a range of parameters, where the range depends on $r$.  Such distributed algorithms are very useful in domains such as sensornetworks, where each point represents a sensing node, the location of which isnot necessarily known.  We also introduce a new geometric object called the Delaunay-\\\\v{C}ech shape,which is geometrically more appropriate than an $\\\\alpha-$shape for some cases,and show that it is topologically equivalent to $\\\\alpha-$shapes.',\n",
              " 'The objective of this study is to detect and quantify the periodic behaviorof the signals using topological methods. We propose to use delay-coordinateembeddings as a tool to measure the periodicity of signals. Moreover, we usepersistent homology for analyzing the structure of point clouds ofdelay-coordinate embeddings. A method for finding the appropriate value ofdelay is proposed based on the autocorrelation function of the signals. Weapply this topological approach to wheeze signals by introducing a model basedon their harmonic characteristics. Wheeze detection is performed using thefirst Betti numbers of a few number of landmarks chosen from embeddings of thesignals.',\n",
              " 'In this paper, we investigate the problem of how beliefs diffuse amongmembers of social networks. We propose an information flow model (IFM) ofbelief that captures how interactions among members affect the diffusion andeventual convergence of a belief. The IFM model includes a generalized MarkovGraph (GMG) model as a social network model, which reveals that the diffusionof beliefs depends heavily on two characteristics of the social networkcharacteristics, namely degree centralities and clustering coefficients. Weapply the IFM to both converged belief estimation and belief control strategyoptimization. The model is compared with an IFM including the Barabasi-Albertmodel, and is evaluated via experiments with published real social networkdata.',\n",
              " 'Persistent homology and zigzag persistent homology are techniques which trackthe homology over a sequence of spaces, outputting a set of intervalscorresponding to birth and death times of homological features in the sequence.This paper presents a method for choosing a homology class to correspond toeach of the intervals at each time point. For each homology class a specificrepresentative cycle is stored, with the choice of homology class andrepresentative cycle being both geometrically relevant and compatible with thebirth-death interval decomposition. After describing the method in detail andproving its correctness, we illustrate the utility of the method by applying itto the study of coverage holes in time-varying sensor networks.',\n",
              " \"We present a novel set of methods for analyzing coverage properties indynamic sensor networks. The dynamic sensor network under consideration isstudied through a series of snapshots, and is represented by a sequence ofsimplicial complexes, built from the communication graph at each time point. Amethod from computational topology called zigzag persistent homology takes thissequence of simplicial complexes as input, and returns a `barcode' containingthe birth and death times of homological features in this sequence. We deriveuseful statistics from this output for analyzing time-varying coverageproperties. Further, we propose a method which returns specific representativecycles for these homological features, at each point along the birth-deathintervals. These representative cycles are then used to track coverage holes inthe network, and obtain size estimates for individual holes at each time point.A weighted barcode, incorporating the size information, is then used as avisual and quantitative descriptor of the dynamic network coverage.\",\n",
              " 'This study relates the local property of node dominance to local and globalproperties of a network. Iterative removal of dominated nodes yields adistributed algorithm for computing a core-periphery decomposition of a socialnetwork, where nodes in the network core are seen to be essential in terms ofnetwork flow and global structure. Additionally, the connected components inthe periphery give information about the community structure of the network,aiding in community detection. A number of explicit results are derived,relating the core and periphery to network flow, community structure and globalnetwork structure, which are corroborated by observational results. The methodis illustrated using a real world network (DBLP co-authorship network), withground-truth communities.',\n",
              " 'In this paper, we are interested in modeling the diffusion of information ina multilayer network using thermodynamic diffusion approach. State of eachagent is viewed as a topic mixture represented by a distribution over multipletopics. We have observed and learned diffusion-related thermodynamical patternsin the training data set, and we have used the estimated diffusion structure topredict the future states of the agents. A priori knowledge of a fraction ofthe state of all agents changes the problem to be a Kalman predictor problemthat refines the predicted system state using the error in estimation of theagents. A real world Twitter data set is then used to evaluate and validate ourinformation diffusion model.',\n",
              " 'A discriminative structured analysis dictionary is proposed for theclassification task. A structure of the union of subspaces (UoS) is integratedinto the conventional analysis dictionary learning to enhance the capability ofdiscrimination. A simple classifier is also simultaneously included into theformulated functional to ensure a more complete consistent classification. Thesolution of the algorithm is efficiently obtained by the linearized alternatingdirection method of multipliers. Moreover, a distributed structured analysisdictionary learning is also presented to address large scale datasets. It cangroup-(class-) independently train the structured analysis dictionaries bydifferent machines/cores/threads, and therefore avoid a high computationalcost. A consensus structured analysis dictionary and a global classifier arejointly learned in the distributed approach to safeguard the discriminativepower and the efficiency of classification. Experiments demonstrate that ourmethod achieves a comparable or better performance than the state-of-the-artalgorithms in a variety of visual classification tasks. In addition, thetraining and testing computational complexity are also greatly reduced.',\n",
              " 'In this paper, we propose to use a Conditional Generative Adversarial Network(CGAN) for distilling (i.e. transferring) knowledge from sensor data andenhancing low-resolution target detection. In unconstrained surveillancesettings, sensor measurements are often noisy, degraded, corrupted, and evenmissing/absent, thereby presenting a significant problem for multi-modalfusion. We therefore specifically tackle the problem of a missing modality inour attempt to propose an algorithm based on CGANs to generate representativeinformation from the missing modalities when given some other availablemodalities. Despite modality gaps, we show that one can distill knowledge fromone set of modalities to another. Moreover, we demonstrate that it achievesbetter performance than traditional approaches and recent teacher-studentmodels.',\n",
              " 'We develop a novel theoretical framework for understating OT schemesrespecting a class structure. For this purpose, we propose a convex OT programwith a sum-of-norms regularization term, which provably recovers the underlyingclass structure under geometric assumptions. Furthermore, we derive anaccelerated proximal algorithm with a closed-form projection and proximaloperator scheme, thereby affording a more scalable algorithm for computingoptimal transport plans. We provide a novel argument for the uniqueness of theoptimum even in the absence of strong convexity. Our experiments show that thenew regularizer not only results in a better preservation of the classstructure in the data but also yields additional robustness to the datageometry, compared to previous regularizers.',\n",
              " 'Zero-shot learning (ZSL) which aims to recognize unseen object classes byonly training on seen object classes, has increasingly been of great interestin Machine Learning, and has registered with some successes. Most existing ZSLmethods typically learn a projection map between the visual feature space andthe semantic space and mainly suffer which is prone to a projection domainshift primarily due to a large domain gap between seen and unseen classes. Inthis paper, we propose a novel inductive ZSL model based on projecting bothvisual and semantic features into a common distinct latent space withclass-specific knowledge, and on reconstructing both visual and semanticfeatures by such a distinct common space to narrow the domain shift gap. Weshow that all these constraints on the latent space, class-specific knowledge,reconstruction of features and their combinations enhance the robustnessagainst the projection domain shift problem, and improve the generalizationability to unseen object classes. Comprehensive experiments on four benchmarkdatasets demonstrate that our proposed method is superior to state-of-the-artalgorithms.',\n",
              " 'We present a novel adversarial framework for training deep belief networks(DBNs), which includes replacing the generator network in the methodology ofgenerative adversarial networks (GANs) with a DBN and developing a highlyparallelizable numerical algorithm for training the resulting architecture in astochastic manner. Unlike the existing techniques, this framework can beapplied to the most general form of DBNs with no requirement for backpropagation. As such, it lays a new foundation for developing DBNs on a parwith GANs with various regularization units, such as pooling and normalization.Foregoing back-propagation, our framework also exhibits superior scalability ascompared to other DBN and GAN learning techniques. We present a number ofnumerical experiments in computer vision as well as neurosciences to illustratethe main advantages of our approach.',\n",
              " \"The purpose of this paper is to infer a global (collective) model oftime-varying responses of a set of nodes as a dynamic graph, where theindividual time series are respectively observed at each of the nodes. Themotivation of this work lies in the search for a connectome model whichproperly captures brain functionality upon observing activities in differentregions of the brain and possibly of individual neurons. We formulate theproblem as a quadratic objective functional of observed node signals over shorttime intervals, subjected to the proper regularization reflecting the graphsmoothness and other dynamics involving the underlying graph's Laplacian, aswell as the time evolution smoothness of the underlying graph. The resultingjoint optimization is solved by a continuous relaxation and an introduced novelgradient-projection scheme. We apply our algorithm to a real-world datasetcomprising recorded activities of individual brain cells. The resulting modelis shown to not only be viable but also efficiently computable.\",\n",
              " 'Nonlinear transmission of 80 and 140 femtosecond pulsed light with $0.79 \\\\mum$ wavelength through single walled carbon nanotubes suspended in watercontaining sodium dodecyl sulphate is studied. Pulse-width independentsaturation absorption and negative cubic nonlinearity are observed,respectively, in open and closed aperture Z-scan experiments. The theoreticalexpressions derived to analyze the z-dependent transmission in the saturablelimit require two photon absorption coefficient $\\\\beta_0\\\\sim$ $1.4 cm/MW$ and anonlinear index $\\\\gamma \\\\sim -5.5 \\\\times10^{-11} cm^2/W$ to fit the data.',\n",
              " 'The integration of complex oxides on silicon presents opportunities to extendand enhance silicon technology with novel electronic, magnetic, and photonicproperties. Among these materials, barium titanate (BaTiO3) is a particularlystrong ferroelectric perovskite oxide with attractive dielectric andelectro-optic properties. Here we demonstrate nanophotonic circuitsincorporating ferroelectric BaTiO3 thin films on the ubiquitoussilicon-on-insulator (SOI) platform. We grow epitaxial, single-crystallineBaTiO3 directly on SOI and engineer integrated waveguide structures thatsimultaneously confine light and an RF electric field in the BaTiO3 layer.Using on-chip photonic interferometers, we extract a large effective Pockelscoefficient of 213 plus minus 49 pm/V, a value more than six times larger thanfound in commercial optical modulators based on lithium niobate. Themonolithically integrated BaTiO3 optical modulators show modulation bandwidthin the gigahertz regime, which is promising for broadband applications.',\n",
              " 'We determine the surface reconstruction of SrTiO3 used to achievesuperconducting FeSe films in experiments, which is different from the 1x1 TiO2terminated SrTiO3 assumed by most previous theoretical studies. In particular,we identify the existence of a double TiO2 layer at the SrTiO3-FeSe interfacethat plays two important roles. First, it facilitates the epitaxial growth ofFeSe. Second, ab initio calculations reveal a strong tendency for electrons totransfer from an oxygen deficient SrTiO3 surface to FeSe when the double TiO2layer is present. As a better electron donor than previously proposedinterfacial structures, the double layer helps to remove the hole pocket in theFeSe at the {\\\\Gamma} point of the Brillouin zone and leads to a band structurecharacteristic of superconducting samples. The characterization of theinterface structure presented here is a key step towards the resolution of manyopen questions about this novel superconductor.',\n",
              " 'A central problem in modern condensed matter physics is the understanding ofmaterials with strong electron correlations. Despite extensive work, theessential physics of many of these systems is not understood and there is verylittle ability to make predictions in this class of materials. In thismanuscript we share our personal views on the major open problems in the fieldof correlated electron systems. We discuss some possible routes to makeprogress in this rich and fascinating field. This manuscript is the result ofthe vigorous discussions and deliberations that took place at Johns HopkinsUniversity during a three-day workshop January 27, 28, and 29, 2020 thatbrought together six senior scientists and 46 more junior scientists. Our hope,is that the topics we have presented will provide inspiration for othersworking in this field and motivation for the idea that significant progress canbe made on very hard problems if we focus our collective energies.',\n",
              " 'Detecting extreme events in large datasets is a major challenge in climatescience research. Current algorithms for extreme event detection are build uponhuman expertise in defining events based on subjective thresholds of relevantphysical variables. Often, multiple competing methods produce vastly differentresults on the same dataset. Accurate characterization of extreme events inclimate simulations and observational data archives is critical forunderstanding the trends and potential impacts of such events in a climatechange content. This study presents the first application of Deep Learningtechniques as alternative methodology for climate extreme events detection.Deep neural networks are able to learn high-level representations of a broadclass of patterns from labeled data. In this work, we developed deepConvolutional Neural Network (CNN) classification system and demonstrated theusefulness of Deep Learning technique for tackling climate pattern detectionproblems. Coupled with Bayesian based hyper-parameter optimization scheme, ourdeep CNN system achieves 89\\\\%-99\\\\% of accuracy in detecting extreme events(Tropical Cyclones, Atmospheric Rivers and Weather Fronts',\n",
              " 'It is shown that basic origin of radial stability is a centripetal componentof the same vibrational force that makes a \"flea\" levitate.',\n",
              " 'In the report we propose six new implementations of ruCLIP model trained onour 240M pairs. The accuracy results are compared with original CLIP model withRu-En translation (OPUS-MT) on 16 datasets from different domains. Our bestimplementations outperform CLIP + OPUS-MT solution on most of the datasets infew-show and zero-shot tasks. In the report we briefly describe theimplementations and concentrate on the conducted experiments. Inferenceexecution time comparison is also presented in the report.',\n",
              " 'Over the past decade the use of machine learning in meteorology has grownrapidly. Specifically neural networks and deep learning have been used at anunprecedented rate. In order to fill the dearth of resources covering neuralnetworks with a meteorological lens, this paper discusses machine learningmethods in a plain language format that is targeted for the operationalmeteorological community. This is the second paper in a pair that aim to serveas a machine learning resource for meteorologists. While the first paperfocused on traditional machine learning methods (e.g., random forest), here abroad spectrum of neural networks and deep learning methods are discussed.Specifically this paper covers perceptrons, artificial neural networks,convolutional neural networks and U-networks. Like the part 1 paper, thismanuscript discusses the terms associated with neural networks and theirtraining. Then the manuscript provides some intuition behind every method andconcludes by showing each method used in a meteorological example of diagnosingthunderstorms from satellite images (e.g., lightning flashes). This paper isaccompanied with an open-source code repository to allow readers to exploreneural networks using either the dataset provided (which is used in the paper)or as a template for alternate datasets.',\n",
              " 'Recent studies of animal social networks have significantly increased ourunderstanding of animal behavior, social interactions, and many importantecological and epidemiological processes. However, most of the studies are atlow temporal and spatial resolution due to the difficulty in recording accuratecontact information. Domestic animals such as cattle have social behavior andserve as an excellent study system because their position can be explicitly andcontinuously tracked, allowing their social networks to be accuratelyconstructed. We used radio-frequency tags to accurately track cattle positionand analyze high-resolution cattle social networks. We tested the hypothesis oftemporal stationarity and spatial homogeneity in these high-resolution networksand demonstrated substantial spatial-temporal heterogeneity during differentdaily time periods (feeding and non-feeding) and in different areas of the pen(grain bunk, water trough, hay bunk, and other general pen area). The socialnetwork structure is analyzed using global network characteristics (networkdensity, exponential random graph model structure), subgroup clustering(modularity), triadic property (transitivity), and dyadic interactions(correlation coefficient from a quadratic assignment procedure). Cattle tend tohave the strongest and most consistent contacts with others around the hay bunkduring the feeding time. These results cannot be determined from data at lowerspatial (aggregated at entire pen level) or temporal (aggregated at dailylevel) resolution. These results reveal new insights for real-time animalsocial network structure dynamics, providing more accurate descriptions thatallow more accurate modeling of multiple (both direct and indirect) diseasetransmission pathways.',\n",
              " 'We describe here the parallels in astronomy and earth science datasets, theiranalyses, and the opportunities for methodology transfer from astroinformaticsto geoinformatics. Using example of hydrology, we emphasize how meta-data andontologies are crucial in such an undertaking. Using the infrastructure beingdesigned for EarthCube - the Virtual Observatory for the earth sciences - wediscuss essential steps for better transfer of tools and techniques in thefuture e.g. domain adaptation. Finally we point out that it is never a one-wayprocess and there is enough for astroinformatics to learn from geoinformaticsas well.',\n",
              " 'We demonstrate an optical waveguide device, capable of supporting the high,in-vacuum, optical power necessary for trapping a single atom or a cold atomensemble with evanescent fields. Our photonic integrated platforms, withsuspended membrane waveguides, successfully manages optical powers of 6 mW (500um span) to nearly 30 mW (125 um span) over an un-tethered waveguide span. Thisplatform is compatible with laser cooling and magneto-optical traps (MOTs) inthe vicinity of the suspended waveguide, called the membrane MOT and the needleMOT, a key ingredient for efficient trap loading. We evaluate two novel designsthat explore critical thermal management features that enable this large powerhandling. This work represents a significant step toward an integrated platformfor coupling neutral atom quantum systems to photonic and electronic integratedcircuits on silicon.',\n",
              " \"Many statistical models have high accuracy on test benchmarks, but are notexplainable, struggle in low-resource scenarios, cannot be reused for multipletasks, and cannot easily integrate domain expertise. These factors limit theiruse, particularly in settings such as mental health, where it is difficult toannotate datasets and model outputs have significant impact. We introduce amicromodel architecture to address these challenges. Our approach allowsresearchers to build interpretable representations that embed domain knowledgeand provide explanations throughout the model's decision process. Wedemonstrate the idea on multiple mental health tasks: depressionclassification, PTSD classification, and suicidal risk assessment. Our systemsconsistently produce strong results, even in low-resource scenarios, and aremore interpretable than alternative methods.\",\n",
              " 'We reexamine a loosely trapped surface (LTS) proposed as an indicator forstrong gravity and an attractive gravity probe surface (AGPS) as that forgravity. Refined inequalities for them are derived by taking account of angularmomentum, gravitational waves and matters.',\n",
              " 'We reexamine the concept of the attractive gravity probe surface recentlyproposed as an indicator for strength of gravity. Then, we propose three newvariant concepts and show refined inequalities for the four types of the AGPSsby taking account of the angular momentum, gravitational waves and matters.',\n",
              " 'We study the properties of the loosely trapped surface (LTS) and thedynamically transversely trapping surface (DTTS) in Einstein-Maxwell systems.These concepts of surfaces were proposed by the four of the present authors inorder to characterize strong gravity regions. We prove the Penrose-likeinequalities for the area of LTSs/DTTSs. Interestingly, although the naivelyexpected upper bound for the area is that of the photon sphere of aReissner-Nordstroem black hole with the same mass and charge, the obtainedinequalities include corrections represented by the energy density orpressure/tension of electromagnetic fields. Due to this correction, thePenrose-like inequality for the area of LTSs is tighter than the naivelyexpected one. We also evaluate the correction term numerically in theMajumdar-Papapetrou two-black-hole spacetimes.',\n",
              " 'In accordance with current models of the accelerating Universe as a spacetimewith a positive cosmological constant, new results about a cosmological upperbound for the area of stable marginally outer trapped surfaces are found takinginto account angular momentum, gravitational waves and matter. Compared toprevious results which take into account only some of the aforementionedvariables, the bound is found to be tighter, giving a concrete limit to thesize of black holes especially relevant in the early Universe.',\n",
              " 'Weakly-supervised anomaly detection aims at learning an anomaly detector froma limited amount of labeled data and abundant unlabeled data. Recent worksbuild deep neural networks for anomaly detection by discriminatively mappingthe normal samples and abnormal samples to different regions in the featurespace or fitting different distributions. However, due to the limited number ofannotated anomaly samples, directly training networks with the discriminativeloss may not be sufficient. To overcome this issue, this paper proposes a novelstrategy to transform the input data into a more meaningful representation thatcould be used for anomaly detection. Specifically, we leverage an autoencoderto encode the input data and utilize three factors, hidden representation,reconstruction residual vector, and reconstruction error, as the newrepresentation for the input data. This representation amounts to encode a testsample with its projection on the training data manifold, its direction to itsprojection and its distance to its projection. In addition to this encoding, wealso propose a novel network architecture to seamlessly incorporate those threefactors. From our extensive experiments, the benefits of the proposed strategyare clearly demonstrated by its superior performance over the competitivemethods.',\n",
              " 'Time series anomaly detection strives to uncover potential abnormal behaviorsand patterns from temporal data, and has fundamental significance in diverseapplication scenarios. Constructing an effective detection model usuallyrequires adequate training data stored in a centralized manner, however, thisrequirement sometimes could not be satisfied in realistic scenarios. As aprevailing approach to address the above problem, federated learning hasdemonstrated its power to cooperate with the distributed data available whileprotecting the privacy of data providers. However, it is still unclear that howexisting time series anomaly detection algorithms perform with decentralizeddata storage and privacy protection through federated learning. To study this,we conduct a federated time series anomaly detection benchmark, namedFedTADBench, which involves five representative time series anomaly detectionalgorithms and four popular federated learning methods. We would like to answerthe following questions: (1)How is the performance of time series anomalydetection algorithms when meeting federated learning? (2) Which federatedlearning method is the most appropriate one for time series anomaly detection?(3) How do federated time series anomaly detection approaches perform ondifferent partitions of data in clients? Numbers of results as well ascorresponding analysis are provided from extensive experiments with varioussettings. The source code of our benchmark is publicly available athttps://github.com/fanxingliu2020/FedTADBench.',\n",
              " 'We show that semigroup C*-algebras are groupoid C*-algebras.',\n",
              " 'We describe the Jacobson topology of the primitive ideal space ofself-similar k-graph C*-algebras under certain conditions.',\n",
              " 'Let $(M, \\\\alpha)$ be a $2n+1$-dimensional connected compact contact toricmanifold of Reeb type. Suppose the contact form $\\\\alpha$ is regular, we findconditions under which $M$ is homeomorphic to $S^{2n+1}$.',\n",
              " 'We give a sufficient condition on totally disconnected topological graphssuch that their associated topological graph algebras are purely infinite.',\n",
              " 'In this paper, we characterize Banach lattices on which each Dunford-Pettisoperator (or weak Dunford-Pettis) is unbounded absolute weak Dunford-Pettisoperator and the converse.',\n",
              " 'In this paper, we study the two phase flow problem with surface tension inthe ideal incompressible magnetohydrodynamics. We first prove the localwell-posedness of the two phase flow problem with surface tension, thendemonstrate that as surface tension tends to zero, the solution of the twophase flow problem with surface tension converges to the solution of the twophase flow problem without surface tension.',\n",
              " 'Let $(M, \\\\omega)$ be a connected, compact symplectic manifold equipped with aHamiltonian $S^1$ action.  We prove that, as fundamental groups of topological spaces,$\\\\pi_1(M)=\\\\pi_1(\\\\hbox{minimum})=\\\\pi_1(\\\\hbox{maximum})=\\\\pi_1(M_{red})$, where$M_{red}$ is the symplectic quotient at any value in the image of the momentmap $\\\\phi$.',\n",
              " 'Let $(M, \\\\omega)$ be a connected compact symplectic manifold equipped with aHamiltonian SU(2) or SO(3) action. We prove that, as fundamental group oftopological spaces, $\\\\pi_1(M)=\\\\pi_1(M_{red})$, where $M_{red}$ is thesymplectic quotient at any value of the moment map $\\\\phi$.',\n",
              " 'Recently Quantum Battle of The Sexes Game has been studied by Luca Marinattoand Tullio Weber. Yet some important problems exist in their scheme. Here wepropose a new scheme to quantize Battle of The Sexes Game, and this scheme willtruly remove the dilemma that exists in the classical form of the game.',\n",
              " \"We prove convergence of critical points $u^h$ of the nonlinear elasticenergies $E^h$ of thin incompressible plates $\\\\Omega^h=\\\\Omega \\\\times (-h/2,h/2)$, which satisfy the von K\\\\'arm\\\\'an scaling: $E^h(u^h)\\\\leq Ch^4$, tocritical points of the appropriate limiting (incompressible von K\\\\'arm\\\\'an)functional.\",\n",
              " 'We define branching systems for finitely aligned higher-rank graphs. Fromthese we construct concrete representations of higher-rank graph C*-algebras onHilbert spaces. We prove a generalized Cuntz-Krieger uniqueness theorem forperiodic single-vertex 2-graphs. We use this result to give a sufficientcondition under which representations of periodic single-vertex 2-graphC*-algebras arising from branching systems are faithful.',\n",
              " 'In this paper, we prove the local well-posedness of the free boundary problemin incompressible elastodynamics under a mixed type stability condition, i.e.,for each point of the free boundary, at least one of the Taylor sign condition$-\\\\partial_n p>0$ and the non-collinearity condition holds. This gives anaffirmative answer to a problem raised by Trakhinin in [30].',\n",
              " 'We pose a conjecture on the K-theory of the self-similar $k$-graph C*-algebraof a standard product of odometers. We generalize the C*-algebra$\\\\mathcal{Q}_S$ to any subset of $\\\\mathbb{N}^\\\\times \\\\setminus \\\\{1\\\\}$ and thenrealize it as the self-similar $k$-graph C*-algebra of a standard product ofodometers.',\n",
              " 'The shear viscosity of QED plasma at finite temperature and density iscalculated by solving Boltzmann equation with variational approach. The resultshows the small chemical potential enhances the viscosity in leading-log orderby adding a chemical potential quadratic term to the viscosity for the puretemperature environment.',\n",
              " 'Traditionally, quantum state correlation can be obtained with calculations ona state density matrix already known. Here, we propose a model with whichcorrelations of unknown quantum states can be obtained. There are no needs ofclassical communication in the course of coupling, optimization and complicatedcalculations. All we need are weak coupling and ancillary systems. We detailthe model on the state in which particles belong to the different owners. Aconcisely example is elaborated in the last part of this paper.',\n",
              " 'We study the pseudorapidity dependent hydrodynamic response in heavy-ioncollisions. A differential hydrodynamic relation is obtained for elliptic flow.Using event-by-event simulations of 3+1D MUSIC, with initial conditionsgenerated via a multi-phase transport (AMPT) model, the differential responserelation is verified. Based on the response relation, we find that thetwo-point correlation of elliptic flow in pseudorapidity are separated into thefluid response and the two-point correlation of initial eccentricity.',\n",
              " 'A cesium atomic fountain clock is under development at Huazhong University ofScience and Technology (HUST) in China. In this paper, we describe theconstruction of the entire fountain clock system and report the preliminaryresults. A frequency stability of $2.5\\\\times 10^{-13} {\\\\tau}^{-1/2}$ has beenachieved by inter-comparison with a hydrogen maser, and the factors limitingthe frequency stability are also discussed.',\n",
              " 'We study quantitatively the formation and radiation acceleration ofelectron-positron pair plasmoids produced by photon-photon collisions nearGalactic black holes (GBHs). The terminal ejecta velocity is found to becompletely determined by the total disk luminosity, proton loading factor anddisk size, with no dependence on the initial velocity. We discuss potentialapplications to the recently discovered Galactic superluminal sources GRS1915,GROJ1655 and possibly other GBHs.',\n",
              " \"We propose a new, simple but powerful algorithm to analyze the gamma-rayburst temporal structures based on identifying non-statistical variations(``peaks'') in the time histories. Detailed analyses of the bursts from thethird BATSE catalog show that $\\\\sim 30$ bursts have more than 20 peaksindividually. Upon identifying most of the peaks in those bursts, we show thatthe peak fluence $S_i$ and peak interval $\\\\delta_i$ distributions within eachburst are consistent with log-normal distributions. Furthermore, we show thatGaussian (in linear space) and power-law distributions for peak fluences areruled out, as is the Poisson distribution for peak intervals.\",\n",
              " 'We construct the non-canonical kinetic term of a k-essence field directlyfrom the effective equation of state function $w_k(z)$, which describes theproperties of the dark energy. Adopting the usual parametrizations of equationof state we numerically reproduce the shape of the non-canonical kinetic termand discuss some features of the constructed form of k-essence.',\n",
              " 'Assume $(M, \\\\omega)$ is a connected, compact 6 dimensional symplecticmanifold equipped with a semi-free Hamiltonian circle action, such that thefixed point set consists of isolated points or compact orientable surfaces. Werestrict attention to the case $\\\\dim$ $H^2(M)<3$. We give a complete list ofthe possible manifolds, determine their equivariant cohomology ring andequivariant Chern classes. Some of these manifolds are classified up todiffeomorphism. We also show the existence for a few cases.',\n",
              " \"Let $(M, \\\\omega)$ be a connected, compact 6-dimensional symplectic manifoldequipped with a semi-free Hamiltonian $S^1$ action such that the fixed pointset consists of isolated points or surfaces. Assume dim $H^2(M)<3$, in\\\\cite{L}, we defined a certain invariant of such spaces which consists of fixedpoint data and twist type, and we divided the possible values of theseinvariants into six ``types''. In this paper, we construct such manifolds withthese ``types''. As a consequence, we have a precise list of the values ofthese invariants.\",\n",
              " 'Tensor methods have gained increasingly attention from various applications,including machine learning, quantum chemistry, healthcare analytics, socialnetwork analysis, data mining, and signal processing, to name a few. Sparsetensors and their algorithms become critical to further improve the performanceof these methods and enhance the interpretability of their output. This workpresents a sparse tensor algorithm benchmark suite (PASTA) for single- andmulti-core CPUs. To the best of our knowledge, this is the first benchmarksuite for sparse tensor world. PASTA targets on: 1) helping application usersto evaluate different computer systems using its representative computationalworkloads; 2) providing insights to better utilize existed computerarchitecture and systems and inspiration for the future design. This benchmarksuite is publicly released https://gitlab.com/tensorworld/pasta.',\n",
              " 'Tensor computations present significant performance challenges that impact awide spectrum of applications ranging from machine learning, healthcareanalytics, social network analysis, data mining to quantum chemistry and signalprocessing. Efforts to improve the performance of tensor computations includeexploring data layout, execution scheduling, and parallelism in common tensorkernels. This work presents a benchmark suite for arbitrary-order sparse tensorkernels using state-of-the-art tensor formats: coordinate (COO) andhierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set ofreference tensor kernel implementations that are compatible with real-worldtensors and power law tensors extended from synthetic graph generationtechniques. We also propose Roofline performance models for these kernels toprovide insights of computer platforms from sparse tensor view.',\n",
              " 'Sparse matrix vector multiplication (SpMV) is an important kernel inscientific and engineering applications. The previous optimizations are sparsematrix format specific and expose the choice of the best format to applicationprogrammers. In this work we develop an auto-tuning framework to bridge gapbetween the specific optimized kernels and their general-purpose use. Wepropose an SpMV auto-tuner (SMAT) that provides an unified interface based oncompressed sparse row (CSR) to programmers by implicitly choosing the bestformat and the fastest implementation of any input sparse matrix in runtime.SMAT leverage a data mining model, which is formulated based on a set ofperformance parameters extracted from 2373 matrices in UF sparse matrixcollection, to fast search the best combination. The experiments show that SMATachieves the maximum performance of 75 GFLOP/s in single-precision and 33GFLOP/s in double-precision on Intel, and 41 GFLOP/s in single-precision and 34GFLOP/s in double-precision on AMD. Compared with the sparse functions in MKLlibrary, SMAT runs faster by more than 3 times.',\n",
              " 'The past decade has witnessed many great successes of machine learning (ML)and deep learning (DL) applications in agricultural systems, including weedcontrol, plant disease diagnosis, agricultural robotics, and precisionlivestock management. Despite tremendous progresses, one downside of such ML/DLmodels is that they generally rely on large-scale labeled datasets fortraining, and the performance of such models is strongly influenced by the sizeand quality of available labeled data samples. In addition, collecting,processing, and labeling such large-scale datasets is extremely costly andtime-consuming, partially due to the rising cost in human labor. Therefore,developing label-efficient ML/DL methods for agricultural applications hasreceived significant interests among researchers and practitioners. In fact,there are more than 50 papers on developing and applying deep-learning-basedlabel-efficient techniques to address various agricultural problems since 2016,which motivates the authors to provide a timely and comprehensive review ofrecent label-efficient ML/DL methods in agricultural applications. To this end,we first develop a principled taxonomy to organize these methods according tothe degree of supervision, including weak supervision (i.e., active learningand semi-/weakly- supervised learning), and no supervision (i.e., un-/self-supervised learning), supplemented by representative state-of-the-artlabel-efficient ML/DL methods. In addition, a systematic review of variousagricultural applications exploiting these label-efficient algorithms, such asprecision agriculture, plant phenotyping, and postharvest quality assessment,is presented. Finally, we discuss the current problems and challenges, as wellas future research directions. A well-classified paper list can be accessed athttps://github.com/DongChen06/Label-efficient-in-Agriculture.',\n",
              " 'We propose a method to visualize vortex cores based on manipulation of thepressure field produced by isolated vortices in incompressible flow. Underideal conditions the function $D=2|\\\\nabla P|/\\\\nabla^2 P$ yields an approximatedistance to vortex centerlines. As opposed to local methods to identifycoherent structures, isosurfaces of $D$ produce a field of vortex tubesequidistant to the vortex core center which, ideally, are independent of vortexintensity or size. In contrast to other line-vortex identification methods,which typically rely on algorithms to detect vortex core lines and frequentlyneed complex implementations, the proposed method can be computed from thelocal Eulerian velocity and pressure fields as easily as vortex identificationmethods such as the $Q$ and $\\\\lambda_2$ criteria. $2|\\\\nabla P|/\\\\nabla^2 P$results in the exact distance to the core center for a Rankine vortex and is ingeneral valid for the region of a vortex where there is pure rotation, yieldingan approximation to the distance farther from the core in other simpleone-dimensional vortex models. The methodology performs well in all tests weattempted, though limitations are presented and discussed. The method isdemonstrated for a canonical Burgers vortex, a Bodewadt vortex, homogeneousisotropic turbulent flow, the wake of a propeller, a heaving plate, a turningcontainership and the airwake of a surface combatant. The proposed method helpsto better visualize vortical flow fields by displaying vortex cores,complementing methods like $Q$ and $\\\\lambda_2$ which display vortical volumes.',\n",
              " 'Epilepsy is a neurological disorder with recurrent seizures of complexity andrandomness. Until now, the mechanism of epileptic randomness has not been fullyelucidated. Inspired by the recent finding that astrocyte GTPase-activatingprotein (G-protein)-coupled receptors could be involved in stochastic epilepticseizures, we proposed a neuron-astrocyte network model, incorporating the noiseof the astrocytic second messager, inositol triphosphate (IP3) which ismodulated by the G-protein)-coupled receptor activation. Based on this model,we have statistically analysed the transitions of epileptic seizures byperforming tens of simulation trials. Our simulation results show that theincrease of the IP3 noise intensity induces the depolarization-block epilepticseizures together with an increase in neuronal firing frequency. Meanwhile, abistable state of neuronal firing emerges under certain noise intensity, duringwhich the neuronal firing pattern switches between regular sparse spiking andepileptic seizure states. This random presence of epileptic seizures is absentwhen the noise intensity continues to increase, accompanying with an increasein the epileptic depolarization block duration. The simulation results alsoshed light on the fact that calcium signals in astrocytes play significantroles in the pattern formations of the epileptic seizure. Our results provide apotential pathway for understanding the epileptic randomness.',\n",
              " \"Modern IT system operation demands the integration of system software andhardware metrics. As a result, it generates a massive amount of data, which canbe potentially used to make data-driven operational decisions. In the basicform, the decision model needs to monitor a large set of machine data, such asCPU utilization, allocated memory, disk and network latency, and predicts thesystem metrics to prevent performance degradation. Nevertheless, building aneffective prediction model in this scenario is rather challenging as the modelhas to accurately capture the long-range coupling dependency in theMultivariate Time-Series (MTS). Moreover, this model needs to have lowcomputational complexity and can scale efficiently to the dimension of dataavailable. In this paper, we propose a highly efficient model named HigeNet topredict the long-time sequence time series. We have deployed the HigeNet onproduction in the D-matrix platform. We also provide offline evaluations onseveral publicly available datasets as well as one online dataset todemonstrate the model's efficacy. The extensive experiments show that trainingtime, resource usage and accuracy of the model are found to be significantlybetter than five state-of-the-art competing models.\",\n",
              " \"I have submitted a new version to arXiv:1910.11174. I forget to choose toreplace the old version, but submitted a new one. It's my mistake.\",\n",
              " 'Entangled photon pairs-discrete light quanta that exhibit non-classicalcorrelations-play a central role in quantum information and quantumcommunication technologies. It is a natural demand from technologicalapplications on the intensity of the entangled photon pairs, such thatsufficient signal strength can be achieved. Here we propose approaches based onklystron tubes that could potentially achieve stable amplification of theentangled photon pairs generated by spontaneous parametric down conversion(SPDC) and entanglement transfer.',\n",
              " 'Inspired by dynamic programming, we propose Stochastic Virtual GradientDescent (SVGD) algorithm where the Virtual Gradient is defined by computationalgraph and automatic differentiation. The method is computationally efficientand has little memory requirements. We also analyze the theoretical convergenceproperties and implementation of the algorithm. Experimental results onmultiple datasets and network models show that SVGD has advantages over otherstochastic optimization methods.',\n",
              " \"Cloud latency has critical influences on the success of cloud applications.Therefore, characterizing cloud network performance is crucial for analyzingand satisfying different latency requirements. By focusing on the cloud'soutbound network latency, this case study on Google App Engine confirms thenecessity of optimizing application deployment. More importantly, our modelingeffort has established a divide-and-conquer framework to address the complexityin understanding and investigating the cloud latency.\",\n",
              " 'Almost all materials are anisotropic. In this paper, interface relations ofanisotropic elliptic partial differential equations involving discontinuitiesacross interfaces are derived in two and three dimensions. Compared withisotropic cases, the invariance of partial differential equations and the jumpconditions under orthogonal coordinates transformation is not valid anymore. Asystematic approach to derive the interface relations is established in thispaper for anisotropic elliptic interface problems, which can be important forderiving high order accurate numerical methods.',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the sentence transformers library to perform embeddings with a pre-trained BERT model"
      ],
      "metadata": {
        "id": "eFN5uqiix9z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMk4DCH2_7AM",
        "outputId": "20f9070b-a43e-451a-b75e-f136c09fad34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=f1e1d30b6611ce9958a591bd002d42c5862dcb21243796dc69431e08d8fb8815\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "embeddings = model.encode(data)\n",
        "\n",
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYQTIUgw_7DT",
        "outputId": "1fd4840b-c9df-40c0-d9e8-0e7664988704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.47090787, -0.34158197,  0.38470918, ..., -0.08545293,\n",
              "        -0.47554964,  0.3649967 ],\n",
              "       [-0.49331838,  0.4214058 ,  0.5074889 , ...,  0.07310393,\n",
              "        -0.12202027,  0.06903689],\n",
              "       [-0.05840614, -0.01529718,  1.0211536 , ..., -0.2641936 ,\n",
              "         0.16799808,  0.10303508],\n",
              "       ...,\n",
              "       [-0.3196099 ,  0.22967564,  0.7998239 , ..., -0.02169785,\n",
              "        -0.13840786,  0.607152  ],\n",
              "       [-0.7710979 ,  0.1965076 ,  1.161349  , ..., -0.04780566,\n",
              "         0.04038496,  0.16807193],\n",
              "       [-0.19984746,  0.1548395 ,  0.23086819, ..., -0.09329325,\n",
              "        -0.05783095, -0.40037242]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcZhz5Fv_7IA",
        "outputId": "dc2f5331-44af-4067-8549-cf9062e560c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a vector database with pinecone API"
      ],
      "metadata": {
        "id": "eXJVVN6ryMqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMUNo22_TQiI",
        "outputId": "518bd0f1-a9fe-41bd-8e79-cbbf60a9a237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.15)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "pinecone.init(api_key=\"removed API key for submission\", environment=\"us-west4-gcp-free\")"
      ],
      "metadata": {
        "id": "lSm2IPspTSGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.delete_index(\"quickstart\")\n",
        "#pinecone.delete_index(\"nc-univ-abstracts\")\n",
        "\n",
        "pinecone.create_index(\"quickstart\", dimension=768, metric=\"euclidean\")"
      ],
      "metadata": {
        "id": "mrm-bwO9TXwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(\"quickstart\")"
      ],
      "metadata": {
        "id": "kVABZUu5TmQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "author_names = list(NCSU['Author']) + list(UNCC['Author']) + list(UNC['Author'])\n",
        "\n",
        "title = list(NCSU['Title']) + list(UNCC['Title']) + list(UNC['Title'])"
      ],
      "metadata": {
        "id": "1kQ5tRWsTt7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding all of the embedded abstracts to the database and removing all special characters since the database cannot store those"
      ],
      "metadata": {
        "id": "TZPXOSDDyYCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "\n",
        "ascii_chars = set(string.printable)\n",
        "\n",
        "for z in range(0, 6001, 100):\n",
        "  if z == 6000:\n",
        "    index.upsert([\n",
        "      (''.join(filter(lambda x: x in ascii_chars, author_names[x] + \" \" + title[x])), [float(y) for y in list(embeddings[x])]) for x in range(6000,6386)\n",
        "    ])\n",
        "  else: \n",
        "    index.upsert([\n",
        "    (''.join(filter(lambda x: x in ascii_chars, author_names[x] + \" \" + title[x])), [float(y) for y in list(embeddings[x])]) for x in range(z,z+100)\n",
        "  ])\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "ztKS5YfOTm4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paragraph_vector(paragraph):\n",
        "  test_data = word_tokenize(paragraph.lower())\n",
        "  return [float(x) for x in model.infer_vector(test_data)]"
      ],
      "metadata": {
        "id": "-Ng0ZPlYVGQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding a test sentence"
      ],
      "metadata": {
        "id": "gDKjOff3yjHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am deeply interested in computational chemistry because it offers a powerful toolkit to explore and understand the behavior of molecules and chemical reactions using computer simulations and modeling techniques. The ability to accurately predict molecular structures, energetics, and properties using quantum mechanical calculations fascinates me. I am particularly drawn to the field of drug discovery and design, where computational chemistry plays a crucial role in screening and optimizing potential drug candidates, saving time and resources in the drug development process. The opportunity to contribute to the development of novel computational methods and algorithms that enhance our understanding of chemical phenomena and enable the discovery of new materials and therapeutic agents inspires me to pursue research in computational chemistry.\"\n",
        "\n",
        "# Convert the sentence to a vector embedding\n",
        "embedding = model.encode([sentence])\n",
        "[float(x) for x in embedding[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiCoMP_pFfG5",
        "outputId": "6f75ace7-13d3-46f4-c660-e827905ef61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.4834110140800476,\n",
              " 0.169001504778862,\n",
              " 0.5910281538963318,\n",
              " -0.3540003299713135,\n",
              " 0.2510969340801239,\n",
              " -1.081726312637329,\n",
              " 0.07124583423137665,\n",
              " -0.9381856918334961,\n",
              " 0.8304867744445801,\n",
              " -0.6335754990577698,\n",
              " 0.6101561188697815,\n",
              " 0.3172786235809326,\n",
              " -0.5610018968582153,\n",
              " 0.5212736129760742,\n",
              " -0.10263371467590332,\n",
              " 0.2027750015258789,\n",
              " 0.8174994587898254,\n",
              " 0.013464704155921936,\n",
              " -0.14559295773506165,\n",
              " -0.25942179560661316,\n",
              " 0.3581311106681824,\n",
              " 0.12325866520404816,\n",
              " -0.25607332587242126,\n",
              " 0.9319225549697876,\n",
              " 0.6840067505836487,\n",
              " -0.3887467384338379,\n",
              " 0.08847934007644653,\n",
              " 1.1225191354751587,\n",
              " 0.15239231288433075,\n",
              " 0.4359714686870575,\n",
              " 0.024752654135227203,\n",
              " -0.9460771083831787,\n",
              " -1.0376107692718506,\n",
              " 0.2995322346687317,\n",
              " -0.0977633148431778,\n",
              " 0.6892662644386292,\n",
              " -0.08383104205131531,\n",
              " -0.31456804275512695,\n",
              " -0.17637695372104645,\n",
              " 0.2261277288198471,\n",
              " -0.4644375443458557,\n",
              " -0.36589086055755615,\n",
              " 0.11404850333929062,\n",
              " 0.2191469967365265,\n",
              " -0.1608125865459442,\n",
              " -0.09966467320919037,\n",
              " -0.5466580390930176,\n",
              " 0.7271758317947388,\n",
              " -0.683257520198822,\n",
              " -0.353024959564209,\n",
              " -0.5202299356460571,\n",
              " -0.6696293354034424,\n",
              " -0.2722501754760742,\n",
              " -0.31340262293815613,\n",
              " 0.16025543212890625,\n",
              " -0.010796409100294113,\n",
              " -0.3230041563510895,\n",
              " -0.03639126569032669,\n",
              " 0.6451469659805298,\n",
              " -0.6598265171051025,\n",
              " -0.12384356558322906,\n",
              " -0.14136448502540588,\n",
              " 0.3637385368347168,\n",
              " -0.2615443468093872,\n",
              " 0.8293708562850952,\n",
              " -0.5153766870498657,\n",
              " -0.8900648355484009,\n",
              " -0.36183229088783264,\n",
              " 0.03199046850204468,\n",
              " -0.5623898506164551,\n",
              " 0.5531062483787537,\n",
              " 0.12952905893325806,\n",
              " -0.6391092538833618,\n",
              " -0.40848156809806824,\n",
              " -0.4530991315841675,\n",
              " -0.202621191740036,\n",
              " 0.7508754730224609,\n",
              " -0.7162512540817261,\n",
              " -0.1736798733472824,\n",
              " -0.3717719614505768,\n",
              " -0.8376011848449707,\n",
              " -0.39659959077835083,\n",
              " -0.16012388467788696,\n",
              " 1.0772813558578491,\n",
              " 0.6914435625076294,\n",
              " 0.6511245369911194,\n",
              " 0.5322595834732056,\n",
              " 0.011185124516487122,\n",
              " -0.31250452995300293,\n",
              " -0.8810184001922607,\n",
              " -0.45395877957344055,\n",
              " -0.11777964234352112,\n",
              " -0.20100829005241394,\n",
              " 0.4623860716819763,\n",
              " 0.47777217626571655,\n",
              " -0.1603623777627945,\n",
              " 0.06560995429754257,\n",
              " -0.21807542443275452,\n",
              " -0.7611631155014038,\n",
              " 0.7678064107894897,\n",
              " -0.020464926958084106,\n",
              " 0.1808958351612091,\n",
              " -0.282629132270813,\n",
              " -0.23012375831604004,\n",
              " 0.04925248771905899,\n",
              " 0.2645329236984253,\n",
              " 0.12950977683067322,\n",
              " -0.6523787975311279,\n",
              " -0.28079110383987427,\n",
              " 0.15016403794288635,\n",
              " 0.22727224230766296,\n",
              " 0.02702961303293705,\n",
              " -0.3906356990337372,\n",
              " -0.31675058603286743,\n",
              " -0.008731353096663952,\n",
              " -0.8513585329055786,\n",
              " 0.06776056438684464,\n",
              " 0.965826153755188,\n",
              " -0.238980233669281,\n",
              " 0.21705076098442078,\n",
              " -0.053069911897182465,\n",
              " 0.4841451048851013,\n",
              " 0.3796708583831787,\n",
              " 0.9192278385162354,\n",
              " 0.21069148182868958,\n",
              " -0.21919530630111694,\n",
              " -0.41947388648986816,\n",
              " 0.03553565591573715,\n",
              " 0.0586073100566864,\n",
              " -0.7837898135185242,\n",
              " 0.2703603506088257,\n",
              " 1.02886164188385,\n",
              " 0.09387056529521942,\n",
              " -0.11962084472179413,\n",
              " 0.8192969560623169,\n",
              " -0.2154136598110199,\n",
              " -0.26376470923423767,\n",
              " 0.05787426978349686,\n",
              " -0.4537310302257538,\n",
              " 0.6670237183570862,\n",
              " -0.6741600036621094,\n",
              " -0.07470550388097763,\n",
              " 1.1841211318969727,\n",
              " -0.43360471725463867,\n",
              " 0.06961298733949661,\n",
              " -0.4930737316608429,\n",
              " -0.02646789699792862,\n",
              " -0.8109670281410217,\n",
              " -0.05264946073293686,\n",
              " -0.14265203475952148,\n",
              " 0.18747735023498535,\n",
              " -0.15468326210975647,\n",
              " -0.4607672691345215,\n",
              " 0.34128567576408386,\n",
              " 0.5070769190788269,\n",
              " 0.6023016571998596,\n",
              " -1.439828872680664,\n",
              " -0.029827892780303955,\n",
              " -0.06933580338954926,\n",
              " 0.25393274426460266,\n",
              " -0.5182441473007202,\n",
              " -0.643322229385376,\n",
              " 0.28326934576034546,\n",
              " -0.4370120167732239,\n",
              " -0.3430541157722473,\n",
              " -0.646935760974884,\n",
              " 0.8196424245834351,\n",
              " -0.1777190864086151,\n",
              " 0.28581613302230835,\n",
              " 0.4609762132167816,\n",
              " 0.22780831158161163,\n",
              " -0.09675263613462448,\n",
              " 0.29911625385284424,\n",
              " 0.060538724064826965,\n",
              " -0.299936980009079,\n",
              " -0.253600537776947,\n",
              " 0.2423153966665268,\n",
              " -1.0020161867141724,\n",
              " -0.4348905682563782,\n",
              " -0.11777850240468979,\n",
              " 0.18152469396591187,\n",
              " -0.758861780166626,\n",
              " -0.2296263426542282,\n",
              " -0.19220319390296936,\n",
              " -0.3749682307243347,\n",
              " 0.11907525360584259,\n",
              " 0.7397757172584534,\n",
              " 0.7458308339118958,\n",
              " -0.6070781350135803,\n",
              " -0.13906386494636536,\n",
              " -0.15993790328502655,\n",
              " -0.15111497044563293,\n",
              " -1.0930649042129517,\n",
              " -0.4594348669052124,\n",
              " 0.3341590166091919,\n",
              " 0.7448921203613281,\n",
              " -0.42930495738983154,\n",
              " 0.48637551069259644,\n",
              " 0.05633912235498428,\n",
              " -0.4259110987186432,\n",
              " -0.08447714149951935,\n",
              " 0.1576339602470398,\n",
              " -0.5352683067321777,\n",
              " -0.32643604278564453,\n",
              " -0.40810948610305786,\n",
              " -1.2996480464935303,\n",
              " 0.20603330433368683,\n",
              " -0.6550683975219727,\n",
              " -0.21833838522434235,\n",
              " -1.009352445602417,\n",
              " -0.6905750036239624,\n",
              " 0.5874229073524475,\n",
              " 0.28724074363708496,\n",
              " -0.40983638167381287,\n",
              " 0.09517773985862732,\n",
              " 0.8327935338020325,\n",
              " -0.7669949531555176,\n",
              " 0.2419595569372177,\n",
              " 0.6387110948562622,\n",
              " 1.0652233362197876,\n",
              " 0.5143527984619141,\n",
              " -0.2682969570159912,\n",
              " 0.4687100052833557,\n",
              " -0.9945353269577026,\n",
              " -0.0327615849673748,\n",
              " -0.5103244781494141,\n",
              " -0.32948848605155945,\n",
              " -0.3914984464645386,\n",
              " 0.9947801828384399,\n",
              " 0.28684529662132263,\n",
              " -0.12622053921222687,\n",
              " 0.12583476305007935,\n",
              " 0.33095210790634155,\n",
              " 0.11969605088233948,\n",
              " 0.24327021837234497,\n",
              " -0.9722341299057007,\n",
              " -0.30021923780441284,\n",
              " 0.25051769614219666,\n",
              " 0.3915242552757263,\n",
              " -0.27657201886177063,\n",
              " -0.39042896032333374,\n",
              " 0.10997433960437775,\n",
              " 0.038561444729566574,\n",
              " -0.4552801549434662,\n",
              " 0.017012055963277817,\n",
              " 0.2728756368160248,\n",
              " -0.6275349855422974,\n",
              " -0.305950403213501,\n",
              " 1.3843505382537842,\n",
              " -1.243637204170227,\n",
              " -0.1333594024181366,\n",
              " -0.2942454218864441,\n",
              " 0.11405513435602188,\n",
              " -0.446161687374115,\n",
              " 0.9564609527587891,\n",
              " -0.8733813762664795,\n",
              " -0.0851360559463501,\n",
              " -0.9217798709869385,\n",
              " -0.35115915536880493,\n",
              " 0.7050327062606812,\n",
              " -0.6750115156173706,\n",
              " -0.1992761194705963,\n",
              " 0.2947012782096863,\n",
              " -0.021060669794678688,\n",
              " -0.5138475298881531,\n",
              " 0.5279321670532227,\n",
              " -0.7250202894210815,\n",
              " -0.8067187070846558,\n",
              " 0.24981454014778137,\n",
              " -0.18478292226791382,\n",
              " -0.46789273619651794,\n",
              " -0.12362387031316757,\n",
              " -0.03463822603225708,\n",
              " -0.021220732480287552,\n",
              " 0.3426749110221863,\n",
              " -0.10533437132835388,\n",
              " -0.6006977558135986,\n",
              " -0.6224422454833984,\n",
              " 0.3570305407047272,\n",
              " -0.23830904066562653,\n",
              " -0.45899006724357605,\n",
              " 1.1151902675628662,\n",
              " 0.2807987928390503,\n",
              " -1.629880428314209,\n",
              " -1.0625572204589844,\n",
              " -0.19008105993270874,\n",
              " -0.3317180871963501,\n",
              " -0.10854600369930267,\n",
              " -0.09633217006921768,\n",
              " -0.0890829861164093,\n",
              " -0.540151834487915,\n",
              " -0.5208582878112793,\n",
              " -0.7430130839347839,\n",
              " 0.44292160868644714,\n",
              " 0.3543619215488434,\n",
              " -1.3011146783828735,\n",
              " 0.7310519218444824,\n",
              " 0.3569747805595398,\n",
              " 0.025646209716796875,\n",
              " -0.06874461472034454,\n",
              " -0.33612895011901855,\n",
              " 0.551521360874176,\n",
              " 0.018481891602277756,\n",
              " -0.1820627748966217,\n",
              " 0.4364793002605438,\n",
              " 0.05396488308906555,\n",
              " 0.6369023323059082,\n",
              " 0.06315803527832031,\n",
              " -0.21274015307426453,\n",
              " 0.033821169286966324,\n",
              " 0.5795652866363525,\n",
              " 0.037598658353090286,\n",
              " -0.017997978255152702,\n",
              " 0.40585505962371826,\n",
              " 0.1742316335439682,\n",
              " 0.19646285474300385,\n",
              " 0.5984729528427124,\n",
              " -0.6435614824295044,\n",
              " -0.7823957204818726,\n",
              " -0.746494710445404,\n",
              " -0.9867071509361267,\n",
              " 0.23802903294563293,\n",
              " -0.334810733795166,\n",
              " -0.46210843324661255,\n",
              " 0.4130778908729553,\n",
              " 0.23272141814231873,\n",
              " 0.27551987767219543,\n",
              " 0.4087516665458679,\n",
              " 0.07646457850933075,\n",
              " 0.3958374261856079,\n",
              " -0.2236545979976654,\n",
              " -0.4435398578643799,\n",
              " 0.30207720398902893,\n",
              " -0.5439279079437256,\n",
              " -0.2548365890979767,\n",
              " -0.864757776260376,\n",
              " 0.09279797226190567,\n",
              " -0.05208973214030266,\n",
              " 0.29639777541160583,\n",
              " -1.1758992671966553,\n",
              " 0.14781683683395386,\n",
              " -0.21734413504600525,\n",
              " 0.4273167848587036,\n",
              " 0.6187858581542969,\n",
              " -0.017069751396775246,\n",
              " -0.4464109539985657,\n",
              " -0.3751446604728699,\n",
              " -0.6347092986106873,\n",
              " 0.07011391222476959,\n",
              " -0.23768168687820435,\n",
              " 0.13254819810390472,\n",
              " -0.24649372696876526,\n",
              " 0.7665796279907227,\n",
              " -0.9526006579399109,\n",
              " 0.06956837326288223,\n",
              " 0.3182743191719055,\n",
              " 0.36414462327957153,\n",
              " 0.049789562821388245,\n",
              " 0.4351429343223572,\n",
              " -0.055632397532463074,\n",
              " 0.7463388442993164,\n",
              " -0.2568080425262451,\n",
              " 0.1335718184709549,\n",
              " -0.6104350686073303,\n",
              " 0.3753048777580261,\n",
              " -1.0509079694747925,\n",
              " -0.22881728410720825,\n",
              " 0.06828027218580246,\n",
              " 0.16445592045783997,\n",
              " 0.09428560733795166,\n",
              " 0.06611470133066177,\n",
              " -0.2723466753959656,\n",
              " 0.05668190121650696,\n",
              " 0.5397003889083862,\n",
              " -0.4492364227771759,\n",
              " -0.38507503271102905,\n",
              " 0.18286967277526855,\n",
              " 0.9305722117424011,\n",
              " 0.5472723245620728,\n",
              " 0.7901932597160339,\n",
              " -0.6858828067779541,\n",
              " 0.312019407749176,\n",
              " 0.22006307542324066,\n",
              " 1.3444361686706543,\n",
              " -0.6466298699378967,\n",
              " -1.1172409057617188,\n",
              " -0.330427885055542,\n",
              " 0.001016056165099144,\n",
              " 0.18347114324569702,\n",
              " -0.26495465636253357,\n",
              " 0.13906490802764893,\n",
              " 0.3748539090156555,\n",
              " 0.39306336641311646,\n",
              " -0.6761675477027893,\n",
              " -0.5738317966461182,\n",
              " -0.32419124245643616,\n",
              " -0.5363122820854187,\n",
              " 0.7021301984786987,\n",
              " -0.143929123878479,\n",
              " 0.2635083794593811,\n",
              " 0.4630591571331024,\n",
              " 0.4498339891433716,\n",
              " -0.35596954822540283,\n",
              " 0.9759362936019897,\n",
              " -0.34367015957832336,\n",
              " 0.6916468143463135,\n",
              " -0.4058583974838257,\n",
              " -0.714150607585907,\n",
              " 1.8831912279129028,\n",
              " 0.24071860313415527,\n",
              " 0.5470904111862183,\n",
              " 0.23744261264801025,\n",
              " 0.6494584083557129,\n",
              " -0.3342660367488861,\n",
              " -0.4448799192905426,\n",
              " -0.07249423861503601,\n",
              " 0.07517839968204498,\n",
              " -0.4888211488723755,\n",
              " -1.2355759143829346,\n",
              " 0.3405296802520752,\n",
              " 0.34820905327796936,\n",
              " -0.015150515362620354,\n",
              " 0.4583342671394348,\n",
              " 0.4128561019897461,\n",
              " 0.2395176887512207,\n",
              " -0.4446457624435425,\n",
              " 0.7544910311698914,\n",
              " -0.14260689914226532,\n",
              " -0.4466989040374756,\n",
              " 0.49850979447364807,\n",
              " 0.147196963429451,\n",
              " 0.43260252475738525,\n",
              " 0.9512490630149841,\n",
              " -0.6890206336975098,\n",
              " 0.8419506549835205,\n",
              " 0.5608687400817871,\n",
              " 0.37065020203590393,\n",
              " -0.08172114193439484,\n",
              " 0.5179684162139893,\n",
              " -0.14752234518527985,\n",
              " -0.5052317380905151,\n",
              " -0.3337855935096741,\n",
              " -0.13591066002845764,\n",
              " -0.026566002517938614,\n",
              " -0.20612826943397522,\n",
              " 0.01962990313768387,\n",
              " -0.020911213010549545,\n",
              " 0.45735782384872437,\n",
              " 0.45987892150878906,\n",
              " -0.12678496539592743,\n",
              " 0.7314666509628296,\n",
              " -1.1621469259262085,\n",
              " 0.6645482778549194,\n",
              " -0.7112718820571899,\n",
              " 0.6932340264320374,\n",
              " 0.0288897305727005,\n",
              " -0.24999505281448364,\n",
              " 0.17140883207321167,\n",
              " -1.3659121990203857,\n",
              " 0.6792722940444946,\n",
              " -0.042839981615543365,\n",
              " -1.0159788131713867,\n",
              " -0.07912881672382355,\n",
              " -0.060059234499931335,\n",
              " 1.0416830778121948,\n",
              " -0.07685218751430511,\n",
              " 0.1394292414188385,\n",
              " 1.0991935729980469,\n",
              " 0.1967533975839615,\n",
              " -0.3814699947834015,\n",
              " 0.7315831184387207,\n",
              " 0.663821816444397,\n",
              " -0.8713594675064087,\n",
              " 0.17499034106731415,\n",
              " -0.9832661151885986,\n",
              " -0.4394237995147705,\n",
              " 0.35924822092056274,\n",
              " 0.669144868850708,\n",
              " 0.04200252890586853,\n",
              " -1.0371687412261963,\n",
              " -0.2745751440525055,\n",
              " -0.28529638051986694,\n",
              " 0.013834122568368912,\n",
              " -0.27894991636276245,\n",
              " 0.29875969886779785,\n",
              " -0.6335266828536987,\n",
              " -0.9652854800224304,\n",
              " 0.44991540908813477,\n",
              " -0.6839722394943237,\n",
              " -1.190161108970642,\n",
              " 0.03106607496738434,\n",
              " 0.59428471326828,\n",
              " 0.5023156404495239,\n",
              " -0.13943666219711304,\n",
              " -0.4703998267650604,\n",
              " -0.14276060461997986,\n",
              " 0.40372905135154724,\n",
              " 0.010980319231748581,\n",
              " -0.5219789743423462,\n",
              " 0.5563080310821533,\n",
              " 0.2536904215812683,\n",
              " -0.6779183149337769,\n",
              " 0.011505036614835262,\n",
              " 0.4214462637901306,\n",
              " 0.2822658121585846,\n",
              " 0.4772544205188751,\n",
              " -0.02600664459168911,\n",
              " -1.1760034561157227,\n",
              " -0.035090330988168716,\n",
              " 0.2872074246406555,\n",
              " 0.22621458768844604,\n",
              " -0.17299631237983704,\n",
              " 0.45096728205680847,\n",
              " -0.3696877360343933,\n",
              " -0.6733629703521729,\n",
              " -0.11825857311487198,\n",
              " -0.23305076360702515,\n",
              " 0.44923606514930725,\n",
              " 0.27515894174575806,\n",
              " 0.5613399744033813,\n",
              " 0.5714658498764038,\n",
              " -0.2895519435405731,\n",
              " 0.10359374433755875,\n",
              " 0.24190649390220642,\n",
              " 0.5420558452606201,\n",
              " -0.20276805758476257,\n",
              " -0.18429240584373474,\n",
              " 0.6181062459945679,\n",
              " -0.0405707061290741,\n",
              " -0.8441381454467773,\n",
              " 1.083082914352417,\n",
              " -1.2277867794036865,\n",
              " -0.4342961311340332,\n",
              " -1.0011672973632812,\n",
              " 0.055652908980846405,\n",
              " 0.3959779441356659,\n",
              " 0.46722716093063354,\n",
              " -0.1641809195280075,\n",
              " 0.15900951623916626,\n",
              " -0.16904506087303162,\n",
              " -0.3785657584667206,\n",
              " -0.2663247883319855,\n",
              " -0.07759071886539459,\n",
              " -0.32562023401260376,\n",
              " 0.5939561128616333,\n",
              " 0.4976596534252167,\n",
              " 0.3199900984764099,\n",
              " 0.350736528635025,\n",
              " -0.4130457043647766,\n",
              " 0.30275601148605347,\n",
              " -0.5838265419006348,\n",
              " 0.481865793466568,\n",
              " 0.7248039245605469,\n",
              " 0.965062141418457,\n",
              " -0.7403867244720459,\n",
              " 0.6656032204627991,\n",
              " -0.5088745355606079,\n",
              " -1.1378529071807861,\n",
              " -0.3510078489780426,\n",
              " 0.4931023120880127,\n",
              " -0.34473615884780884,\n",
              " 0.4130070209503174,\n",
              " 0.8736181259155273,\n",
              " 0.09139014780521393,\n",
              " 0.12985298037528992,\n",
              " 0.020117152482271194,\n",
              " 0.16323237121105194,\n",
              " 0.4357222318649292,\n",
              " -0.1943686455488205,\n",
              " -0.492484450340271,\n",
              " -1.215177059173584,\n",
              " -0.18767113983631134,\n",
              " 0.48934298753738403,\n",
              " -0.5441914200782776,\n",
              " 0.008524966426193714,\n",
              " -0.46311163902282715,\n",
              " 0.06211215257644653,\n",
              " -0.2911219000816345,\n",
              " 0.1859472244977951,\n",
              " 0.16721494495868683,\n",
              " 0.38566339015960693,\n",
              " 0.176881343126297,\n",
              " 0.29319316148757935,\n",
              " 0.870747447013855,\n",
              " 0.4159321188926697,\n",
              " -0.5830735564231873,\n",
              " 0.2806953191757202,\n",
              " -0.5233403444290161,\n",
              " -0.1569286286830902,\n",
              " 0.524074137210846,\n",
              " -0.4062962532043457,\n",
              " -0.3115209937095642,\n",
              " 0.3771420121192932,\n",
              " -0.28951457142829895,\n",
              " 0.2946382761001587,\n",
              " -0.2569740414619446,\n",
              " 0.6397933959960938,\n",
              " 0.13352569937705994,\n",
              " 0.18949633836746216,\n",
              " -0.19795843958854675,\n",
              " -0.09562774002552032,\n",
              " -0.739957332611084,\n",
              " -0.37977221608161926,\n",
              " -0.3173568844795227,\n",
              " 0.4508016109466553,\n",
              " -0.18950389325618744,\n",
              " 0.2305677831172943,\n",
              " -0.12165352702140808,\n",
              " 0.6734914779663086,\n",
              " 0.5204340219497681,\n",
              " 0.9592624306678772,\n",
              " -0.3740699291229248,\n",
              " 0.44139134883880615,\n",
              " 0.5239579677581787,\n",
              " 0.955331563949585,\n",
              " 0.5909199118614197,\n",
              " -1.8624762296676636,\n",
              " -0.9169961214065552,\n",
              " -0.12246115505695343,\n",
              " -0.6040111780166626,\n",
              " -0.38386446237564087,\n",
              " 0.08985626697540283,\n",
              " -1.2085371017456055,\n",
              " 0.34326326847076416,\n",
              " 0.8659334182739258,\n",
              " 0.06325340270996094,\n",
              " 0.2982959747314453,\n",
              " -0.3275166451931,\n",
              " -0.20828716456890106,\n",
              " -0.5383251905441284,\n",
              " -0.13892477750778198,\n",
              " -0.5295964479446411,\n",
              " 0.5592051148414612,\n",
              " -0.016424020752310753,\n",
              " -0.3158409297466278,\n",
              " -0.6950355172157288,\n",
              " 0.2756451964378357,\n",
              " 0.27161914110183716,\n",
              " -0.34754785895347595,\n",
              " -0.2883864641189575,\n",
              " -1.071709156036377,\n",
              " -0.25910383462905884,\n",
              " 0.0423758402466774,\n",
              " 0.4136944115161896,\n",
              " 0.463712215423584,\n",
              " 1.1184552907943726,\n",
              " -0.4157792925834656,\n",
              " 0.31173378229141235,\n",
              " -0.9430137872695923,\n",
              " 0.583310604095459,\n",
              " -0.21433576941490173,\n",
              " 1.1912145614624023,\n",
              " 0.233158677816391,\n",
              " 0.38195616006851196,\n",
              " 0.4155484735965729,\n",
              " 0.32614827156066895,\n",
              " -0.079041987657547,\n",
              " -0.0026111695915460587,\n",
              " 0.19940339028835297,\n",
              " 0.16182643175125122,\n",
              " -0.2983437180519104,\n",
              " 0.1666198968887329,\n",
              " 0.3681451678276062,\n",
              " -0.19993960857391357,\n",
              " 0.22974279522895813,\n",
              " -0.49823659658432007,\n",
              " 0.5129152536392212,\n",
              " -0.06074228510260582,\n",
              " 0.4988740086555481,\n",
              " 1.065096139907837,\n",
              " 0.2712801992893219,\n",
              " 0.4588439464569092,\n",
              " -0.8123376965522766,\n",
              " 0.5332621335983276,\n",
              " 0.6127858757972717,\n",
              " -0.0880374014377594,\n",
              " -0.22639420628547668,\n",
              " -0.8113781213760376,\n",
              " -0.8860412836074829,\n",
              " 0.07667567580938339,\n",
              " -0.6360722780227661,\n",
              " -0.22370988130569458,\n",
              " -0.6583579182624817,\n",
              " -0.3499706983566284,\n",
              " 0.592497706413269,\n",
              " -0.9502511024475098,\n",
              " -0.4819231331348419,\n",
              " -0.1318226009607315,\n",
              " 0.935218095779419,\n",
              " -0.31879907846450806,\n",
              " -0.38338419795036316,\n",
              " -0.03496670722961426,\n",
              " 0.9844277501106262,\n",
              " -0.5879870653152466,\n",
              " -0.8120787143707275,\n",
              " 0.32641586661338806,\n",
              " -0.5074783563613892,\n",
              " -0.5755581855773926,\n",
              " 0.49158361554145813,\n",
              " 0.677544116973877,\n",
              " -0.3147003650665283,\n",
              " -0.991203784942627,\n",
              " 0.42279255390167236,\n",
              " -0.29666224122047424,\n",
              " -0.33950984477996826,\n",
              " 0.9290257692337036,\n",
              " 0.8138178586959839,\n",
              " -0.22743552923202515,\n",
              " 0.14017155766487122,\n",
              " 0.050440024584531784,\n",
              " 0.3980603814125061,\n",
              " -0.06983822584152222,\n",
              " -0.6830775737762451,\n",
              " 0.5095526576042175,\n",
              " -0.006699555553495884,\n",
              " 0.41153639554977417,\n",
              " 0.6583609580993652,\n",
              " 0.3197305202484131,\n",
              " -0.003962259739637375,\n",
              " 0.2083171308040619,\n",
              " 0.1634412407875061,\n",
              " -0.0474931076169014,\n",
              " -0.3472948372364044,\n",
              " 0.7655101418495178,\n",
              " -0.40601515769958496,\n",
              " 0.38986116647720337,\n",
              " 0.6726309061050415,\n",
              " 0.11190178990364075,\n",
              " 1.0018463134765625,\n",
              " 0.7358578443527222,\n",
              " 0.46635162830352783,\n",
              " -0.9354339838027954,\n",
              " -0.39179015159606934,\n",
              " 0.5910742282867432,\n",
              " 0.061634812504053116,\n",
              " 0.41395682096481323,\n",
              " -0.4257085621356964,\n",
              " -0.43834784626960754,\n",
              " 0.4788256883621216,\n",
              " 1.0219910144805908,\n",
              " -0.8631024360656738,\n",
              " 1.3323585987091064,\n",
              " 0.06485258042812347,\n",
              " 0.04055964946746826,\n",
              " 0.30149710178375244,\n",
              " 0.03809472918510437,\n",
              " -1.089672565460205,\n",
              " 0.6610482335090637,\n",
              " -0.4298318922519684,\n",
              " 0.7751889824867249,\n",
              " 0.07302159070968628,\n",
              " 0.16613061726093292,\n",
              " -0.12119969725608826,\n",
              " -0.01633908413350582,\n",
              " 0.4652174711227417,\n",
              " 0.4943268299102783,\n",
              " 0.20596180856227875,\n",
              " -1.0877385139465332,\n",
              " 0.24518048763275146,\n",
              " -0.03413844108581543,\n",
              " -0.6843758225440979,\n",
              " 0.33826538920402527,\n",
              " -0.5484766960144043,\n",
              " -0.4062325358390808,\n",
              " 0.2423042356967926,\n",
              " -0.10609807074069977,\n",
              " -0.6155352592468262,\n",
              " -0.1282542198896408]"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting search results for the test sentence"
      ],
      "metadata": {
        "id": "6Z1FdThtylJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[index.query(vector= [float(x) for x in embedding[0]],top_k=10,include_values=True)['matches'][x]['id'] for x in range(10)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTrMnqtpUM-U",
        "outputId": "2b408214-037b-47e5-d449-047a084d0f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Paul Zimmerman TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search',\n",
              " 'Zixuan Cang A review of mathematical representations of biomolecules',\n",
              " 'Ehssan Nazockdast A fast platform for simulating flexible fiber suspensions applied to cell mechanics',\n",
              " 'Lubos Mitas Energetics and Dipole Moment of Transition Metal Monoxides by Quantum Monte Carlo',\n",
              " 'Junier Oliva MolecularRNN: Generating realistic molecular graphs with optimized properties',\n",
              " 'ALEXANDER TROPSHA Deep Reinforcement Learning for De-Novo Drug Design',\n",
              " 'Daniel Munoz SpeedMachines: Anytime Structured Prediction',\n",
              " 'Justin Baker Quantum Computing at the Frontiers of Biological Sciences',\n",
              " 'Edward Grant Computation of molecular excited states on IBM quantum computers using a discriminative variational quantum eigensolver',\n",
              " 'Zixuan Cang Mathematical deep learning for pose and binding affinity prediction and ranking in D3R Grand Challenges']"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    }
  ]
}